{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ioLoShcpx6H7"
   },
   "source": [
    "\n",
    "# Intro to LangChain\n",
    "\n",
    "![](https://python.langchain.com/svg/langchain_stack_112024_dark.svg)\n",
    "\n",
    "LangChain is an open-source framework that gives developers the tools they need to create applications using large language models (LLMs). In its essence, LangChain is a prompt orchestration tool that makes it easier for teams to connect various prompts interactively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 55685,
     "status": "ok",
     "timestamp": 1713514624058,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "miOF_GZU5mst",
    "outputId": "489dd3e4-06e9-4fb0-97d4-acb640612a24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU openai\n",
    "%pip install -qU langchain\n",
    "%pip install -qU langchain-core\n",
    "%pip install -qU langchain-experimental\n",
    "%pip install -qU langchain-community\n",
    "%pip install -qU langchain-openai\n",
    "%pip install -qU langchain-ollama\n",
    "%pip install -qU tiktoken\n",
    "%pip install -qU pydantic[email]\n",
    "%pip install -qU jupyter ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yrb7nxgq7nvR"
   },
   "source": [
    "# Accessing OpenAI Directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "from google import genai\n",
    "\n",
    "load_dotenv(\"../../.env\", override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ[\"OPENAI_API_KEY\"] = \"<the key>\"\n",
    "#openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "client = openai.OpenAI()\n",
    "model=\"gpt-5-nano\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 286,
     "status": "ok",
     "timestamp": 1713514904192,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "YoxylcB85gDr"
   },
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=model):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "        ]\n",
    "    response = client.chat.completions.create(model=model,\n",
    "                                              messages=messages,\n",
    "                                              )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m8GyMwn47XMn"
   },
   "source": [
    "## Simple Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1048,
     "status": "ok",
     "timestamp": 1713514927826,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "ZKQp2YrMyBvz",
    "outputId": "dc37e5db-bf91-44f4-97df-f7be826be323"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. In standard decimal arithmetic, 1 + 1 = 2. In base-2 (binary), it's written as 10.\n"
     ]
    }
   ],
   "source": [
    "response = get_completion(\"Can you tell me how much is 1+1?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mw2bnNelykOv"
   },
   "source": [
    "## Queries with custom prompts using formatted strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 361,
     "status": "ok",
     "timestamp": 1713515082959,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "GrEcvuU7yjtu",
    "outputId": "995ff889-84b8-4f18-e439-1c04fb8cf208"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate the text that is delimited by triple backticks\n",
      "into a style that is American English in a calm and respectful tone\n",
      ".\n",
      "text: ```\n",
      "Arrr, I be fuming that me blender lid flew off and splattered me kitchen walls\n",
      "with smoothie! And to make matters worse, the warranty don't cover the cost of\n",
      "cleaning up me kitchen. I need yer help right now, matey!\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_email = \"\"\"\n",
    "Arrr, I be fuming that me blender lid flew off and splattered me kitchen walls\n",
    "with smoothie! And to make matters worse, the warranty don't cover the cost of\n",
    "cleaning up me kitchen. I need yer help right now, matey!\n",
    "\"\"\"\n",
    "\n",
    "style = \"\"\"American English in a calm and respectful tone\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"Translate the text \\\n",
    "that is delimited by triple backticks\n",
    "into a style that is {style}.\n",
    "text: ```{customer_email}```\n",
    "\"\"\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1688,
     "status": "ok",
     "timestamp": 1713515087348,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "haWe0_HNys3B",
    "outputId": "355dbeba-c25d-4e23-bc49-0015e9121250"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm upset that my blender lid flew off and splattered my kitchen walls with smoothie. To make matters worse, the warranty doesn't cover the cost of cleaning up my kitchen. I could really use your help right now.\n"
     ]
    }
   ],
   "source": [
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why should we use a framework to wrap the direct calls to OpenAI?    \n",
    "\n",
    "The LangChain pipelines consist of the following modules:\n",
    "\n",
    "+ **Models**: Models mostly cover Large Language models. A large language model of considerable size is a model that comprises a neural network with numerous parameters and is trained on vast quantities of unlabeled text  \n",
    "+ **Prompts**: prompt is the input that we give to any system to refine our answers to make them more accurate or more specific according to our use case. Many times you may want to get more structured information than just text back. We can use Prompts in conjunction with Parsers\n",
    "+ **Parsers**: Output parsers are responsible for taking the output of an LLM and transforming it to a more suitable format. This is very useful when you are using LLMs to generate any form of structured data.  \n",
    "+ **Memory**: Chains and Agents in LangChain operate in a stateless mode by default, meaning that they handle each incoming query independently. However, there are certain applications, like chatbots, where it is of great importance to retain previous interactions, both over the short and long term. This is where the concept of ‚ÄúMemory‚Äù comes into play.  \n",
    "+ **Chains**: Chains provide a means to merge various components into a unified application. A chain can be created, for instance, that receives input from a user, formats it using a PromptTemplate, and subsequently transmits the formatted reply to an LLM. More intricate chains can be generated by integrating multiple chains with other components.  \n",
    "+ **Agents**: Certain applications may necessitate not only a pre-determined sequence of LLM/other tool calls but also an uncertain sequence that is dependent on the user‚Äôs input. These kinds of sequences include an ‚Äúagent‚Äù that has access to a range of tools. Based on the user input, the agent may determine which of these tools, if any, should be called.  \n",
    "+ **Callbacks**: Allow you to hook into the various stages of your LLM application. This is useful for logging, monitoring, streaming, and other tasks.  \n",
    "+ **Indexes**: Indexes are information stored in local databases that can be used for Augment the capabilities of the models being used. We will see them in action with RAG pipelines  \n",
    "\n",
    "Let's wrap the pipeline:\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:4800/format:webp/1*05zEoeNU7DVYOFzjugiF_w.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NkjFnz46y0nQ"
   },
   "source": [
    "\n",
    "# I - Simple Pipelines with Langchain  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1713515364077,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "Okkq2c5w_eI8"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.prompts import HumanMessagePromptTemplate\n",
    "from langchain_core.prompts import ChatMessagePromptTemplate\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.messages import AIMessage\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "import json\n",
    "from pydantic import BaseModel, Field, model_validator, EmailStr, ValidationError\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "executionInfo": {
     "elapsed": 615,
     "status": "ok",
     "timestamp": 1713515361990,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "bh56xPYvyv16"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "model1 = ChatOpenAI(model=\"gpt-5-nano\")\n",
    "model2 = ChatOpenAI(model=\"gpt-4o\")\n",
    "model3 = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "model4 = OllamaLLM(model=\"gemma2:2b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a3I6VXnL0OVp"
   },
   "source": [
    "### Once you've installed and initialized the LLM of your choice, we could use it!   \n",
    "### Let's ask it some question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2939,
     "status": "ok",
     "timestamp": 1713515370013,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "WuKYnH2Jzxb8",
    "outputId": "f4623a5d-d7c8-4501-b6ff-3a491fdd49dc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"The sky appears blue primarily due to a phenomenon called Rayleigh scattering. When sunlight enters the Earth's atmosphere, it is made up of different colors, each with varying wavelengths. Blue light has a shorter wavelength compared to other colors like red or yellow.\\n\\nAs sunlight passes through the atmosphere, it collides with gas molecules and small particles. Because blue light is scattered in all directions more than other colors due to its shorter wavelength, we see a predominance of blue when we look up at the sky.\\n\\nDuring sunrise and sunset, the sun's light has to pass through a thicker layer of the atmosphere, which scatters the shorter blue wavelengths out of our line of sight and allows the longer wavelengths, like red and orange, to dominate, giving the sky those warm colors.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 153, 'prompt_tokens': 13, 'total_tokens': 166, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_29330a9688', 'id': 'chatcmpl-CyQkbtAWIoeT21VKv4hHa2it7ffH6', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019bc3eb-7015-7fe1-8673-8b714d69efc6-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 13, 'output_tokens': 153, 'total_tokens': 166, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.invoke(\"Why is the sky blue?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Because sunlight (which is white) scatters as it passes through Earth‚Äôs atmosphere. The air is full of tiny gas molecules that scatter shorter wavelengths of light (blue and violet) more than longer wavelengths (red, yellow).\\n\\nA few details:\\n- Violet light is scattered even more than blue, but we don‚Äôt see violet in the sky because the sun‚Äôs violet is weak and the atmosphere absorbs much of it, and our eyes are less sensitive to violet.\\n- The blue light that is scattered in all directions reaches our eyes from every part of the sky, so the sky looks blue most of the day.\\n- At sunrise and sunset, sunlight travels through more atmosphere, scattering away the blues and leaving reds/oranges, which is why the sky can look orange or pink then.\\n\\nSo, the sky is blue mainly due to Rayleigh scattering of sunlight by the atmosphere.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 1015, 'prompt_tokens': 12, 'total_tokens': 1027, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 832, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CyQl2nwMSK7b0WbddMxBlIbSq9Ntt', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019bc3eb-d6a8-7543-83c1-6b21ddd228c9-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 12, 'output_tokens': 1015, 'total_tokens': 1027, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 832}})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.invoke(\"Why is the sky blue?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EhhGOhxq0W_W"
   },
   "source": [
    "### Prompt templates convert raw user input to better input to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3054,
     "status": "ok",
     "timestamp": 1713515377035,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "PnJOraQa0E8I",
    "outputId": "32f2b136-dcaa-43f6-f1ab-acd1643374fe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Great question! Sunlight looks white, but it‚Äôs really made of many colors. When the sunlight hits the air in our sky, it bumps into lots of tiny air molecules. Blue light waves get scattered in all directions more than the other colors. So we see blue light coming from everywhere, and the sky looks blue most of the time.\\n\\nSometimes at sunrise or sunset the light has to travel through more air, so the blue is scattered away and we see red, orange, and pink colors instead. On cloudy days the sky can look gray. Want to try a quick color activity at home later?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 1282, 'prompt_tokens': 23, 'total_tokens': 1305, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 1152, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CyQlNDtsrhrayoZUz80hAKh1AioMQ', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019bc3ec-2a65-7f32-8087-af1b59f4b32c-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 23, 'output_tokens': 1282, 'total_tokens': 1305, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 1152}})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([(\"system\", \"You are a first grade teacher.\"),\n",
    "                                           (\"user\", \"{input}\")\n",
    "                                         ])\n",
    "\n",
    "chain = prompt | model1\n",
    "chain.invoke({\"input\": \"Why is the sky blue?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n5cGE6b42KE9"
   },
   "source": [
    "### The output of a ChatModel (and therefore, of this chain) is a message object. However, it's often much more convenient to work with strings.  \n",
    "### Let's add a simple output parser to convert the chat message to a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "executionInfo": {
     "elapsed": 313,
     "status": "ok",
     "timestamp": 1713515382544,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "UsCRlie31g3u"
   },
   "outputs": [],
   "source": [
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "executionInfo": {
     "elapsed": 2646,
     "status": "ok",
     "timestamp": 1713515388989,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "46h0vZIf2PDV",
    "outputId": "c5c35513-e44e-4d8f-c235-df4557a0c432"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Great question! Here‚Äôs a simple way to think about it:\\n\\n- Sunlight looks white, but it‚Äôs really made of many colors.\\n- As the sunlight travels through the air, the tiny particles in the air scatter blue light in all directions.\\n- So we see the sky as blue everywhere we look most of the day.\\n- At sunrise and sunset, the light travels through more air, so the blue light gets scattered away and we see reds and oranges.\\n\\nA quick safety note: don‚Äôt look directly at the Sun.\\n\\nIf you want, we can do a tiny kid-friendly demo in class to show how light and colors work. Can you tell me what color you see in the sky today?'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | model1 | output_parser\n",
    "chain.invoke({\"input\": \"Why is the sky blue?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3MHk0lzH3fuN"
   },
   "source": [
    "# II - Exploring Chain Elements - Langchain Expression Language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d3hI0QJe45VY"
   },
   "source": [
    "Notice this line of the code, where we piece together these different components into a single chain using LCEL:  \n",
    "\n",
    "**chain = prompt | model | output_parser**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XW6Fd-Ko4YvE"
   },
   "source": [
    "## 1. [Prompts](https://python.langchain.com/docs/modules/model_io/prompts/)    \n",
    "prompt is a BasePromptTemplate, which means it takes in a dictionary of template variables and produces a PromptValue.  \n",
    "+ A PromptValue is a wrapper around a completed prompt that can be passed to either a LLM (which takes a string as input) or ChatModel (which takes a sequence of messages as input).  \n",
    "+ It can work with either language model type because it defines logic both for producing BaseMessages and for producing a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 1316,
     "status": "ok",
     "timestamp": 1713474572510,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "E5ycNd1q3tcA",
    "outputId": "a8c073ad-6640-4ce9-e685-8f99d563c564"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why did the chicken join a band? Because it had the drumsticks. Want another one?'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"tell me a short joke about {topic}\")\n",
    "\n",
    "chain = prompt | model1 | output_parser\n",
    "\n",
    "chain.invoke({\"topic\": \"chicken\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 221,
     "status": "ok",
     "timestamp": 1713474583609,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "p5s4zuDi4aFq",
    "outputId": "2a4a9515-797a-4517-ed23-495586dbf66c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content='tell me a short joke about ice cream', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_value = prompt.invoke({\"topic\": \"ice cream\"})\n",
    "prompt_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 213,
     "status": "ok",
     "timestamp": 1713474593988,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "u1hBd2io9dWI",
    "outputId": "adb45ee8-0068-4202-e85d-e9d15c25b9e7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='tell me a short joke about ice cream', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_value.to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 245,
     "status": "ok",
     "timestamp": 1713474597165,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "binqpy-79f4z",
    "outputId": "19d2ddf6-5aaf-45ff-e61b-ca722244e7da"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: tell me a short joke about ice cream'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_value.to_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "executionInfo": {
     "elapsed": 2380,
     "status": "ok",
     "timestamp": 1713474723187,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "bz5qa_TCDN4d",
    "outputId": "faa4f7a3-22d3-4e24-c416-4d281ebda444"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Great question! Here‚Äôs a simple way to think about it:\\n\\n- The sun shines white light, which is made of all the colors.\\n- The air in our sky has tiny particles. When sunlight hits them, the light gets bounced around in many directions. This is called scattering.\\n- Blue light waves are shorter, so they scatter the most. That blue light goes everywhere in the sky, so the sky looks blue most of the time.\\n- At sunrise or sunset, the sun‚Äôs light travels through more air. Lots of blue light gets scattered away, and we see warm colors like red and orange.\\n\\nSo, blue sky because blue light scatters the most in our air. Want to draw a blue sky or try a simple color-scattering activity?'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([(\"system\", \"You are a first grade teacher.\"),\n",
    "                                           (\"user\", \"{input}\")\n",
    "                                         ])\n",
    "chain = prompt | model1 | output_parser\n",
    "chain.invoke({\"input\": \"Why is the sky blue?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rGLNmBQy5B8S"
   },
   "source": [
    "## 2. [Models](https://python.langchain.com/docs/modules/model_io/chat/quick_start/)    \n",
    "#### The PromptValue is then passed to model.  \n",
    "#### In this case our model is an OpenAI ChatModel, meaning it will output a BaseMessage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1433,
     "status": "ok",
     "timestamp": 1713473317439,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "X2wt5_jf4eEe",
    "outputId": "4f2a654e-77fe-4f9c-d29e-4c60fcb2a29d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Why did the ice cream cone break up with the sundae? \\n\\nBecause it found someone cooler!' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 15, 'total_tokens': 35, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_29330a9688', 'id': 'chatcmpl-CyQmXPMBPa4cE607JkFKGXaXiVKuW', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--019bc3ed-43bf-7253-be28-76a14d93ffbb-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 15, 'output_tokens': 20, 'total_tokens': 35, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "<class 'langchain_core.messages.ai.AIMessage'>\n"
     ]
    }
   ],
   "source": [
    "message = model3.invoke(prompt_value)\n",
    "print(message)\n",
    "print(type(message))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZCME5D_A5VTc"
   },
   "source": [
    "#### If our model was an Ollama LLM, it would output a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the ice cream cone go to the doctor? \n",
      "\n",
      "Because it was feeling crumby! üòÇüç¶ \n",
      "\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "message = model4.invoke(prompt_value)\n",
    "print(message)\n",
    "print(type(message))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 1237,
     "status": "ok",
     "timestamp": 1713473332520,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "yREwkxMj5J5E",
    "outputId": "e27443f4-67be-4694-d19d-ffeee3b8ac6f"
   },
   "source": [
    "#### That is why we should treat the model output with an output parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GEeoPLa652D3"
   },
   "source": [
    "## 3. [Output parsers](https://python.langchain.com/docs/modules/model_io/output_parsers/)    \n",
    "And lastly we pass our model output to the output_parser, which is a BaseOutputParser meaning it takes either a string or a BaseMessage as input. The specific StrOutputParser simply converts any input into a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 199,
     "status": "ok",
     "timestamp": 1713473342078,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "Kt6xF6wt5THU",
    "outputId": "c6ff9d82-73a0-4805-cbbf-e9aab16ffe3d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why did the ice cream cone break up with the sundae? \\n\\nBecause it found someone cooler!'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parser.invoke(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n"
     ]
    }
   ],
   "source": [
    "print(type(message))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vifudtoO6Kt2"
   },
   "source": [
    "## 4. [Chains](https://python.langchain.com/v0.1/docs/modules/chains/)  \n",
    "\n",
    "#### Chains are combination of steps. We have followed the steps along:\n",
    "\n",
    "+ We passed the user input on the desired topic as {\"topic\": \"ice cream\"}\n",
    "+ The prompt component takes the user input, which is then used to construct a PromptValue after using the topic to construct the prompt.\n",
    "+ The model component takes the generated prompt, and passes into the OpenAI LLM model for evaluation.\n",
    "+ The generated output from the model is a ChatMessage object.\n",
    "+ Finally, the output_parser component takes in a ChatMessage, and transforms this into a Python string, which is returned from the invoke method.\n",
    "\n",
    "#### Langchain has many types of chains using [LCEL](https://python.langchain.com/docs/how_to/#langchain-expression-language-lcel). We are just exploring some in this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3K1Ovj-EZr7"
   },
   "source": [
    "# 5. [Types of Prompts](https://python.langchain.com/docs/modules/model_io/prompts/quick_start/)  \n",
    "#### Let's explore some other types of Prompts  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xqYB7XnjEqJV"
   },
   "source": [
    "## 5.1 PromptTemplate\n",
    "\n",
    "Use PromptTemplate to create a template for a string prompt.  \n",
    "By default, PromptTemplate uses Python‚Äôs str.format syntax for templating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 250,
     "status": "ok",
     "timestamp": 1713515601642,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "GEsb8EBw57ld",
    "outputId": "2a759fbf-62b8-4850-a4d5-7b288ba70dc8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell me a funny joke about chickens.'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = PromptTemplate.from_template(\"Tell me a {adjective} joke about {content}.\")\n",
    "prompt_template.format(adjective=\"funny\", content=\"chickens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 291,
     "status": "ok",
     "timestamp": 1713515623928,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "MR0Ip8O6E1db",
    "outputId": "d64d8ca5-92fd-4381-9311-06879338c46c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell me a joke'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = PromptTemplate.from_template(\"Tell me a joke\")\n",
    "prompt_template.format()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4zLArLmYFBK1"
   },
   "source": [
    "## 5.2 ChatPromptTemplate\n",
    "\n",
    "The prompt to chat models/ is a list of chat messages.  \n",
    "Each chat message is associated with content, and an additional parameter called role. For example, in the OpenAI Chat Completions API, a chat message can be associated with an AI assistant, a human or a system role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 222,
     "status": "ok",
     "timestamp": 1713476275035,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "2jUJRy4RE7bL",
    "outputId": "eed9ebea-f868-418f-8ad8-c31efdae3426"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful AI bot. Your name is Bob.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Hello, how are you doing?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"I'm doing well, thanks!\", additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[]),\n",
       " HumanMessage(content='What is your name?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
    "        (\"human\", \"Hello, how are you doing?\"),\n",
    "        (\"ai\", \"I'm doing well, thanks!\"),\n",
    "        (\"human\", \"{user_input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "messages = chat_template.format_messages(name=\"Bob\", user_input=\"What is your name?\")\n",
    "messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "erMamCwOFWiY"
   },
   "source": [
    "#### This is equivalent as doing the following, directly with OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "Rsgp2i-HFOT9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-CyQn4YiisuIMeXurWu1NRQvxLlEiF', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='My name is Bob. How can I assist you today?', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1768518698, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_29330a9688', usage=CompletionUsage(completion_tokens=12, prompt_tokens=49, total_tokens=61, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful AI bot. Your name is Bob.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Hello, how are you doing?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"I'm doing well, thanks!\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is your name?\"},\n",
    "    ],\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SnzZFr30I1tt"
   },
   "source": [
    "The ChatPromptTemplate.from_messages static method accepts a variety of message representations and is a convenient way to format input to chat models with exactly the messages you want.  \n",
    "\n",
    "For example, in addition to using the 2-tuple representation of (type, content) used above, you could pass in an instance of MessagePromptTemplate or BaseMessage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 221,
     "status": "ok",
     "timestamp": 1713476445408,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "GkIPB2MsI5XR",
    "outputId": "cf7bd703-c86b-4038-ada4-e7ba2797959e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content=\"You are a helpful assistant that re-writes the user's text to sound more upbeat.\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"I don't like eating tasty things\", additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [SystemMessage(content=(\"You are a helpful assistant that re-writes the user's text to \"\n",
    "                            \"sound more upbeat.\")),\n",
    "     HumanMessagePromptTemplate.from_template(\"{text}\"),\n",
    "    ])\n",
    "\n",
    "\n",
    "messages = chat_template.format_messages(text=\"I don't like eating tasty things\")\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48LkjjFSKEah"
   },
   "source": [
    "## 5.3 Message Prompts  \n",
    "LangChain provides different types of MessagePromptTemplate. The most commonly used are\n",
    "+ AIMessagePromptTemplate  \n",
    "+ SystemMessagePromptTemplate  \n",
    "+ HumanMessagePromptTemplate  \n",
    "\n",
    "Which create an AI message, system message and human message respectively.  \n",
    "In cases where the chat model supports taking chat message with arbitrary role, you can use ChatMessagePromptTemplate, which allows user to specify the role name.  \n",
    "https://python.langchain.com/docs/modules/model_io/chat/message_types/  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 224,
     "status": "ok",
     "timestamp": 1713479238486,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "CD6XMqdiJicg",
    "outputId": "8e2700cf-29ff-4341-e6c2-e2237f3ea44b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatMessage(content='May the force be with you', additional_kwargs={}, response_metadata={}, role='Jedi')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"May the {subject} be with you\"\n",
    "\n",
    "chat_message_prompt = ChatMessagePromptTemplate.from_template(role=\"Jedi\", template=prompt)\n",
    "chat_message_prompt.format(subject=\"force\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "exBvBGtwUote"
   },
   "source": [
    "# 5.4 MessagesPlaceholder  \n",
    "LangChain also provides MessagesPlaceholder, which gives you full control of what messages to be rendered during formatting. This can be useful when you are uncertain of what role you should be using for your message prompt templates or when you wish to insert a list of messages during formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 274,
     "status": "ok",
     "timestamp": 1713479503244,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "SecPlj7fKjVY",
    "outputId": "70c306a1-6724-49dd-ae70-3b789b775947"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='What is the best way to learn programming?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='1. Choose a programming language: Decide on a programming language that you want to learn.\\n2. Start with the basics: Familiarize yourself with the basic programming concepts such as variables, \\ndata types and control structures.\\n3. Practice, practice, practice: The best way to learn programming is through hands-on experience', additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[]),\n",
       " HumanMessage(content='Summarize our conversation so far in 10 words.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_prompt = \"Summarize our conversation so far in {word_count} words.\"\n",
    "human_message_template = HumanMessagePromptTemplate.from_template(human_prompt)\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [MessagesPlaceholder(variable_name=\"conversation\"),\n",
    "     human_message_template]\n",
    ")\n",
    "\n",
    "human_message = HumanMessage(content=\"What is the best way to learn programming?\")\n",
    "ai_message = AIMessage(content=\"\"\"\\\n",
    "1. Choose a programming language: Decide on a programming language that you want to learn.\n",
    "2. Start with the basics: Familiarize yourself with the basic programming concepts such as variables, \n",
    "data types and control structures.\n",
    "3. Practice, practice, practice: The best way to learn programming is through hands-on experience\\\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "chat_prompt.format_prompt(conversation=[human_message, ai_message], word_count=\"10\").to_messages()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eg7X0M2hYKgd"
   },
   "source": [
    "# 6 - [Types of Output Parsers](https://python.langchain.com/docs/modules/model_io/output_parsers/)  \n",
    "\n",
    "Language models output text. But many times you may want to get more structured information than just text back. This is where output parsers come in. Output parsers are classes that help structure language model responses.  \n",
    "There are two main methods an output parser must implement:  \n",
    "+ ‚ÄúGet format instructions‚Äù: A method which returns a string containing instructions for how the output of a language model should be formatted.\n",
    "+ ‚ÄúParse‚Äù: A method which takes in a string (assumed to be the response from a language model) and parses it into some structure.\n",
    "\n",
    "And then one optional one:\n",
    "\n",
    "+ ‚ÄúParse with prompt‚Äù: A method which takes in a string (assumed to be the response from a language model) and a prompt (assumed to be the prompt that generated such a response) and parses it into some structure. The prompt is largely provided in the event the OutputParser wants to retry or fix the output in some way, and needs information from the prompt to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BZHW6BBHh5OT"
   },
   "source": [
    "#### Let's explore the **CSV Parser**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "executionInfo": {
     "elapsed": 1022,
     "status": "ok",
     "timestamp": 1713515934432,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "juqjG-q5h_zb"
   },
   "outputs": [],
   "source": [
    "output_parser = CommaSeparatedListOutputParser()\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "prompt = PromptTemplate(template=\"List five {subject}.\\n{format_instructions}\",\n",
    "                        input_variables=[\"subject\"],\n",
    "                        partial_variables={\"format_instructions\": format_instructions},\n",
    ")\n",
    "\n",
    "chain = prompt | model1 | output_parser\n",
    "a = chain.invoke({\"subject\": \"ice cream flavors\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vanilla', 'chocolate', 'strawberry', 'mint chocolate chip', 'cookies and cream']\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(a)\n",
    "print(type(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 762,
     "status": "ok",
     "timestamp": 1713483705005,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "4FuMrePlllnR",
    "outputId": "4aa6ded2-d67b-4ac9-8371-40b1d5a470d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vanilla']\n",
      "['chocolate']\n",
      "['strawberry']\n",
      "['mint chocolate chip']\n",
      "['cookies and cream']\n"
     ]
    }
   ],
   "source": [
    "for s in chain.stream({\"subject\": \"ice cream flavors\"}):\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o2cWky88aEpC"
   },
   "source": [
    "#### Below we go over a more powerful output parser, the **PydanticOutputParser**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lb5BA6iGaaBd"
   },
   "source": [
    "#### a) Define your desired data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserInfo(BaseModel):\n",
    "    name: str\n",
    "    age: int\n",
    "    email: EmailStr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Create a parser instance for the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = PydanticOutputParser(pydantic_object=UserInfo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Create a prompt that formats the output in the required structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "Extract the following information from the text:\n",
    "\n",
    "name: The full name of the user.\n",
    "age: The user's age (as an integer).\n",
    "email: The user's email address.\n",
    "\n",
    "Return the data in JSON format. \n",
    "\n",
    "Here's the text:\n",
    "{text}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d) Set up the Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=prompt_template\n",
    ")\n",
    "\n",
    "# Example text input\n",
    "text_input = \"John Doe, a 30-year-old, can be reached at john.doe@example.com.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e) And a query intended to prompt a language model to populate the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"name\": \"John Doe\",\n",
      "  \"age\": 30,\n",
      "  \"email\": \"john.doe@example.com\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "prompt_and_model = prompt | model3\n",
    "output = prompt_and_model.invoke({\"text\": text_input})\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UserInfo(name='John Doe', age=30, email='john.doe@example.com')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = parser.invoke(output)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"name\":\"John Doe\",\"age\":30,\"email\":\"john.doe@example.com\"}'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.model_dump_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A more complex example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Define your desired data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "executionInfo": {
     "elapsed": 263,
     "status": "ok",
     "timestamp": 1713516574944,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "suvoyAIIYnbk"
   },
   "outputs": [],
   "source": [
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "    # You can add custom validation logic easily with Pydantic.\n",
    "    @model_validator(mode=\"before\")\n",
    "    @classmethod\n",
    "    def question_ends_with_question_mark(cls, values: dict) -> dict:\n",
    "        setup = values.get(\"setup\")\n",
    "        if setup and setup[-1] != \"?\":\n",
    "            raise ValueError(\"Badly formed question!\")\n",
    "        return values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pFiNe5uxai_b"
   },
   "source": [
    "#### b) Set up a parser and the prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "executionInfo": {
     "elapsed": 243,
     "status": "ok",
     "timestamp": 1713516578193,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "tLwcQWypagFB"
   },
   "outputs": [],
   "source": [
    "parser = PydanticOutputParser(pydantic_object=Joke)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rGshQwP-a2Xt"
   },
   "source": [
    "#### c) And a query intended to prompt a language model to populate the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1232,
     "status": "ok",
     "timestamp": 1713516602581,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "jAGBOHFwa3Sa",
    "outputId": "187bb9cf-2995-4359-d9b0-c69405767b0e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Joke(setup='Why did the scarecrow win an award?', punchline='Because he was outstanding in his field.')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_and_model = prompt | model1\n",
    "output = prompt_and_model.invoke({\"query\": \"Tell me a joke.\"})\n",
    "\n",
    "result = parser.invoke(output)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"setup\":\"Why did the scarecrow win an award?\",\"punchline\":\"Because he was outstanding in his field.\"}'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.model_dump_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 - [Memory](https://python.langchain.com/v0.1/docs/modules/memory/)  \n",
    "\n",
    "Most LLM applications have a conversational interface. An essential component of a conversation is being able to refer to information introduced earlier in the conversation. At bare minimum, a conversational system should be able to access some window of past messages directly. A more complex system will need to have a world model that it is constantly updating, which allows it to do things like maintain information about entities and their relationships.\n",
    "\n",
    "TBD - Completely revamped in Langchain"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNRBbpHqLlsR3qsDTCKjs0x",
   "provenance": [
    {
     "file_id": "1LtCkSgnYYJO5Q4IxLEbUqn4Chmc-vJPw",
     "timestamp": 1713469816474
    }
   ]
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
