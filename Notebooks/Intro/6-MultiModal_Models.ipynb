{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bdac04e7-ffd5-49c2-98d4-1be75fd5b3c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Multimodal Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8a34984-db08-4793-b312-ef8390a3990f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install -qU openai\n",
    "!pip install pdf2image\n",
    "!apt-get install -y poppler-utils\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63567bd4-0bfc-4274-a975-f03d1edf4835",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import time\n",
    "import openai\n",
    "import base64\n",
    "import requests\n",
    "import textwrap\n",
    "from pdf2image import convert_from_path\n",
    "import PIL\n",
    "\n",
    "from IPython.display import Image, Audio, Markdown, Math\n",
    "from openai import OpenAI\n",
    "from openai import AzureOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb34d3e6-0319-4e88-bca9-a4559c090f85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "openai.api_type = \"azure\"\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "openai.api_version = \"2023-07-01-preview\"\n",
    "#openai.api_base = \"https://rg-rbi-aa-aitest-dsacademy.openai.azure.com/\"\n",
    "openai.api_base = \"https://chatgpt-summarization.openai.azure.com/\"\n",
    "\n",
    "llm_model_name = \"gpt-4o\"\n",
    "llm_deploy_name = \"pioneers-gpt-4o\"\n",
    "\n",
    "client = AzureOpenAI(api_key=openai.api_key,\n",
    "                     api_version=openai.api_version,\n",
    "                     azure_endpoint=openai.api_base,\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c72b57bd-299a-4b95-b3e4-0f996b53f58e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Dealing with images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9bd460e2-22ab-4cd1-831b-4b1fc71d50c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Opening Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6699d5c5-a8b1-4daf-882c-0e16c554cfe1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "filename = \"../../Data/handwritten.jpg\"\n",
    "Image(filename=filename, width=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ccb5e63-5283-41d9-a84f-abcc806fbbb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Encoding Image (base64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b7e4287-25c5-4073-a1a0-6ea443826230",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def encode_image(input_image, pdf_page=0):\n",
    "    if isinstance(input_image, str):\n",
    "        # Check if the file is a PDF\n",
    "        if input_image.lower().endswith('.pdf'):\n",
    "            pages = convert_from_path(input_image, dpi=300, poppler_path='/usr/bin')\n",
    "            # Assuming you want to encode the first page\n",
    "            input_image = pages[pdf_page]\n",
    "        else:\n",
    "            with open(input_image, \"rb\") as image_file:\n",
    "                return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "    \n",
    "    if isinstance(input_image, PIL.Image.Image):\n",
    "        with io.BytesIO() as buffer:\n",
    "            input_image.save(buffer, format=\"PNG\")  # Save in PNG format or any other\n",
    "            byte_data = buffer.getvalue()\n",
    "            return base64.b64encode(byte_data).decode(\"utf-8\")\n",
    "    else:\n",
    "        raise ValueError(\"Input must be a file path or a PIL image object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e4412b6-89f5-4aa5-a095-886d793da382",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "myimage = encode_image(filename)\n",
    "myimage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e45e868-0bf5-4338-ac2d-46117f049c04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Sending request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fd50c9d-d2c6-4aeb-9763-354789a2e978",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"Can you transcribe this text?\"\n",
    "\n",
    "messages=[\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"text\",\n",
    "          \"text\": f\"{prompt}\"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"image_url\",\n",
    "          \"image_url\": {\n",
    "            \"url\": f\"data:image/png;base64,{myimage}\"\n",
    "          }\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "  ]\n",
    "\n",
    "response = client.chat.completions.create(model=llm_deploy_name,\n",
    "                                          messages=messages,\n",
    "                                          temperature=0.5,\n",
    "                                          max_tokens=256,\n",
    "                                          top_p=1,\n",
    "                                          frequency_penalty=0,\n",
    "                                          presence_penalty=0,\n",
    "                                          #stop=None,\n",
    "                                          )\n",
    "\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ddd690ed-f20c-4903-90d3-529c91ec2ca8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## PDF Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee6418aa-4d9b-452f-acd0-d2163563f9cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Encoding the Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b11c195-f92e-4f51-b44d-e64a8df56747",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "filename = \"../../Data/pdf/NASDAQ_AMZN_2019.pdf\"\n",
    "encode_image(filename, pdf_page=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9223babf-f2cd-4cba-bb95-e9e9cd8c5fc9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Displaying the PDF file (sanity check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8e57ac3-a203-42b8-af83-cd7d6b6a05f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "filename = \"../../Data/pdf/NASDAQ_AMZN_2019.pdf\"\n",
    "pages = convert_from_path(filename, dpi=300, poppler_path='/usr/bin')\n",
    "encode_image(pages[1])\n",
    "with io.BytesIO() as buffer:\n",
    "    pages[1].save(buffer, format=\"PNG\") \n",
    "    byte_data = buffer.getvalue()\n",
    "\n",
    "Image(byte_data, width=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b1be939-9f62-4cc2-b057-91f7bd6a81b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Sending request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a424040-4cc5-4dc3-b0d6-0551d9589aa7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "base64_encoded = encode_image(pages[1])\n",
    "prompt = \"Can you transcribe and translate to German?\"\n",
    "messages=[\n",
    "    {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": f\"{prompt}\"}]},\n",
    "    {\"role\": \"user\", \"content\": [{\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/png;base64,{base64_encoded}\"}}]},\n",
    "          ]\n",
    "\n",
    "try:\n",
    "    response = client.chat.completions.create(\n",
    "        model=llm_deploy_name,\n",
    "        messages=messages,\n",
    "        temperature=1,\n",
    "        # max_tokens=256,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "    )\n",
    "    print(response.choices[0].message.content)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "802c0c8b-14f3-4819-b004-1392407e8657",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Wrapping everything in a function  \n",
    "\n",
    "Example usage:\n",
    "\n",
    "+ **For image file path**  \n",
    "image_file_path = 'path/to/image.png'  \n",
    "prompt = \"Can you transcribe this text?\"  \n",
    "result = process_image(image_file_path, prompt)  \n",
    "print(result)  \n",
    "\n",
    "+ **For PDF file path**  \n",
    "pdf_file_path = 'path/to/document.pdf'  \n",
    "result = process_image(pdf_file_path, prompt)  \n",
    "print(result)  \n",
    "\n",
    "+ **For PIL image object**  \n",
    "image_obj = Image.open('path/to/image.png')  \n",
    "result = process_image(image_obj, prompt)  \n",
    "print(result)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6f5e3e2-4ae6-4e36-8c12-49a9fb81bf92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_image(input_image, prompt, pdf_page=0, detail=\"low\"):\n",
    "    base64_encoded = encode_image(input_image)\n",
    "    \n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": f\"{prompt}\"\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/png;base64,{base64_encoded}\",\n",
    "                        \"detail\": detail  # You can set this to \"low\", \"high\", or \"auto\"\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=llm_deploy_name,\n",
    "            messages=messages,\n",
    "            temperature=1,\n",
    "            max_tokens=256,\n",
    "            top_p=1,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87dc9663-77ce-45b7-91bd-83d2acf43597",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Depending of the hanwriting, the results may not be perfect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0acf213-0b0f-48d4-bb96-54abcc1d5678",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "filename = \"../../Data/handwritten2.jpg\"\n",
    "Image(filename=filename, width=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71de9b19-624e-45f6-8079-c27740b48926",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"Can you transcribe this text?\"\n",
    "print(process_image(filename, prompt, detail=\"high\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b19be29-10cc-4359-bcc4-d4e4313904e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "202f49e5-e66d-484e-9486-e7aa8a931fe5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "filename = \"../../Data/handwritten2.jpg\"\n",
    "prompt = \"Can you translate this text to English?\"\n",
    "print(process_image(filename, prompt, detail=\"high\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "800d60d8-8715-4103-aea1-86106225e4b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba57262a-5244-4c1f-bc29-7baf80239626",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"What is the name of the brand in this PDF Page?\"\n",
    "filename = \"../../Data/pdf/NASDAQ_AMZN_2019.pdf\"\n",
    "print(process_image(filename, prompt, pdf_page=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42d7f833-c5cf-4388-90fb-19d65b0c718b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69a8f9dd-0a0f-4bda-b3a1-3985fbae5ccd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "filename = \"../../Data/fossils.jpeg\"\n",
    "Image(filename=filename, width=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27e141e7-fcf9-4454-ae81-690f06e5a330",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"Can you describe this image?\"\n",
    "print(process_image(filename, prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1129cc3c-8f72-4ad9-a9b3-333fb79e7a19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40c28fe4-1e25-48b6-b021-4bfda2822c4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "filename = \"../../Data/Sign.png\"\n",
    "Image(filename=filename, width=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa9b32de-ddd4-482a-b6c1-176b116d959c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"What is the type of tree behind the sign? Answer in one line\"\n",
    "print(process_image(filename, prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "807bc0f8-6791-43e9-b190-8d581a9a3e00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Technical Aspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68dd6825-9933-4c9c-9592-358dbe4464ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "filename = \"../../Data/Lightroom.jpeg\"\n",
    "Image(filename=filename, width=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "047af486-50bf-4be2-9443-17992ff51150",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"Tell me what my Lighthouse settings should be to get this type of filter. I need numeric values. It is ok to approximate. Think step by step\"\n",
    "print(process_image(filename, prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "265cd3d6-610e-416e-9fd4-2736b976a222",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### A Math problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75dd896e-29d9-411c-8829-6160574d9054",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "filename = \"../../Data/triangle.png\"\n",
    "Image(filename=filename, width=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6acce6d1-187d-48d9-a00b-3d1b194fa69d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "base64_image = encode_image(filename)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=llm_deploy_name,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that responds in Markdown. Help me with my math homework!\"},\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"What's the area of the triangle?\"},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\n",
    "                \"url\": f\"data:image/png;base64,{base64_image}\"}\n",
    "            }\n",
    "        ]}\n",
    "    ],\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "Markdown(response.choices[0].message.content)\n",
    "#displayHTML(f\"$${response.choices[0].message.content}$$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66a07f22-9446-44d7-8805-eb8512e1793b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Using a URL instead of an image:  \n",
    "(no need to encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e2c9baf-800a-43bf-a93e-42d3053acfdb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Image(url=\"https://upload.wikimedia.org/wikipedia/commons/e/e2/The_Algebra_of_Mohammed_Ben_Musa_-_page_82b.png\")\n",
    "Image(url=\"https://i.ytimg.com/vi/nTB6pdf2ae4/hq720.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "627ada46-cf9b-4179-9750-b4f4a7e8f94d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=llm_deploy_name,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that responds in Markdown. Help me with my math homework!\"},\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"Can you solve this problem?\"},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\n",
    "                \"url\": \"https://i.ytimg.com/vi/nTB6pdf2ae4/hq720.jpg\"}\n",
    "            }\n",
    "        ]}\n",
    "    ],\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b77e0db-a9ad-4dc1-98cf-24613074bd13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### A full pipeline: Working with many images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5dec57e-701c-4a64-8f54-6c377d404886",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### A function to resize the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59b76598-d549-460e-9242-1296634868fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def resize_image(image, max_dimension):\n",
    "    width, height = image.size\n",
    "    \n",
    "    # Check if the image has a palette and convert it to true color mode\n",
    "    if image.mode == \"P\":\n",
    "        if \"transparency\" in image.info:\n",
    "            image = image.convert(\"RGBA\")\n",
    "        else:\n",
    "            image = image.convert(\"RGB\")\n",
    "\n",
    "    if width > max_dimension or height > max_dimension:\n",
    "        if width > height:\n",
    "            new_width = max_dimension\n",
    "            new_height = int(height * (max_dimension / width))\n",
    "        else:\n",
    "            new_height = max_dimension\n",
    "            new_width = int(width * (max_dimension / height))\n",
    "        image = image.resize((new_width, new_height), PIL.Image.LANCZOS)\n",
    "        timestamp = time.time()\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c832ae36-6c03-4893-b549-fff88d943c27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### A function to convert to PNG format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7ece76e-f096-4e57-95a4-e4858017bada",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def convert_to_png(image):\n",
    "    with io.BytesIO() as output:\n",
    "        image.save(output, format=\"PNG\")\n",
    "        return output.getvalue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3037d223-8bd0-45ed-8275-b4407826de72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Processing = converting, resizing and encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a9eead2-683d-4a98-a487-32a25d84bfd4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_image(path, max_size):\n",
    "    with PIL.Image.open(path) as image:\n",
    "        width, height = image.size\n",
    "        mimetype = image.get_format_mimetype()\n",
    "        if mimetype == \"image/png\" and width <= max_size and height <= max_size:\n",
    "            with open(path, \"rb\") as f:\n",
    "                encoded_image = base64.b64encode(f.read()).decode('utf-8')\n",
    "                return (encoded_image, max(width, height))  # returns a tuple consistently\n",
    "        else:\n",
    "            resized_image = resize_image(image, max_size)\n",
    "            png_image = convert_to_png(resized_image)\n",
    "            return (base64.b64encode(png_image).decode('utf-8'),\n",
    "                    max(width, height)  # same tuple metadata\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1694a4d-2a0a-438b-90a1-197af2e087ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### A function to decide if the image will be treated as low or high definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbcd9aa9-5a9d-4bf1-acb1-48c5d00a40a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_image_content(image, maxdim, detail_threshold):\n",
    "    detail = \"low\" if maxdim < detail_threshold else \"high\"\n",
    "    return {\n",
    "        \"type\": \"image_url\",\n",
    "        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image}\", \"detail\": detail}\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59e0310f-020e-45ba-a557-341663658ce3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Setting the System and User messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b994943d-f0f9-48ab-afb4-ab3c80e8e995",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def set_system_message(sysmsg):\n",
    "    return [{\n",
    "        \"role\": \"system\",\n",
    "        \"content\": {\"text\": sysmsg}\n",
    "    }]\n",
    "\n",
    "def set_user_message(user_msg_str,\n",
    "                     file_path_list=[],      # A list of file paths to images.\n",
    "                     max_size_px=1024,       # Shrink images for lower API consumption\n",
    "                     file_names_list=None,   # You can set original upload names to show AI\n",
    "                     tiled=False,            # True is the API Reference method\n",
    "                     detail_threshold=700):  # any images below this get 512px \"low\" mode\n",
    "\n",
    "    if not isinstance(file_path_list, list):  # create empty list for weird input\n",
    "        file_path_list = []\n",
    "\n",
    "    if not file_path_list:  # no files, no tiles\n",
    "        tiled = False\n",
    "\n",
    "    if file_names_list and len(file_names_list) == len(file_path_list):\n",
    "        file_names = file_names_list\n",
    "    else:\n",
    "        file_names = [os.path.basename(path) for path in file_path_list]\n",
    "\n",
    "    base64_images = [process_image(path, max_size_px) for path in file_path_list]\n",
    "\n",
    "    uploaded_images_text = \"\"\n",
    "    if file_names:\n",
    "        uploaded_images_text = \"\\n\\n---\\n\\nUploaded images:\\n\" + '\\n'.join(file_names)\n",
    "\n",
    "    if tiled:\n",
    "        content = [{\"type\": \"text\", \"text\": user_msg_str + uploaded_images_text}]\n",
    "        content += [create_image_content(image, maxdim, detail_threshold)\n",
    "                    for image, maxdim in base64_images]\n",
    "        return [{\"role\": \"user\", \"content\": {\"messages\": content}}]\n",
    "    else:\n",
    "        return [{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": {\"messages\": [user_msg_str + uploaded_images_text]\n",
    "                        + [{\"image\": image} for image, _ in base64_images]}\n",
    "          }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e245e86-4f04-44d8-a432-d4bd7dcf7581",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "system_msg = \"\"\"\n",
    "You are VisionPal, an AI assistant powered by GPT-4o with\n",
    "Built-in vision capabilities:\n",
    "- extract text from image\n",
    "- describe images\n",
    "- analyze image contents\n",
    "- logical problem-solving requiring machine vision\n",
    "\"\"\".strip()\n",
    "\n",
    "user_msg = \"\"\"\n",
    "How many images were received?\n",
    "Describe the contents.\n",
    "Describe the quality.\n",
    "Repeat back the file names sent.\n",
    "\"\"\".strip()\n",
    "\n",
    "max_size = 512  # downsizes if any dimension above this\n",
    "image_paths = [\"../../Data/handwritten.jpg\", \"../../Data/handwritten2.jpg\", \"../../Data/handwritten3.jpg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bfbeb07-1fbb-467a-8ca4-c897ec00cbe4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "system = set_system_message(system_msg)\n",
    "chat_hist = []  # list of more user/assistant items\n",
    "user = set_user_message(user_msg, image_paths, max_size)\n",
    "\n",
    "params = {\n",
    "  \"model\": llm_deploy_name, \n",
    "  \"temperature\": 0.5, \n",
    "  \"user\": \"my_customer\",\n",
    "  \"max_tokens\": 500, \n",
    "  \"top_p\": 0.5, \n",
    "  \"stream\": True,\n",
    "  \"messages\": system + chat_hist + user,\n",
    "}\n",
    "\n",
    "# Fixing malformed parameters. Can be refactored\n",
    "\n",
    "if isinstance(params[\"messages\"][0][\"content\"], dict):\n",
    "    params[\"messages\"][0][\"content\"] = params[\"messages\"][0][\"content\"][\"text\"]\n",
    "\n",
    "if isinstance(params[\"messages\"][1][\"content\"], dict):\n",
    "    text_part = params[\"messages\"][1][\"content\"][\"messages\"][0]  # Extract text\n",
    "    image_part = params[\"messages\"][1][\"content\"][\"messages\"][1]  # Extract image\n",
    "    params[\"messages\"][1][\"content\"] = [\n",
    "        {\"type\": \"text\", \"text\": text_part},\n",
    "        {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/png;base64,{image_part['image']}\"}}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "749a83aa-504f-44da-8857-02502e3e99c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Making the call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c30c6973-e816-4037-9d98-4903f1ce3af1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "start = time.perf_counter()\n",
    "try:\n",
    "    client = AzureOpenAI(api_key=openai.api_key,\n",
    "                         api_version=openai.api_version,\n",
    "                         azure_endpoint=openai.api_base,\n",
    "                     )\n",
    "    response = client.chat.completions.create(**params)\n",
    "\n",
    "    if params.get(\"stream\", False):  # Check if streaming\n",
    "        reply = \"\"\n",
    "        print(f\"---\\nSENT:\\n{params['messages'][-1]['content'][0]['text']}\\n---\")\n",
    "    \n",
    "    for chunk in response:\n",
    "        if hasattr(chunk, \"choices\") and chunk.choices:  # Ensure choices exist\n",
    "            delta = chunk.choices[0].delta\n",
    "            if hasattr(delta, \"content\") and delta.content:\n",
    "                reply += delta.content\n",
    "                print(delta.content, end=\"\")\n",
    "        else:\n",
    "            print(\"\\n[Warning] Empty chunk received:\", chunk)\n",
    "\n",
    "    print(\"\\n---\\nFinal Reply:\\n\", reply)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during API call: {e}\")\n",
    "    response = None\n",
    "\n",
    "print(f\"\\n[elapsed: {time.perf_counter()-start:.2f} seconds]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70c7f175-7a23-400b-9a0f-e54cff8af9a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## [Speech to text with Whisper](https://platform.openai.com/docs/guides/speech-to-text)  \n",
    "[Github](https://github.com/openai/whisper)  \n",
    "\n",
    "![](https://raw.githubusercontent.com/openai/whisper/main/approach.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6643369e-e4a9-4b6f-af4f-9c8eea4c4c67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2.1 Transcriptions and Translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fa8f968-c35f-45bf-bdbe-ffc1d970517a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "client_whisper = AzureOpenAI(api_key=openai.api_key,\n",
    "                             api_version=\"2024-06-01\",\n",
    "                             azure_endpoint=\"https://chatgpt-summarization.openai.azure.com/openai/deployments/pioneers-whisper/audio/translations?api-version=2024-06-01\",\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a44e9cfd-6dd8-42bd-8d09-3deaacdc3707",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "filename=r\"../../Data/sound_english.mp3\"\n",
    "Audio(filename=filename, autoplay=True, rate=22050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "733f111d-93f9-4564-83c5-0e0b274c6972",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "audio_file = open(filename, \"rb\")\n",
    "\n",
    "try:\n",
    "    transcription = client_whisper.audio.translations.create(\n",
    "        model=\"pioneers-whisper\", \n",
    "        file=audio_file,\n",
    "    )\n",
    "    print(transcription.text)\n",
    "except Exception as e:\n",
    "    print(\"API Error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e92770e-b1ce-4e6f-8dda-b6f03906be17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "filename=r\"../../Data/sound_german.mp3\"\n",
    "Audio(filename=filename, autoplay=True, rate=22050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3106247-7c76-4daf-9b8e-978304fc1735",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "audio_file = open(filename, \"rb\")\n",
    "\n",
    "try:\n",
    "    transcription = client_whisper.audio.translations.create(\n",
    "        model=\"pioneers-whisper\", \n",
    "        file=audio_file,\n",
    "    )\n",
    "    print(transcription.text)\n",
    "except Exception as e:\n",
    "    print(\"API Error:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c56d25f-0694-41e7-9ef8-9ccd6c6bb449",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### [Translations](https://platform.openai.com/docs/api-reference/audio/createTranslation?lang=python)  \n",
    "It only supports translation into English at this time.  \n",
    "[Supported Languages](https://github.com/openai/whisper#available-models-and-languages)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ba914d1-a563-46f3-9c9a-5b0ab457a3d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "filename=r\"../../Data/sound_portuguese.mp3\"\n",
    "Audio(filename=filename, autoplay=True, rate=22050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2103063-85e2-49ed-8f4b-12950d525433",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "audio_file = open(filename, \"rb\")\n",
    "try:\n",
    "    transcription = client_whisper.audio.translations.create(\n",
    "        model=\"pioneers-whisper\", \n",
    "        file=audio_file,\n",
    "        prompt=\"Translate audio from original Portuguese to English\",\n",
    "    )\n",
    "    print(transcription.text)\n",
    "except Exception as e:\n",
    "    print(\"API Error:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50a78015-e3aa-4144-a342-b784ce031be4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### [Correcting transcriptions with ChatGPT](https://platform.openai.com/docs/guides/speech-to-text/improving-reliability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1f609fe-0bd3-4f04-87a7-0da1a8984716",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "```\n",
    "system_prompt = \"\"\"You are a helpful assistant for the company ZyntriQix. \n",
    "Your task is to correct any spelling discrepancies in the transcribed text. \n",
    "Make sure that the names of the following products are spelled correctly: \n",
    "ZyntriQix, Digique Plus, CynapseFive, VortiQore V8, EchoNix Array, OrbitalLink \n",
    "Seven, DigiFractal Matrix, PULSE, RAPT, B.R.I.C.K., Q.U.A.R.T.Z., F.L.I.N.T. \n",
    "Only add necessary punctuation such as periods, commas, and capitalization, \n",
    "and use only the context provided.\"\"\"\n",
    "\n",
    "def generate_corrected_transcript(temperature, system_prompt, audio_file):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        temperature=temperature,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": transcribe(audio_file, \"\")\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "corrected_text = generate_corrected_transcript(0, system_prompt, fake_company_filepath)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2df2ffe2-f78c-4956-acaf-9b60a2d10414",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## [Text to speech](https://platform.openai.com/docs/guides/text-to-speech)  \n",
    "The Audio API provides a speech endpoint based on our TTS (text-to-speech) model.   \n",
    "It comes with 6 built-in voices: [\"alloy\",\"echo\",\"fable\",\"onyx\",\"nova\",\"shimmer\"] and can be used to:\n",
    "+ Narrate a written blog post\n",
    "+ Produce spoken audio in multiple languages\n",
    "+ Give real time audio output using streaming\n",
    "\n",
    "The default response format is \"mp3\", but other formats like \"opus\", \"aac\", \"flac\", and \"pcm\" are available.\n",
    "\n",
    "+ Opus: For internet streaming and communication, low latency.\n",
    "+ AAC: For digital audio compression, preferred by YouTube, Android, iOS.\n",
    "+ FLAC: For lossless audio compression, favored by audio enthusiasts for archiving.\n",
    "+ WAV: Uncompressed WAV audio, suitable for low-latency applications to avoid decoding overhead.\n",
    "+ PCM: Similar to WAV but containing the raw samples in 24kHz (16-bit signed, low-endian), without the header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f80638c-0e6b-4253-ad9e-3c05cb8bc0be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Model not available in our Region\n",
    "\n",
    "\n",
    "foutput = \"mp3\"\n",
    "speech_file_path = f\"../../Data/speech.{foutput}\"\n",
    "response = client.audio.speech.create(\n",
    "    model=\"tts-1\",\n",
    "    response_format=foutput,\n",
    "    voice=\"echo\", #[\"alloy\",\"echo\",\"fable\",\"onyx\",\"nova\",\"shimmer\"]\n",
    "    input=\"I am very glad to be teaching here, You are wonderful students!\",\n",
    ")\n",
    "\n",
    "\n",
    "response.write_to_file(speech_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a966b6c-c68a-49d1-95ed-119f1abd634b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "filename=speech_file_path\n",
    "Audio(filename=filename, autoplay=True, rate=22050)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d1a80bd-f954-4055-bf3f-44bd44c1344f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## [Text to Image (Dall-e)](https://platform.openai.com/docs/guides/images/usage)  \n",
    "\n",
    "The Images API provides three methods for interacting with images:\n",
    "\n",
    "+ Creating images from scratch based on a text prompt (DALL·E 3 and DALL·E 2)\n",
    "+ Creating edited versions of images by having the model replace some areas of a pre-existing image, based on a new text prompt (DALL·E 2 only)\n",
    "+ Creating variations of an existing image (DALL·E 2 only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd467cf9-378d-44b5-91df-0c563121abc9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Generations\n",
    "\n",
    "The image generations endpoint allows you to create an original image given a text prompt. When using DALL·E 3, images can have a size of 1024x1024, 1024x1792 or 1792x1024 pixels.\n",
    "\n",
    "By default, images are generated at standard quality, but when using DALL·E 3 you can set quality: \"hd\" for enhanced detail. Square, standard quality images are the fastest to generate.\n",
    "\n",
    "You can request 1 image at a time with DALL·E 3 (request more by making parallel requests) or up to 10 images at a time using DALL·E 2 with the n parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66beb751-1cbe-4e88-a4a5-e3c6d42f6cb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Model not available in our Region\n",
    "\n",
    "response = client.images.generate(\n",
    "    model=\"dall-e-3\",\n",
    "    prompt=\"A class about Generative AI in the University of Vienna\",\n",
    "    size=\"1024x1024\",\n",
    "    quality=\"standard\",\n",
    "    response_format='url', #'b64_json',\n",
    "    n=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e11b19b8-17fc-41e1-827a-65669dc82779",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Image(url=response.data[0].url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fae0fba0-bda4-4cfd-85af-b45680d8309e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "response = client.images.generate(\n",
    "    model=\"dall-e-3\",\n",
    "    prompt=\"A futuristic view of Vienna\",\n",
    "    size=\"1024x1024\",\n",
    "    quality=\"standard\",\n",
    "    response_format='b64_json',\n",
    "    n=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec91bed1-57f6-42ed-a6e5-b723a63b89c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#import base64\n",
    "Image(base64.decodebytes(str.encode(response.data[0].b64_json)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b2db643-bd88-4036-abeb-39d96d2f76a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with open(\"../../Data/imageToSave.png\", \"wb\") as f:\n",
    "    f.write(base64.decodebytes(str.encode(response.data[0].b64_json)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e30da032-d6b0-449a-9548-d761dccfdda4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Edits  \n",
    "\n",
    "Also known as \"inpainting\", the image edits endpoint allows you to edit or extend an image by uploading an image and mask indicating which areas should be replaced. The transparent areas of the mask indicate where the image should be edited, and the prompt should describe the full new image, not just the erased area. This endpoint can enable experiences like DALL·E image editing in ChatGPT Plus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8e9896a-f520-4fbd-b0d2-e42d260ef9d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Image(\"../../Data/Vienna_image.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94b1a201-1bff-4b01-b3ba-f15c25648653",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Image(\"../../Data/Vienna_mask.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e0fdb20-55b2-4d51-8cf0-8b15b6168e3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "response = client.images.edit(\n",
    "    model=\"dall-e-2\",\n",
    "    image=open(\"../../Data/Vienna_image.png\", \"rb\"),\n",
    "    mask=open(\"../../Data/Vienna_mask.png\", \"rb\"),\n",
    "    prompt=\"A new ferris whell in the heart of Vienna\",\n",
    "    response_format='b64_json',\n",
    "    n=1,\n",
    "    size=\"512x512\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2773f431-86f8-4ad7-a8e7-f38358c22743",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Image(url=response.data[0].url)\n",
    "Image(base64.decodebytes(str.encode(response.data[0].b64_json)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e529aa6-4e5e-4896-8d3d-c590f3c79319",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with open(\"../../Data/imageToSave2.png\", \"wb\") as f:\n",
    "    f.write(base64.decodebytes(str.encode(response.data[0].b64_json)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8397280-62f8-47f2-ae2c-9108b9eb01ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Variations (DALL·E 2 only)  \n",
    "The image variations endpoint allows you to generate a variation of a given image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce1f0f71-8b0b-49f7-a3c5-8dc0bb6e0660",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "response = client.images.create_variation(\n",
    "    model=\"dall-e-2\",\n",
    "    image=open(\"../Data/Vienna_image.png\", \"rb\"),\n",
    "    n=1,\n",
    "    response_format='b64_json',\n",
    "    size=\"512x512\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d61ccfc4-a389-491f-9487-42e092f75b0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Image(base64.decodebytes(str.encode(response.data[0].b64_json)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2694fc70-4cf1-4da8-99ea-c376ba59f0e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Video Processing  \n",
    "While it's not possible to directly send a video to the API, GPT-4o can understand videos if you sample frames and then provide them as images. It performs better at this task than GPT-4 Turbo.\n",
    "\n",
    "Since GPT-4o in the API does not yet support audio-in (as of Jun 2024), we'll use a combination of GPT-4o and Whisper to process both the audio and visual for a provided video, and showcase two usecases:\n",
    "1. Summarization\n",
    "2. Question and Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1170310-e56a-4ec7-bd5f-6fef8a6b833d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Setup for Video Processing\n",
    "We'll use two python packages for video processing - opencv-python and moviepy. \n",
    "\n",
    "These require [ffmpeg](https://ffmpeg.org/about.html), so make sure to install this beforehand. Depending on your OS, you may need to run `brew install ffmpeg` or `sudo apt install ffmpeg`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71f86362-da3d-4f59-bd46-d75a1b6b7599",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install opencv-python --quiet\n",
    "%pip install --upgrade moviepy\n",
    "%pip install -U imageio-ffmpeg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf4ebf46-e3b5-4560-9fb8-03f0dc44371c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Process the video into two components: frames and audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61bbce68-538b-480d-92fa-d65e2d23eea6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import imageio\n",
    "from moviepy.video.io.VideoFileClip import VideoFileClip\n",
    "\n",
    "# We'll be using the OpenAI DevDay Keynote Recap video. You can review the video here: https://www.youtube.com/watch?v=h02ti0Bl6zk\n",
    "VIDEO_PATH = \"../../Data/keynote_recap.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d55356f-8af4-4dd9-9441-b6281974f191",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_video(video_path, seconds_per_frame=5):\n",
    "    base64Frames = []\n",
    "    base_video_path, _ = os.path.splitext(video_path)\n",
    "\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = video.get(cv2.CAP_PROP_FPS)\n",
    "    frames_to_skip = int(fps * seconds_per_frame)\n",
    "    curr_frame=0\n",
    "\n",
    "    # Loop through the video and extract frames at specified sampling rate\n",
    "    while curr_frame < total_frames - 1:\n",
    "        video.set(cv2.CAP_PROP_POS_FRAMES, curr_frame)\n",
    "        success, frame = video.read()\n",
    "        if not success:\n",
    "            break\n",
    "        _, buffer = cv2.imencode(\".jpg\", frame)\n",
    "        base64Frames.append(base64.b64encode(buffer).decode(\"utf-8\"))\n",
    "        curr_frame += frames_to_skip\n",
    "    video.release()\n",
    "\n",
    "    # Extract audio from video\n",
    "    audio_path = f\"{base_video_path}.mp3\"\n",
    "    clip = VideoFileClip(video_path)\n",
    "    clip.audio.write_audiofile(audio_path, bitrate=\"32k\")\n",
    "    clip.audio.close()\n",
    "    clip.close()\n",
    "\n",
    "    print(f\"Extracted {len(base64Frames)} frames\")\n",
    "    print(f\"Extracted audio to {audio_path}\")\n",
    "    return base64Frames, audio_path\n",
    "\n",
    "# Extract 1 frame per second. You can adjust the `seconds_per_frame` parameter to change the sampling rate\n",
    "base64Frames, audio_path = process_video(VIDEO_PATH, seconds_per_frame=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b80a19c-b1d2-4ed5-80b9-77de376b3883",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"{len(base64Frames)} frames extracted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d485550-51f1-4e29-bb94-444fb58612dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, update_display, Image, Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "794c2e0e-2b63-46a7-8de5-b7d24041a3a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Show an empty display at first\n",
    "display_id = \"frame_display\"\n",
    "display(Image(data=b\"\"), display_id=display_id)\n",
    "\n",
    "# Loop through frames and update display\n",
    "for img in base64Frames:\n",
    "    update_display(Image(data=base64.b64decode(img.encode(\"utf-8\")), width=600), display_id=display_id)\n",
    "    time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8eff6b13-f09f-4647-943e-019eb3f2f6c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Play the audio\n",
    "Audio(audio_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18ca059b-7690-4de9-bb41-612de2b45aad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Example 1: Summarization\n",
    "Now that we have both the video frames and the audio, let's run a few different tests to generate a video summary to compare the results of using the models with different modalities. We should expect to see that the summary generated with context from both visual and audio inputs will be the most accurate, as the model is able to use the entire context from the video.\n",
    "\n",
    "1. Visual Summary\n",
    "2. Audio Summary\n",
    "3. Visual + Audio Summary\n",
    "\n",
    "#### Visual Summary\n",
    "The visual summary is generated by sending the model only the frames from the video. With just the frames, the model is likely to capture the visual aspects, but will miss any details discussed by the speaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c31cc70-8a85-4954-bde0-383ac940fd8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=llm_deploy_name,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are generating a video summary. Please provide a summary of the video. Respond in Markdown.\"},\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"These are the frames from the video.\"},  # Wrap text in a dict\n",
    "            *map(lambda x: {\"type\": \"image_url\", \n",
    "                            \"image_url\": {\"url\": f'data:image/jpg;base64,{x}', \"detail\": \"low\"}}, base64Frames)\n",
    "        ]}\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8fb0766d-f081-4b20-b626-ec9039735cff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The results are as expected - the model is able to capture the high level aspects of the video visuals, but misses the details provided in the speech.\n",
    "\n",
    "#### Audio Summary\n",
    "The audio summary is generated by sending the model the audio transcript. With just the audio, the model is likely to bias towards the audio content, and will miss the context provided by the presentations and visuals.\n",
    "\n",
    "`{audio}` input for GPT-4o isn't currently available but will be coming soon! For now, we use our existing `whisper` model to process the audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e399281-37e0-4507-ba49-190c28a4e160",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Transcribe the audio\n",
    "try:\n",
    "    transcription = client_whisper.audio.translations.create(\n",
    "        model=\"pioneers-whisper\", \n",
    "        file=open(\"../../Data/keynote_recap.mp3\", \"rb\"),\n",
    "        prompt=\"Translate audio from original Portuguese to English\",\n",
    "    )\n",
    "    print(transcription.text)\n",
    "except Exception as e:\n",
    "    print(\"API Error:\", e)\n",
    "\n",
    "## OPTIONAL: Uncomment the line below to print the transcription\n",
    "#print(\"Transcript: \", transcription.text + \"\\n\\n\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=llm_deploy_name,\n",
    "    messages=[\n",
    "    {\"role\": \"system\", \"content\":\"\"\"You are generating a transcript summary. Create a summary of the provided transcription. Respond in Markdown.\"\"\"},\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"text\", \"text\": f\"The audio transcription is: {transcription.text}\"}\n",
    "        ],\n",
    "    }\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1a541cc-721e-48ab-b9dd-ae3454d76116",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The audio summary is biased towards the content discussed during the speech, but comes out with much less structure than the video summary.\n",
    "\n",
    "#### Audio + Visual Summary\n",
    "The Audio + Visual summary is generated by sending the model both the visual and the audio from the video at once. When sending both of these, the model is expected to better summarize since it can perceive the entire video at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b078e6a-7e73-45ab-9a3a-08fcf4f39eb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Generate a summary with visual and audio\n",
    "response = client.chat.completions.create(\n",
    "    model=llm_deploy_name,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"\"\"You are generating a video summary. Create a summary of the provided video and its transcript. Respond in Markdown.\"\"\"},\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"These are the frames from the video.\"},  # Wrap text in a dict\n",
    "            *map(lambda x: {\"type\": \"image_url\", \n",
    "                            \"image_url\": {\"url\": f'data:image/jpg;base64,{x}', \"detail\": \"low\"}}, base64Frames),\n",
    "            {\"type\": \"text\", \"text\": f\"The audio transcription is: {transcription.text}\"}\n",
    "        ]}\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa0abc6f-f9dd-4c13-8ff6-636ef351cc65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "After combining both the video and audio, we're able to get a much more detailed and comprehensive summary for the event which uses information from both the visual and audio elements from the video.\n",
    "\n",
    "### Example 2: Question and Answering\n",
    "For the Q&A, we'll use the same concept as before to ask questions of our processed video while running the same 3 tests to demonstrate the benefit of combining input modalities:\n",
    "1. Visual Q&A\n",
    "2. Audio Q&A\n",
    "3. Visual + Audio Q&A "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bdc66fc-c797-438d-83e4-42b32710b10e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "QUESTION = \"Question: Why did Sam Altman have an example about raising windows and turning the radio on?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9354200-e1c1-4ec3-99e2-f0bdc6eeda15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "qa_visual_response = client.chat.completions.create(\n",
    "    model=llm_deploy_name,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Use the video to answer the provided question. Respond in Markdown.\"},\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"These are the frames from the video.\"},  # Wrap text in a dict\n",
    "            *map(lambda x: {\"type\": \"image_url\", \"image_url\": {\"url\": f'data:image/jpg;base64,{x}', \"detail\": \"low\"}}, base64Frames),\n",
    "            {\"type\": \"text\", \"text\": QUESTION}  # Wrap QUESTION in a dict\n",
    "        ]}\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(\"Visual QA:\\n\" + qa_visual_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54235895-6bc3-4c03-a071-0b75e9808257",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "qa_audio_response = client.chat.completions.create(\n",
    "    model=llm_deploy_name,\n",
    "    messages=[\n",
    "    {\"role\": \"system\", \"content\":\"\"\"Use the transcription to answer the provided question. Respond in Markdown.\"\"\"},\n",
    "    {\"role\": \"user\", \"content\": f\"The audio transcription is: {transcription.text}. \\n\\n {QUESTION}\"},\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "print(\"Audio QA:\\n\" + qa_audio_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "462cb207-8165-4570-98f1-5a28384bbdc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "qa_both_response = client.chat.completions.create(\n",
    "    model=llm_deploy_name,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Use the video and transcription to answer the provided question.\"},\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"These are the frames from the video.\"},  # Wrap text in a dict\n",
    "            *map(lambda x: {\"type\": \"image_url\", \n",
    "                            \"image_url\": {\"url\": f'data:image/jpg;base64,{x}', \"detail\": \"low\"}}, base64Frames),\n",
    "            {\"type\": \"text\", \"text\": f\"The audio transcription is: {transcription.text}\"},  # Wrap transcription\n",
    "            {\"type\": \"text\", \"text\": QUESTION}  # Wrap QUESTION in a dict\n",
    "        ]}\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(\"Both QA:\\n\" + qa_both_response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "353b9d8b-fa21-4e8e-9207-78b29e7d86c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Comparing the three answers, the most accurate answer is generated by using both the audio and visual from the video. Sam Altman did not discuss the raising windows or radio on during the Keynote, but referenced an improved capability for the model to execute multiple functions in a single request while the examples were shown behind him.\n",
    "\n",
    "## Conclusion\n",
    "Integrating many input modalities such as audio, visual, and textual, significantly enhances the performance of the model on a diverse range of tasks. This multimodal approach allows for more comprehensive understanding and interaction, mirroring more closely how humans perceive and process information. \n",
    "\n",
    "Currently, GPT-4o in the API supports text and image inputs, with audio capabilities coming soon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce81525c-87ba-4da8-91e8-f6a0af5d1442",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6 Moderations  \n",
    "The moderations endpoint is a tool you can use to check whether text is potentially harmful. Developers can use it to identify content that might be harmful and take action, for instance by filtering it.\n",
    "\n",
    "The models classifies the following categories:  \n",
    "Category\tDescription  \n",
    "+ hate\tContent that expresses, incites, or promotes hate based on race, gender, ethnicity, religion, nationality, sexual orientation, disability status, or caste.\n",
    "+ Hateful content aimed at non-protected groups (e.g., chess players) is harassment.  \n",
    "+ hate/threatening\tHateful content that also includes violence or serious harm towards the targeted group based on race, gender, ethnicity, religion, nationality, sexual orientation, disability status, or caste.  \n",
    "+ harassment\tContent that expresses, incites, or promotes harassing language towards any target.  \n",
    "+ harassment/threatening\tHarassment content that also includes violence or serious harm towards any target.  \n",
    "+ self-harm\tContent that promotes, encourages, or depicts acts of self-harm, such as suicide, cutting, and eating disorders.  \n",
    "+ self-harm/intent\tContent where the speaker expresses that they are engaging or intend to engage in acts of self-harm, such as suicide, cutting, and eating disorders.     \n",
    "+ self-harm/instructions\tContent that encourages performing acts of self-harm, such as suicide, cutting, and eating disorders, or that gives instructions or advice on how to commit such acts.  \n",
    "+ sexual\tContent meant to arouse sexual excitement, such as the description of sexual activity, or that promotes sexual services (excluding sex education and wellness).\n",
    "+ sexual/minors\tSexual content that includes an individual who is under 18 years old.  \n",
    "+ violence\tContent that depicts death, violence, or physical injury.  \n",
    "+ violence/graphic\tContent that depicts death, violence, or physical injury in graphic detail.  \n",
    "\n",
    "The moderation endpoint is free to use for most developers. For higher accuracy, try splitting long pieces of text into smaller chunks each less than 2,000 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae8fefcf-5029-4094-b5b7-ac0fcb15c30c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Examples from: https://www.politico.eu/article/15-most-offensive-things-trump-campaign-feminism-migration-racism/\n",
    "\n",
    "sample_text = \"\"\"\n",
    "“I have black guys counting my money. … I hate it,” Trump told John R. O’Donnell, the former president of Trump Plaza Hotel & Casino, \n",
    "according O’Donnell’s account in his 1991 book “Trumped!” “The only guys I want counting my money are short guys that wear yarmulkes all day.”\n",
    "Trump, according to O’Donnell, went on to say, “‘Laziness is a trait in blacks. It really is, I believe that.”\n",
    "\"\"\"\n",
    "\n",
    "response = client.moderations.create(input=sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dad7f37f-564e-4198-ba8d-0495bb912994",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(response.to_json())"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "6-MultiModal_Models",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
