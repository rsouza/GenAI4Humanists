{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdac04e7-ffd5-49c2-98d4-1be75fd5b3c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Semantic_Image_Search\n",
    "[source](https://github.com/emily-gibbs/articles/blob/main/image_semantic_search_with_vectors/code_walkthrough.ipynb)\n",
    "\n",
    "Have you ever taken forever trying to find a specific photo in a photo folder? Or tried to search an image repository for just the right image for a presentation and can't seem to find what you're looking for? It may be that the technology underlying your image search is based on outdated methods. \n",
    "\n",
    "The most straightforward and traditional way to perform an image search is with tagsâ€Š-â€Šeach image is assigned relevant tags, and when someone searches the tag those images are returned. However, there are several problems with this approach:\n",
    "\n",
    "Can't handle synonymsâ€Š-â€Šwhat if the tag is 'funny', but the user searches with 'silly'? It would be very hard to tag an image with ALL of the ways a user could phrase their search. \n",
    "No sense of degreeâ€Š-â€Šwith thousands of images, a user would likely prefer to see images ranked by how closely they match the query (e.g. from most silly to least silly)\n",
    "Time consumingâ€Š-â€ŠIt takes time and effort to make sure all images are correctly tagged and that relevant tags are assigned when new images are added. \n",
    "What a user really wants is to be able to search based upon meaning, not a finite set of words. This is where vectors embeddings come to the rescue! ðŸ¥³\n",
    "\n",
    "a vector embedding is a list of numbers that represents features of something. They represent the semantic meaning of unstructured data such as chunks of text, images, videos, etc. We can measure how similar two vectors are (are therefore how similar their source objects are) with the cosine similarity formula.\n",
    "\n",
    "What if we leveraged this idea for an image search? Rememberâ€Š-â€Šonce an object is embedded into the meaning space it doesn't matter what form it took before (text, image, audio, etc.). The only thing that matters is it's meaning. So if we create vectors for all of our images, then create a vector for our search text, we should be able to use cosine similarity to compare them and find what images best match our search!\n",
    "\n",
    "Sound kind of confusing? Let's walk through the code implementation step-by-step so we can see what a proof of concept of this would look like.\n",
    "\n",
    "First, we must ensure that the image embeddings and the text embeddings are in the same meaning space. This is important! Models that create vector embeddings from inputs each create their own unique meaning space. This is because the features of the vector are unique to the model (and the number of features may be different too!). We can't just embed an image with a random image model then embed our search text with a different random text model and compare the two to each other -even if they did have the same number of features, its like comparing apples to orangutans and you'll get back nonsense results!\n",
    "\n",
    "Fortunately there are models that have been trained on multiple input modalities and embed them into the same meaning space. The one we'll be using today is called [OpenCLIP](https://huggingface.co/docs/hub/en/open_clip), an open-source implementation of the CLIP model created by OpenAI. \n",
    "\n",
    "OpenCLIP actually has several model versions that you can load and use. We'll be using the \"ViT-g-14\" model for this tutorial, but I encourage you to explore the other models that are available. \n",
    "\n",
    "Let's start by installing some libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8a34984-db08-4793-b312-ef8390a3990f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "!pip install -q openai\n",
    "!pip install pdf2image\n",
    "!pip install -q langchain\n",
    "!pip install -q langchain-experimental\n",
    "\n",
    "!pip install open_clip_torch\n",
    "#dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63567bd4-0bfc-4274-a975-f03d1edf4835",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import time\n",
    "import openai\n",
    "import base64\n",
    "import requests\n",
    "import textwrap\n",
    "#from pdf2image import convert_from_path\n",
    "import PIL\n",
    "\n",
    "from IPython.display import Image, Audio, Markdown, Math\n",
    "from openai import OpenAI\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "from langchain_experimental.open_clip import OpenCLIPEmbeddings # requires that open_clip_torch is installed\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08b65434-66e6-4320-bf31-aa0583c9ecda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Loading model using LangChain wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86a6c54f-158a-4a54-8eaf-da955c131931",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"ViT-g-14\"\n",
    "checkpoint = \"laion2b_s34b_b88k\"\n",
    "clip_embd = OpenCLIPEmbeddings(model_name=model_name, checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bc156dd-f79c-476e-96ca-1ccd3b2ac734",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now we need to create a vector embedding for all of the images that we want to search. I have a folder of ~40 bird images in a folder simply called \"Images\" that you can find at the Github repo for this project, or you can use your own set of images. Below is a function to create an embedding for each image in our folder and store them in a JSON file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d59d8643-2fc1-47ea-b852-9903953a9417",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create a JSON file with data in the format of\n",
    "{\n",
    "    image_uri: \n",
    "        'embedding': image_vector\n",
    "}\n",
    "\"\"\"\n",
    "def embed_all_images(images_dir: str):\n",
    "\n",
    "    # Check if we already have some image vectors made (if we do, don't want to remake them every time we add a new image)\n",
    "    old_image_vectors = {}\n",
    "    if os.path.exists('image_vectors.json'):\n",
    "        with open('image_vectors.json', 'r') as file:\n",
    "            old_image_vectors = json.load(file)\n",
    "\n",
    "    image_files = os.listdir(images_dir)\n",
    "    image_uris = [os.path.join(images_dir, image_file) for image_file in image_files]\n",
    "    new_image_vectors = {}\n",
    "    for image_uri in image_uris:\n",
    "        print(f\"Embedding image {image_uri}\")\n",
    "        if image_uri in old_image_vectors:\n",
    "            # Reuse the vector embedding created before\n",
    "            new_image_vectors[image_uri] = old_image_vectors[image_uri]\n",
    "        else:\n",
    "            # Create the vector embedding for this image\n",
    "            image_vector = clip_embd.embed_image([image_uri])[0]\n",
    "            new_image_vectors[image_uri] = {'embedding': image_vector}\n",
    "\n",
    "    # Write the dictionary to a JSON file\n",
    "    with open('image_vectors.json', 'w') as file:\n",
    "        json.dump(new_image_vectors, file, indent=4)\n",
    "\n",
    "embed_all_images(images_dir = \"../../Data/Images/\")\n",
    "print(f\"{len(json.load(open('image_vectors.json')))} images embedded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae7f75a9-f4c8-440e-b89d-5540cf53f042",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Loading the JSON file for use with the rest of the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db58a522-71ba-4bb0-8a69-5d3fecc033ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load the image vectors from the JSON file\n",
    "with open(file=\"./image_vectors.json\", mode='r') as file:\n",
    "    image_data = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "684d7785-a291-48b5-b63f-3307d14b1582",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now we need a function that allows us to pass in a search text and get back the images that best fit that search. It must:\n",
    "\n",
    "+ Create the vector embedding for the search text\n",
    "+ Calculate the cosine similarity between that search text and all of the image vector embeddings\n",
    "+ Return the images that are most similar to that search vector\n",
    "\n",
    "\n",
    "Here's the code for that below. We're actually going to create the search vector outside of this function (we'll see why in just a bitâ€¦):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bc57fb9-c88d-4dbe-8e69-10a076e0fa03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def display_top_search_results(\n",
    "    query_vector: list[float], image_data: dict[str, list[float]], num_results: int = 5\n",
    "):\n",
    "\n",
    "    # Calculate the cosine similarity between the query vector and all image vectors and add that to the image_info dict\n",
    "    for image_uri, data in image_data.items():\n",
    "        similarity = cosine_similarity(\n",
    "            np.array([query_vector]), np.array([data[\"embedding\"]])\n",
    "        )[0][0]\n",
    "        data[\"query_similarity\"] = similarity\n",
    "\n",
    "    # Sort the image_info dict by the similarity value and return the top num_results\n",
    "    top_results = dict(\n",
    "        sorted(\n",
    "            image_data.items(),\n",
    "            key=lambda item: item[1][\"query_similarity\"],\n",
    "            reverse=True,\n",
    "        )[:num_results]\n",
    "    )\n",
    "    \n",
    "    # Display the images with their scores in a grid with 3 columns\n",
    "    # Calculate number of rows and columns needed\n",
    "    num_cols = 3\n",
    "    num_rows = (num_results + num_cols - 1) // num_cols  # Ceiling division\n",
    "\n",
    "    # Create subplot grid\n",
    "    fig, axs = plt.subplots(num_rows, num_cols, figsize=(15, 5 * num_rows))\n",
    "    axs = axs.ravel()  # Flatten the 2D array of axes to make indexing easier\n",
    "\n",
    "    # Display images\n",
    "    for i, (image_uri, data) in enumerate(top_results.items()):\n",
    "        axs[i].imshow(PIL.Image.open(image_uri))\n",
    "        axs[i].set_title(f\"Score: {data['query_similarity']:.2f}\")\n",
    "        axs[i].axis(\"off\")\n",
    "\n",
    "    # Hide any empty subplots\n",
    "    for j in range(i + 1, len(axs)):\n",
    "        axs[j].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c7d5e7c-0b0c-4312-abce-07cdfe73a110",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's test this out by asking for the top 3 images that match the search for a 'funny bird':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "987d58a1-78d6-42ed-a6b8-53e4da9789c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "search_text = \"funny bird\"\n",
    "query_vector = clip_embd.embed_documents([search_text])[0]\n",
    "display_top_search_results(query_vector, image_data, num_results=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc0a8ba8-2b88-4b85-b49c-7ede6ac1541f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Those look pretty funny to me! Now let's see if it can handle synonyms by searching instead for 'silly bird':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f190b862-4485-4d70-8dff-266d4dbf562b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "search_text = \"silly bird\"\n",
    "query_vector = clip_embd.embed_documents([search_text])[0]\n",
    "display_top_search_results(query_vector, image_data, num_results=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5728fb21-acb0-4e74-9fc9-227f985a6a6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Perfect! Now let's try something much more abstract that would likely never end up in an image tagâ€Š-â€Š'bad hair day':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6c3f375-068f-455a-86c4-0d06b81d3ed0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "search_text = \"bad hair day\"\n",
    "query_vector = clip_embd.embed_documents([search_text])[0]\n",
    "display_top_search_results(query_vector, image_data, num_results=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "469a9c00-3766-49a8-bd07-ae8f8c845053",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Our search can even handle this more abstract search pretty well!\n",
    "\n",
    "#### Reverse Image Search\n",
    "This setup is actually more flexible than you might think. Rememberâ€Š-â€Šas long as we have a search vector in the same meaning space, it doesn't really matter what that vector was made from. It could be made from audio, text, or another imageâ€Š-â€Ša vector is a vector! \n",
    "\n",
    "Imagine you already have an image, and you want to find images that are similar (perhaps you've taken a picture of a bird and want to look up what species it is). We can do that, and we don't even have to change our function! Here's the image we're going to use for our search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04c4fdfc-3d8d-410e-8104-4821ac0838fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def display_image(image_uri):\n",
    "    image = PIL.Image.open(image_uri)\n",
    "    plt.imshow(image)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "display_image(\"../../Data/Images/toucan-4185361_640.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d8ecd2d-46b5-4a5b-bbb2-28489d4a73bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "To search for similar images we just create a search vector from this image instead of from a search text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e81c2263-1ba6-4259-b744-9873e96e3106",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "search_image = \"../../Data/Images/toucan-4185361_640.jpg\"\n",
    "query_vector = clip_embd.embed_image([search_image])[0]\n",
    "display_top_search_results(query_vector, image_data, num_results=3)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Semantic_Image_Search",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
