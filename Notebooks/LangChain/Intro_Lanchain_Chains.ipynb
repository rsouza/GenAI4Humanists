{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ioLoShcpx6H7"
   },
   "source": [
    "\n",
    "# Intro to LangChain\n",
    "\n",
    "LangChain is an open-source framework that gives developers the tools they need to create applications using large language models (LLMs). In its essence, LangChain is a prompt orchestration tool that makes it easier for teams to connect various prompts interactively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 55685,
     "status": "ok",
     "timestamp": 1713514624058,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "miOF_GZU5mst",
    "outputId": "489dd3e4-06e9-4fb0-97d4-acb640612a24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydantic==1.10.12 in /home/renato/Documents/env_default/lib/python3.10/site-packages (1.10.12)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/renato/Documents/env_default/lib/python3.10/site-packages (from pydantic==1.10.12) (4.11.0)\n"
     ]
    }
   ],
   "source": [
    "#!pip install -qU openai\n",
    "#!pip install -qU langchain\n",
    "#!pip install -qU langchain-openai\n",
    "#!pip install -qU tiktoken\n",
    "\n",
    "!pip install pydantic==\"1.10.12\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yrb7nxgq7nvR"
   },
   "source": [
    "# Accessing OpenAI Directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 286,
     "status": "ok",
     "timestamp": 1713514904192,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "YoxylcB85gDr"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "#os.environ[\"OPENAI_API_KEY\"] = \"<the key>\"\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "client = openai.OpenAI()\n",
    "\n",
    "#model=\"gpt-3.5-turbo\"\n",
    "#model=\"gpt-4o\"\n",
    "model=\"gpt-4o-mini\"\n",
    "\n",
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "        ]\n",
    "    response = client.chat.completions.create(model=model,\n",
    "                                              messages=messages,\n",
    "                                              temperature=2\n",
    "                                              )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m8GyMwn47XMn"
   },
   "source": [
    "## Simple Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1048,
     "status": "ok",
     "timestamp": 1713514927826,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "ZKQp2YrMyBvz",
    "outputId": "dc37e5db-bf91-44f4-97df-f7be826be323"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! 1+1 is equal to 2. \n"
     ]
    }
   ],
   "source": [
    "response = get_completion(\"Can you tell me how much is 1+1?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mw2bnNelykOv"
   },
   "source": [
    "## Queries with custom prompts using formatted strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 361,
     "status": "ok",
     "timestamp": 1713515082959,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "GrEcvuU7yjtu",
    "outputId": "995ff889-84b8-4f18-e439-1c04fb8cf208"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate the text that is delimited by triple backticks\n",
      "into a style that is American English in a calm and respectful tone\n",
      ".\n",
      "text: ```\n",
      "Arrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse,the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_email = \"\"\"\n",
    "Arrr, I be fuming that me blender lid \\\n",
    "flew off and splattered me kitchen walls \\\n",
    "with smoothie! And to make matters worse,\\\n",
    "the warranty don't cover the cost of \\\n",
    "cleaning up me kitchen. I need yer help \\\n",
    "right now, matey!\n",
    "\"\"\"\n",
    "\n",
    "style = \"\"\"American English \\\n",
    "in a calm and respectful tone\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"Translate the text \\\n",
    "that is delimited by triple backticks\n",
    "into a style that is {style}.\n",
    "text: ```{customer_email}```\n",
    "\"\"\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1688,
     "status": "ok",
     "timestamp": 1713515087348,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "haWe0_HNys3B",
    "outputId": "355dbeba-c25d-4e23-bc49-0015e9121250"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am upset that my blender lid flew off and made a mess of my kitchen walls with smoothie! To add to that, the warranty does not include the cost of cleaning up my kitchen. I need your help, please!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Should we use a framework to wrap the direct calls to OpenAI?    \n",
    "\n",
    "The LangChain pipelines consist of the following modules:\n",
    "\n",
    "+ **Models**: Models mostly cover Large Language models. A large language model of considerable size is a model that comprises a neural network with numerous parameters and is trained on vast quantities of unlabeled text  \n",
    "+ **Prompts**: prompt is the input that we give to any system to refine our answers to make them more accurate or more specific according to our use case. Many times you may want to get more structured information than just text back. We can use Prompts in conjunction with Parsers\n",
    "+ **Parsers**: Output parsers are responsible for taking the output of an LLM and transforming it to a more suitable format. This is very useful when you are using LLMs to generate any form of structured data.  \n",
    "+ **Memory**: Chains and Agents in LangChain operate in a stateless mode by default, meaning that they handle each incoming query independently. However, there are certain applications, like chatbots, where it is of great importance to retain previous interactions, both over the short and long term. This is where the concept of “Memory” comes into play.  \n",
    "+ **Chains**: Chains provide a means to merge various components into a unified application. A chain can be created, for instance, that receives input from a user, formats it using a PromptTemplate, and subsequently transmits the formatted reply to an LLM. More intricate chains can be generated by integrating multiple chains with other components.  \n",
    "+ **Agents**: Certain applications may necessitate not only a pre-determined sequence of LLM/other tool calls but also an uncertain sequence that is dependent on the user’s input. These kinds of sequences include an “agent” that has access to a range of tools. Based on the user input, the agent may determine which of these tools, if any, should be called.  \n",
    "+ **Callbacks**: Allow you to hook into the various stages of your LLM application. This is useful for logging, monitoring, streaming, and other tasks.  \n",
    "+ **Indexes**: Indexes are information stored in local databases that can be used for Augment the capabilities of the models being used. We will see them in action with RAG pipelines  \n",
    "\n",
    "Let's wrap the pipeline:\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:4800/format:webp/1*05zEoeNU7DVYOFzjugiF_w.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NkjFnz46y0nQ"
   },
   "source": [
    "\n",
    "# I - Simple Pipelines with Langchain  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 615,
     "status": "ok",
     "timestamp": 1713515361990,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "bh56xPYvyv16"
   },
   "outputs": [],
   "source": [
    "#os.environ[\"OPENAI_API_KEY\"] = \"<the key>\"\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "llm=\"gpt-3.5-turbo\"\n",
    "\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1713515364077,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "Okkq2c5w_eI8"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.prompts import HumanMessagePromptTemplate\n",
    "from langchain_core.prompts import ChatMessagePromptTemplate\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.messages import AIMessage\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field, validator\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain.output_parsers import PandasDataFrameOutputParser\n",
    "from langchain.output_parsers.json import SimpleJsonOutputParser\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "from langchain.chains import ConversationChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a3I6VXnL0OVp"
   },
   "source": [
    "Once you've installed and initialized the LLM of your choice, we could use it!  Let's ask it some question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2939,
     "status": "ok",
     "timestamp": 1713515370013,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "WuKYnH2Jzxb8",
    "outputId": "f4623a5d-d7c8-4501-b6ff-3a491fdd49dc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"The sky appears blue because of the way Earth's atmosphere scatters sunlight. Sunlight is made up of all the colors of the rainbow, but blue light is scattered more than other colors because it travels as shorter, smaller waves. When sunlight reaches Earth's atmosphere, the blue light is scattered in all directions by the gases and particles in the air. This is why we see the sky as blue during the day.\", response_metadata={'token_usage': {'completion_tokens': 83, 'prompt_tokens': 13, 'total_tokens': 96}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-8b139933-09ef-4291-b495-e447ee2d0d85-0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"Why is the sky blue?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EhhGOhxq0W_W"
   },
   "source": [
    "Prompt templates convert raw user input to better input to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3054,
     "status": "ok",
     "timestamp": 1713515377035,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "PnJOraQa0E8I",
    "outputId": "32f2b136-dcaa-43f6-f1ab-acd1643374fe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"That's a great question! The sky appears blue because of the way our Earth's atmosphere scatters sunlight. When sunlight reaches the Earth's atmosphere, it is made up of different colors, with blue light being scattered more than the other colors because it travels in shorter, smaller waves. This scattered blue light is what we see when we look up at the sky during the day.\", response_metadata={'token_usage': {'completion_tokens': 76, 'prompt_tokens': 24, 'total_tokens': 100}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-b128213c-e09a-4849-bd0e-cedddd714bec-0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([(\"system\", \"You are a first grade teacher.\"),\n",
    "                                           (\"user\", \"{input}\")\n",
    "                                         ])\n",
    "\n",
    "chain = prompt | model\n",
    "chain.invoke({\"input\": \"Why is the sky blue?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n5cGE6b42KE9"
   },
   "source": [
    "The output of a ChatModel (and therefore, of this chain) is a message object.  \n",
    "However, it's often much more convenient to work with strings.  \n",
    "Let's add a simple output parser to convert the chat message to a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 313,
     "status": "ok",
     "timestamp": 1713515382544,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "UsCRlie31g3u"
   },
   "outputs": [],
   "source": [
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "executionInfo": {
     "elapsed": 2646,
     "status": "ok",
     "timestamp": 1713515388989,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "46h0vZIf2PDV",
    "outputId": "c5c35513-e44e-4d8f-c235-df4557a0c432"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"That's a great question! The sky appears blue because of the way that sunlight interacts with the Earth's atmosphere. When sunlight reaches the Earth, it is made up of different colors, including red, orange, yellow, green, blue, indigo, and violet. These colors are all part of the visible light spectrum.\\n\\nThe Earth's atmosphere is made up of gases and particles. When sunlight enters the atmosphere, the shorter blue wavelengths of light are scattered in all directions by the gases and particles in the air. This is called Rayleigh scattering.\\n\\nSince blue light is scattered more than the other colors, it is scattered in all directions and makes the sky look blue to our eyes. This is why we see the sky as blue during the day.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | model | output_parser\n",
    "chain.invoke({\"input\": \"Why is the sky blue?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3MHk0lzH3fuN"
   },
   "source": [
    "# II - Exploring Chain Elements - Langchain Expression Language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d3hI0QJe45VY"
   },
   "source": [
    "Notice this line of the code, where we piece together these different components into a single chain using LCEL:  \n",
    "\n",
    "**chain = prompt | model | output_parser**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XW6Fd-Ko4YvE"
   },
   "source": [
    "## 1. [Prompts](https://python.langchain.com/docs/modules/model_io/prompts/)    \n",
    "prompt is a BasePromptTemplate, which means it takes in a dictionary of template variables and produces a PromptValue.  \n",
    "+ A PromptValue is a wrapper around a completed prompt that can be passed to either a LLM (which takes a string as input) or ChatModel (which takes a sequence of messages as input).  \n",
    "+ It can work with either language model type because it defines logic both for producing BaseMessages and for producing a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 1316,
     "status": "ok",
     "timestamp": 1713474572510,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "E5ycNd1q3tcA",
    "outputId": "a8c073ad-6640-4ce9-e685-8f99d563c564"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why did the chicken join a band? Because it had the drumsticks!'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"tell me a short joke about {topic}\")\n",
    "chain = prompt | model | output_parser\n",
    "chain.invoke({\"topic\": \"chicken\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 221,
     "status": "ok",
     "timestamp": 1713474583609,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "p5s4zuDi4aFq",
    "outputId": "2a4a9515-797a-4517-ed23-495586dbf66c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content='tell me a short joke about ice cream')])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_value = prompt.invoke({\"topic\": \"ice cream\"})\n",
    "prompt_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 213,
     "status": "ok",
     "timestamp": 1713474593988,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "u1hBd2io9dWI",
    "outputId": "adb45ee8-0068-4202-e85d-e9d15c25b9e7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='tell me a short joke about ice cream')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_value.to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 245,
     "status": "ok",
     "timestamp": 1713474597165,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "binqpy-79f4z",
    "outputId": "19d2ddf6-5aaf-45ff-e61b-ca722244e7da"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: tell me a short joke about ice cream'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_value.to_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "executionInfo": {
     "elapsed": 2380,
     "status": "ok",
     "timestamp": 1713474723187,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "bz5qa_TCDN4d",
    "outputId": "faa4f7a3-22d3-4e24-c416-4d281ebda444"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"That's a great question! The sky appears blue because of the way that sunlight interacts with the Earth's atmosphere. When sunlight reaches the Earth's atmosphere, it is scattered in all directions by the gases and particles in the air. Blue light is scattered more than other colors because it travels as shorter, smaller waves. This is why we see the sky as blue during the day!\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([(\"system\", \"You are a first grade teacher.\"),\n",
    "                                           (\"user\", \"{input}\")\n",
    "                                         ])\n",
    "chain = prompt | model | output_parser\n",
    "chain.invoke({\"input\": \"Why is the sky blue?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rGLNmBQy5B8S"
   },
   "source": [
    "## 2. [Models](https://python.langchain.com/docs/modules/model_io/chat/quick_start/)    \n",
    "The PromptValue is then passed to model.  \n",
    "In this case our model is a ChatModel, meaning it will output a BaseMessage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1433,
     "status": "ok",
     "timestamp": 1713473317439,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "X2wt5_jf4eEe",
    "outputId": "4f2a654e-77fe-4f9c-d29e-4c60fcb2a29d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Why did the ice cream truck break down? Because it had too many sundaes!', response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 15, 'total_tokens': 33}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-97fc72c3-ecf6-4507-a45b-742b6df921f3-0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message = model.invoke(prompt_value)\n",
    "message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZCME5D_A5VTc"
   },
   "source": [
    "If our model was an pure LLM, it would output a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 1237,
     "status": "ok",
     "timestamp": 1713473332520,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "yREwkxMj5J5E",
    "outputId": "e27443f4-67be-4694-d19d-ffeee3b8ac6f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nRobot: Why did the ice cream truck break down? Because it had a fudge-ical!'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm2 = OpenAI(model=\"gpt-3.5-turbo-instruct\")\n",
    "llm2.invoke(prompt_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GEeoPLa652D3"
   },
   "source": [
    "## 3. [Output parsers](https://python.langchain.com/docs/modules/model_io/output_parsers/)    \n",
    "And lastly we pass our model output to the output_parser, which is a BaseOutputParser meaning it takes either a string or a BaseMessage as input. The specific StrOutputParser simply converts any input into a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 199,
     "status": "ok",
     "timestamp": 1713473342078,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "Kt6xF6wt5THU",
    "outputId": "c6ff9d82-73a0-4805-cbbf-e9aab16ffe3d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why did the ice cream truck break down? Because it had too many sundaes!'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parser.invoke(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vifudtoO6Kt2"
   },
   "source": [
    "## 4. [Chains](https://python.langchain.com/v0.1/docs/modules/chains/)  \n",
    "\n",
    "Chains are combination of steps. We have followed the steps along:\n",
    "\n",
    "+ We passed the user input on the desired topic as {\"topic\": \"ice cream\"}\n",
    "+ The prompt component takes the user input, which is then used to construct a PromptValue after using the topic to construct the prompt.\n",
    "+ The model component takes the generated prompt, and passes into the OpenAI LLM model for evaluation.\n",
    "+ The generated output from the model is a ChatMessage object.\n",
    "+ Finally, the output_parser component takes in a ChatMessage, and transforms this into a Python string, which is returned from the invoke method.\n",
    "\n",
    "Langchain is currently revamping all the chain methods to use the [LCEL](https://python.langchain.com/v0.1/docs/expression_language/). That is why we are not exploring all types of chains in this notebook.  , "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3K1Ovj-EZr7"
   },
   "source": [
    "# 5. [Types of Prompts](https://python.langchain.com/docs/modules/model_io/prompts/quick_start/)  \n",
    "Let's explore some other types of Prompts  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xqYB7XnjEqJV"
   },
   "source": [
    "## 5.1 PromptTemplate\n",
    "\n",
    "Use PromptTemplate to create a template for a string prompt.\n",
    "By default, PromptTemplate uses Python’s str.format syntax for templating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 250,
     "status": "ok",
     "timestamp": 1713515601642,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "GEsb8EBw57ld",
    "outputId": "2a759fbf-62b8-4850-a4d5-7b288ba70dc8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell me a funny joke about chickens.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = PromptTemplate.from_template(\"Tell me a {adjective} joke about {content}.\")\n",
    "prompt_template.format(adjective=\"funny\", content=\"chickens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 291,
     "status": "ok",
     "timestamp": 1713515623928,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "MR0Ip8O6E1db",
    "outputId": "d64d8ca5-92fd-4381-9311-06879338c46c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell me a joke'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = PromptTemplate.from_template(\"Tell me a joke\")\n",
    "prompt_template.format()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4zLArLmYFBK1"
   },
   "source": [
    "## 5.2 ChatPromptTemplate\n",
    "\n",
    "The prompt to chat models/ is a list of chat messages.  \n",
    "Each chat message is associated with content, and an additional parameter called role. For example, in the OpenAI Chat Completions API, a chat message can be associated with an AI assistant, a human or a system role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 222,
     "status": "ok",
     "timestamp": 1713476275035,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "2jUJRy4RE7bL",
    "outputId": "eed9ebea-f868-418f-8ad8-c31efdae3426"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful AI bot. Your name is Bob.'),\n",
       " HumanMessage(content='Hello, how are you doing?'),\n",
       " AIMessage(content=\"I'm doing well, thanks!\"),\n",
       " HumanMessage(content='What is your name?')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
    "        (\"human\", \"Hello, how are you doing?\"),\n",
    "        (\"ai\", \"I'm doing well, thanks!\"),\n",
    "        (\"human\", \"{user_input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "messages = chat_template.format_messages(name=\"Bob\", user_input=\"What is your name?\")\n",
    "messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "erMamCwOFWiY"
   },
   "source": [
    "This is equivalent as doing the following, directly with OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "Rsgp2i-HFOT9"
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful AI bot. Your name is Bob.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Hello, how are you doing?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"I'm doing well, thanks!\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is your name?\"},\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SnzZFr30I1tt"
   },
   "source": [
    "The ChatPromptTemplate.from_messages static method accepts a variety of message representations and is a convenient way to format input to chat models with exactly the messages you want.  \n",
    "\n",
    "For example, in addition to using the 2-tuple representation of (type, content) used above, you could pass in an instance of MessagePromptTemplate or BaseMessage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 221,
     "status": "ok",
     "timestamp": 1713476445408,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "GkIPB2MsI5XR",
    "outputId": "cf7bd703-c86b-4038-ada4-e7ba2797959e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content=\"You are a helpful assistant that re-writes the user's text to sound more upbeat.\"), HumanMessage(content=\"I don't like eating tasty things\")]\n"
     ]
    }
   ],
   "source": [
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [SystemMessage(content=(\"You are a helpful assistant that re-writes the user's text to \"\n",
    "                            \"sound more upbeat.\")),\n",
    "     HumanMessagePromptTemplate.from_template(\"{text}\"),\n",
    "    ])\n",
    "\n",
    "messages = chat_template.format_messages(text=\"I don't like eating tasty things\")\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48LkjjFSKEah"
   },
   "source": [
    "## 5.3 Message Prompts  \n",
    "LangChain provides different types of MessagePromptTemplate. The most commonly used are\n",
    "+ AIMessagePromptTemplate  \n",
    "+ SystemMessagePromptTemplate  \n",
    "+ HumanMessagePromptTemplate  \n",
    "\n",
    "Which create an AI message, system message and human message respectively.  \n",
    "In cases where the chat model supports taking chat message with arbitrary role, you can use ChatMessagePromptTemplate, which allows user to specify the role name.  \n",
    "https://python.langchain.com/docs/modules/model_io/chat/message_types/  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 224,
     "status": "ok",
     "timestamp": 1713479238486,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "CD6XMqdiJicg",
    "outputId": "8e2700cf-29ff-4341-e6c2-e2237f3ea44b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatMessage(content='May the force be with you', role='Jedi')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"May the {subject} be with you\"\n",
    "chat_message_prompt = ChatMessagePromptTemplate.from_template(role=\"Jedi\", template=prompt)\n",
    "chat_message_prompt.format(subject=\"force\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "exBvBGtwUote"
   },
   "source": [
    "# 5.4 MessagesPlaceholder  \n",
    "LangChain also provides MessagesPlaceholder, which gives you full control of what messages to be rendered during formatting. This can be useful when you are uncertain of what role you should be using for your message prompt templates or when you wish to insert a list of messages during formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 274,
     "status": "ok",
     "timestamp": 1713479503244,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "SecPlj7fKjVY",
    "outputId": "70c306a1-6724-49dd-ae70-3b789b775947"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='What is the best way to learn programming?'),\n",
       " AIMessage(content='1. Choose a programming language: Decide on a programming language that you want to learn.\\n2. Start with the basics: Familiarize yourself with the basic programming concepts such as variables, data types and control structures.\\n3. Practice, practice, practice: The best way to learn programming is through hands-on experience'),\n",
       " HumanMessage(content='Summarize our conversation so far in 10 words.')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_prompt = \"Summarize our conversation so far in {word_count} words.\"\n",
    "human_message_template = HumanMessagePromptTemplate.from_template(human_prompt)\n",
    "chat_prompt = ChatPromptTemplate.from_messages([MessagesPlaceholder(variable_name=\"conversation\"),\n",
    "                                                human_message_template])\n",
    "human_message = HumanMessage(content=\"What is the best way to learn programming?\")\n",
    "ai_message = AIMessage(content=\"\"\"\\\n",
    "1. Choose a programming language: Decide on a programming language that you want to learn.\n",
    "2. Start with the basics: Familiarize yourself with the basic programming concepts such as variables, data types and control structures.\n",
    "3. Practice, practice, practice: The best way to learn programming is through hands-on experience\\\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "chat_prompt.format_prompt(conversation=[human_message, ai_message], word_count=\"10\").to_messages()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eg7X0M2hYKgd"
   },
   "source": [
    "# 6 - [Types of Output Parsers](https://python.langchain.com/docs/modules/model_io/output_parsers/)  \n",
    "\n",
    "Language models output text. But many times you may want to get more structured information than just text back. This is where output parsers come in. Output parsers are classes that help structure language model responses.  \n",
    "There are two main methods an output parser must implement:  \n",
    "+ “Get format instructions”: A method which returns a string containing instructions for how the output of a language model should be formatted.\n",
    "+ “Parse”: A method which takes in a string (assumed to be the response from a language model) and parses it into some structure.\n",
    "\n",
    "And then one optional one:\n",
    "\n",
    "+ “Parse with prompt”: A method which takes in a string (assumed to be the response from a language model) and a prompt (assumed to be the prompt that generated such a response) and parses it into some structure. The prompt is largely provided in the event the OutputParser wants to retry or fix the output in some way, and needs information from the prompt to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BZHW6BBHh5OT"
   },
   "source": [
    "Let's explore the **CSV Parser**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 1022,
     "status": "ok",
     "timestamp": 1713515934432,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "juqjG-q5h_zb"
   },
   "outputs": [],
   "source": [
    "output_parser = CommaSeparatedListOutputParser()\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "prompt = PromptTemplate(template=\"List five {subject}.\\n{format_instructions}\",\n",
    "                        input_variables=[\"subject\"],\n",
    "                        partial_variables={\"format_instructions\": format_instructions},\n",
    ")\n",
    "\n",
    "model = ChatOpenAI(temperature=0)\n",
    "\n",
    "chain = prompt | model | output_parser\n",
    "a = chain.invoke({\"subject\": \"ice cream flavors\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 762,
     "status": "ok",
     "timestamp": 1713483705005,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "4FuMrePlllnR",
    "outputId": "4aa6ded2-d67b-4ac9-8371-40b1d5a470d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Vanilla']\n",
      "['Chocolate']\n",
      "['Strawberry']\n",
      "['Mint Chocolate Chip']\n",
      "['Cookies and Cream']\n"
     ]
    }
   ],
   "source": [
    "for s in chain.stream({\"subject\": \"ice cream flavors\"}):\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ISARcJkltMH"
   },
   "source": [
    "Let's take a look at the **Pandas Dataframe Parser**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 656,
     "status": "ok",
     "timestamp": 1713516190136,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "Kme-2Acllzar"
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "from typing import Any, Dict\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 288,
     "status": "ok",
     "timestamp": 1713516191328,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "E54nWnY2mFuF"
   },
   "outputs": [],
   "source": [
    "# Solely for documentation purposes.\n",
    "def format_parser_output(parser_output: Dict[str, Any]) -> None:\n",
    "    for key in parser_output.keys():\n",
    "        parser_output[key] = parser_output[key].to_dict()\n",
    "    return pprint.PrettyPrinter(width=4, compact=True).pprint(parser_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fZnAeaY8mnXf"
   },
   "source": [
    "Define your desired Pandas DataFrame and set up a parser + inject instructions into the prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1713516193677,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "EzjjJb65mNe8"
   },
   "outputs": [],
   "source": [
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"num_legs\": [2, 4, 8, 0],\n",
    "        \"num_wings\": [2, 0, 0, 0],\n",
    "        \"num_specimen_seen\": [10, 2, 1, 8],\n",
    "    }\n",
    ")\n",
    "\n",
    "parser = PandasDataFrameOutputParser(dataframe=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X5i7udQWmwEh"
   },
   "source": [
    "Here's an example of a column operation being performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 539,
     "status": "ok",
     "timestamp": 1713516450242,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "kLj8jwNZmNRa",
    "outputId": "5e4b4615-2069-4dd4-dc3a-6afa84924bbc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_wings': {0: 2,\n",
      "               1: 0,\n",
      "               2: 0,\n",
      "               3: 0}}\n"
     ]
    }
   ],
   "source": [
    "df_query = \"Retrieve the num_wings column.\"\n",
    "prompt = PromptTemplate(template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "                        input_variables=[\"query\"],\n",
    "                        partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "                        )\n",
    "\n",
    "chain = prompt | model | parser\n",
    "parser_output = chain.invoke({\"query\": df_query})\n",
    "format_parser_output(parser_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZXcQRVcbm3Fi"
   },
   "source": [
    "Here's an example of a row operation being performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 725,
     "status": "ok",
     "timestamp": 1713516458832,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "EyLK0qB6mUig",
    "outputId": "9b4b8247-e030-4d58-a9bd-1a0f97df381a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': {'num_legs': 2,\n",
      "       'num_specimen_seen': 10,\n",
      "       'num_wings': 2}}\n"
     ]
    }
   ],
   "source": [
    "df_query = \"Retrieve the first row.\"\n",
    "prompt = PromptTemplate(template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "                        input_variables=[\"query\"],\n",
    "                        partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "                        )\n",
    "chain = prompt | model | parser\n",
    "parser_output = chain.invoke({\"query\": df_query})\n",
    "format_parser_output(parser_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o2cWky88aEpC"
   },
   "source": [
    "Below we go over a more powerful output parser, the **PydanticOutputParser**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lb5BA6iGaaBd"
   },
   "source": [
    "a) Define your desired data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "executionInfo": {
     "elapsed": 263,
     "status": "ok",
     "timestamp": 1713516574944,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "suvoyAIIYnbk"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "    # You can add custom validation logic easily with Pydantic.\n",
    "    @validator(\"setup\")\n",
    "    def question_ends_with_question_mark(cls, field):\n",
    "        if field[-1] != \"?\":\n",
    "            raise ValueError(\"Badly formed question!\")\n",
    "        return field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pFiNe5uxai_b"
   },
   "source": [
    "b) Set up a parser + inject instructions into the prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "executionInfo": {
     "elapsed": 243,
     "status": "ok",
     "timestamp": 1713516578193,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "tLwcQWypagFB"
   },
   "outputs": [],
   "source": [
    "parser = PydanticOutputParser(pydantic_object=Joke)\n",
    "prompt = PromptTemplate(template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "                        input_variables=[\"query\"],\n",
    "                        partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rGshQwP-a2Xt"
   },
   "source": [
    "c) And a query intended to prompt a language model to populate the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1232,
     "status": "ok",
     "timestamp": 1713516602581,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "jAGBOHFwa3Sa",
    "outputId": "187bb9cf-2995-4359-d9b0-c69405767b0e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Joke(setup=\"Why couldn't the bicycle stand up by itself?\", punchline='Because it was two tired!')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_and_model = prompt | model\n",
    "output = prompt_and_model.invoke({\"query\": \"Tell me a joke.\"})\n",
    "parser.invoke(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "0de_5EL6b2hp"
   },
   "outputs": [],
   "source": [
    "json_prompt = PromptTemplate.from_template(\n",
    "    \"Return a JSON object with an `answer` key that answers the following question: {question}\"\n",
    ")\n",
    "json_parser = SimpleJsonOutputParser()\n",
    "json_chain = json_prompt | model | json_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 - [Memory](https://python.langchain.com/v0.1/docs/modules/memory/)  \n",
    "\n",
    "Most LLM applications have a conversational interface. An essential component of a conversation is being able to refer to information introduced earlier in the conversation. At bare minimum, a conversational system should be able to access some window of past messages directly. A more complex system will need to have a world model that it is constantly updating, which allows it to do things like maintain information about entities and their relationships.\n",
    "\n",
    "### Outline\n",
    "* ConversationBufferMemory\n",
    "* ConversationBufferWindowMemory\n",
    "* ConversationTokenBufferMemory\n",
    "* ConversationSummaryMemory\n",
    "\n",
    "![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQcYZAK7bD8PPvjbAIe5tLk19SX9zKaSGHlVrE_vKrDk09tCgj7ujp_re7SpYHI8I7yUA&usqp=CAU) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 - ConversationBufferMemory  \n",
    "\n",
    "We are going to use the simplest memory type.  \n",
    "Let's check what is happening behind the scenes using the ```verbose``` mode of the LLM:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, my name is Renato\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello Renato! It's nice to meet you. How can I assist you today?\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(llm=model, memory=memory, verbose=True)\n",
    "conversation.predict(input=\"Hi, my name is Renato\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Renato\n",
      "AI: Hello Renato! It's nice to meet you. How can I assist you today?\n",
      "Human: What is 1+1?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1 + 1 equals 2. Is there anything else you would like to know?'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Renato\n",
      "AI: Hello Renato! It's nice to meet you. How can I assist you today?\n",
      "Human: What is 1+1?\n",
      "AI: 1 + 1 equals 2. Is there anything else you would like to know?\n",
      "Human: What is my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your name is Renato.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### As we are using a memory module, we can check what is stored in there:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi, my name is Renato\n",
      "AI: Hello Renato! It's nice to meet you. How can I assist you today?\n",
      "Human: What is 1+1?\n",
      "AI: 1 + 1 equals 2. Is there anything else you would like to know?\n",
      "Human: What is my name?\n",
      "AI: Your name is Renato.\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi, my name is Renato\\nAI: Hello Renato! It's nice to meet you. How can I assist you today?\\nHuman: What is 1+1?\\nAI: 1 + 1 equals 2. Is there anything else you would like to know?\\nHuman: What is my name?\\nAI: Your name is Renato.\"}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now let's erase the memory buffer and insert something else in it:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi\n",
      "AI: What's up\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationBufferMemory()\n",
    "\n",
    "memory.save_context({\"input\": \"Hi\"}, \n",
    "                    {\"output\": \"What's up\"})\n",
    "\n",
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi\\nAI: What's up\"}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's add even more context:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi\\nAI: What's up\\nHuman: Not much, just hanging\\nAI: Cool\"}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.save_context({\"input\": \"Not much, just hanging\"}, \n",
    "                    {\"output\": \"Cool\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now, let's check if the model still know the information from a previous interaction:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f5cd994-d4f1-49c3-acd2-f39e471eaa82",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi\n",
      "AI: What's up\n",
      "Human: Not much, just hanging\n",
      "AI: Cool\n",
      "Human: What is my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, I do not have access to your personal information, so I do not know your name.\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation = ConversationChain(llm=model,\n",
    "                                 memory=memory,\n",
    "                                 verbose=True\n",
    "                                 )\n",
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "534bc8b5-433a-4476-a3ee-a66cda0ed923",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## ConversationBufferWindowMemory  \n",
    "\n",
    "We can define a buffer size to determine how much context will be kept:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf576a1d-5b6d-46bb-b6f9-05d6c7ef8853",
     "showTitle": false,
     "title": ""
    },
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Not much, just hanging\\nAI: Cool'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "\n",
    "memory.save_context({\"input\": \"Hi\"},\n",
    "                    {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e078bbf4-b9e5-42b6-b970-7e8c60b0e3a2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### We see that only one interaction was saved. Now let's see how it would be with the LLM:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6f4f0c4-fbb3-4983-9f3e-ab03ae5b039f",
     "showTitle": false,
     "title": ""
    },
    "height": 132
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Renato! It's nice to meet you. How can I assist you today?\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "conversation = ConversationChain(llm=model, memory=memory, verbose=False)\n",
    "conversation.predict(input=\"Hi, my name is Renato\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "784f5ca5-c343-4f58-9c21-413095ba833e",
     "showTitle": false,
     "title": ""
    },
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1+1 equals 2. Is there anything else you would like to know?'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb6b6b71-5700-4d90-ba48-dd78b9fa8a70",
     "showTitle": false,
     "title": ""
    },
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, I do not have access to personal information such as your name. Is there anything else you would like to know?\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fdf9c65-c96c-4139-8e52-b46855ded00f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## ConversationTokenBufferMemory\n",
    "\n",
    "Similarly, we can define a buffer size in the quantity of tokens:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a4a2e17-608f-4d7a-8761-568c928e210b",
     "showTitle": false,
     "title": ""
    },
    "height": 132
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'AI: Something Beautiful!\\nHuman: Chatbots are what?\\nAI: Charming!'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = ConversationTokenBufferMemory(llm=model, max_token_limit=30)\n",
    "\n",
    "memory.save_context({\"input\": \"AI is what?!\"},\n",
    "                    {\"output\": \"Amazing!\"})\n",
    "\n",
    "memory.save_context({\"input\": \"Backpropagation is what?\"},\n",
    "                    {\"output\": \"Something Beautiful!\"})\n",
    "\n",
    "memory.save_context({\"input\": \"Chatbots are what?\"}, \n",
    "                    {\"output\": \"Charming!\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "143f5c8b-28e3-40a1-a78a-241ff1eef074",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## ConversationSummaryMemory  \n",
    "\n",
    "This last type of memory creates an automatic summary of the previous interactions to store:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "704981c9-5f30-4bd9-8f26-36d00331e87e",
     "showTitle": false,
     "title": ""
    },
    "height": 285
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'System: The human and AI exchange greetings and discuss the schedule for the day. The AI informs the human of a morning meeting with the product team, work on the LangChain project, and a lunch meeting with a customer interested in AI. The human is advised to bring their laptop to show the latest LLM demo during the lunch meeting.'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a long string\n",
    "schedule = \"There is a meeting at 8am with your product team. \\\n",
    "You will need your powerpoint presentation prepared. \\\n",
    "9am-12pm have time to work on your LangChain \\\n",
    "project which will go quickly because Langchain is such a powerful tool. \\\n",
    "At Noon, lunch at the italian resturant with a customer who is driving \\\n",
    "from over an hour away to meet you to understand the latest in AI. \\\n",
    "Be sure to bring your laptop to show the latest LLM demo.\"\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(llm=model, max_token_limit=100)\n",
    "\n",
    "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})\n",
    "memory.save_context({\"input\": \"What is on the schedule today?\"}, \n",
    "                    {\"output\": f\"{schedule}\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7f74747-7948-4e96-9f47-2ceba8d34d87",
     "showTitle": false,
     "title": ""
    },
    "height": 98
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: The human and AI exchange greetings and discuss the schedule for the day. The AI informs the human of a morning meeting with the product team, work on the LangChain project, and a lunch meeting with a customer interested in AI. The human is advised to bring their laptop to show the latest LLM demo during the lunch meeting.\n",
      "Human: What would be a good demo to show?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"For the LLM demo, it would be great to showcase the latest language model capabilities, such as text generation, language translation, and sentiment analysis. You could also highlight the model's accuracy and efficiency compared to other existing models in the market. Additionally, demonstrating how the model can be easily integrated into different applications or platforms would be impressive to the customer.\""
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation = ConversationChain(llm=model, memory=memory, verbose=True)\n",
    "conversation.predict(input=\"What would be a good demo to show?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50141950-8a4e-453a-80c7-c07a9e369112",
     "showTitle": false,
     "title": ""
    },
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"System: The human and AI exchange greetings and discuss the schedule for the day. The AI informs the human of a morning meeting with the product team, work on the LangChain project, and a lunch meeting with a customer interested in AI. The human is advised to bring their laptop to show the latest LLM demo during the lunch meeting.\\nHuman: What would be a good demo to show?\\nAI: For the LLM demo, it would be great to showcase the latest language model capabilities, such as text generation, language translation, and sentiment analysis. You could also highlight the model's accuracy and efficiency compared to other existing models in the market. Additionally, demonstrating how the model can be easily integrated into different applications or platforms would be impressive to the customer.\"}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNRBbpHqLlsR3qsDTCKjs0x",
   "provenance": [
    {
     "file_id": "1LtCkSgnYYJO5Q4IxLEbUqn4Chmc-vJPw",
     "timestamp": 1713469816474
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
