{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ioLoShcpx6H7"
   },
   "source": [
    "\n",
    "# Intro to LangChain\n",
    "\n",
    "LangChain is an open-source framework that gives developers the tools they need to create applications using large language models (LLMs). In its essence, LangChain is a prompt orchestration tool that makes it easier for teams to connect various prompts interactively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 55685,
     "status": "ok",
     "timestamp": 1713514624058,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "miOF_GZU5mst",
    "outputId": "489dd3e4-06e9-4fb0-97d4-acb640612a24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pydantic[email] in c:\\users\\renato rocha souza\\appdata\\roaming\\python\\python39\\site-packages (2.9.2)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\renato rocha souza\\appdata\\roaming\\python\\python39\\site-packages (from pydantic[email]) (2.23.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\renato rocha souza\\appdata\\roaming\\python\\python39\\site-packages (from pydantic[email]) (4.12.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\renato rocha souza\\appdata\\roaming\\python\\python39\\site-packages (from pydantic[email]) (0.6.0)\n",
      "Collecting email-validator>=2.0.0\n",
      "  Downloading email_validator-2.2.0-py3-none-any.whl (33 kB)\n",
      "Requirement already satisfied: idna>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from email-validator>=2.0.0->pydantic[email]) (3.3)\n",
      "Collecting dnspython>=2.0.0\n",
      "  Downloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
      "     -------------------------------------- 313.6/313.6 kB 1.5 MB/s eta 0:00:00\n",
      "Installing collected packages: dnspython, email-validator\n",
      "Successfully installed dnspython-2.7.0 email-validator-2.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script email_validator.exe is installed in 'C:\\Users\\Renato Rocha Souza\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU openai\n",
    "%pip install -qU langchain\n",
    "%pip install -qU langchain-openai\n",
    "%pip install -qU langchain-ollama\n",
    "%pip install -qU tiktoken\n",
    "%pip install -qU pydantic[email]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yrb7nxgq7nvR"
   },
   "source": [
    "# Accessing OpenAI Directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 286,
     "status": "ok",
     "timestamp": 1713514904192,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "YoxylcB85gDr"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "#os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "client = openai.OpenAI()\n",
    "\n",
    "def get_completion(prompt, model=\"gpt-4o-mini\"):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "        ]\n",
    "    response = client.chat.completions.create(model=model,\n",
    "                                              messages=messages,\n",
    "                                              temperature=0.2\n",
    "                                              )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m8GyMwn47XMn"
   },
   "source": [
    "## Simple Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1048,
     "status": "ok",
     "timestamp": 1713514927826,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "ZKQp2YrMyBvz",
    "outputId": "dc37e5db-bf91-44f4-97df-f7be826be323"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 + 1 equals 2.\n"
     ]
    }
   ],
   "source": [
    "response = get_completion(\"Can you tell me how much is 1+1?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mw2bnNelykOv"
   },
   "source": [
    "## Queries with custom prompts using formatted strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 361,
     "status": "ok",
     "timestamp": 1713515082959,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "GrEcvuU7yjtu",
    "outputId": "995ff889-84b8-4f18-e439-1c04fb8cf208"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate the text that is delimited by triple backticks\n",
      "into a style that is American English in a calm and respectful tone\n",
      ".\n",
      "text: ```\n",
      "Arrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse,the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_email = \"\"\"\n",
    "Arrr, I be fuming that me blender lid \\\n",
    "flew off and splattered me kitchen walls \\\n",
    "with smoothie! And to make matters worse,\\\n",
    "the warranty don't cover the cost of \\\n",
    "cleaning up me kitchen. I need yer help \\\n",
    "right now, matey!\n",
    "\"\"\"\n",
    "\n",
    "style = \"\"\"American English \\\n",
    "in a calm and respectful tone\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"Translate the text \\\n",
    "that is delimited by triple backticks\n",
    "into a style that is {style}.\n",
    "text: ```{customer_email}```\n",
    "\"\"\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1688,
     "status": "ok",
     "timestamp": 1713515087348,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "haWe0_HNys3B",
    "outputId": "355dbeba-c25d-4e23-bc49-0015e9121250"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am quite frustrated that the lid of my blender came off and splattered my kitchen walls with smoothie. To make matters worse, the warranty does not cover the cost of cleaning up my kitchen. I would appreciate your assistance with this issue. Thank you.\n"
     ]
    }
   ],
   "source": [
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why should we use a framework to wrap the direct calls to OpenAI?    \n",
    "\n",
    "The LangChain pipelines consist of the following modules:\n",
    "\n",
    "+ **Models**: Models mostly cover Large Language models. A large language model of considerable size is a model that comprises a neural network with numerous parameters and is trained on vast quantities of unlabeled text  \n",
    "+ **Prompts**: prompt is the input that we give to any system to refine our answers to make them more accurate or more specific according to our use case. Many times you may want to get more structured information than just text back. We can use Prompts in conjunction with Parsers\n",
    "+ **Parsers**: Output parsers are responsible for taking the output of an LLM and transforming it to a more suitable format. This is very useful when you are using LLMs to generate any form of structured data.  \n",
    "+ **Memory**: Chains and Agents in LangChain operate in a stateless mode by default, meaning that they handle each incoming query independently. However, there are certain applications, like chatbots, where it is of great importance to retain previous interactions, both over the short and long term. This is where the concept of ‚ÄúMemory‚Äù comes into play.  \n",
    "+ **Chains**: Chains provide a means to merge various components into a unified application. A chain can be created, for instance, that receives input from a user, formats it using a PromptTemplate, and subsequently transmits the formatted reply to an LLM. More intricate chains can be generated by integrating multiple chains with other components.  \n",
    "+ **Agents**: Certain applications may necessitate not only a pre-determined sequence of LLM/other tool calls but also an uncertain sequence that is dependent on the user‚Äôs input. These kinds of sequences include an ‚Äúagent‚Äù that has access to a range of tools. Based on the user input, the agent may determine which of these tools, if any, should be called.  \n",
    "+ **Callbacks**: Allow you to hook into the various stages of your LLM application. This is useful for logging, monitoring, streaming, and other tasks.  \n",
    "+ **Indexes**: Indexes are information stored in local databases that can be used for Augment the capabilities of the models being used. We will see them in action with RAG pipelines  \n",
    "\n",
    "Let's wrap the pipeline:\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:4800/format:webp/1*05zEoeNU7DVYOFzjugiF_w.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NkjFnz46y0nQ"
   },
   "source": [
    "\n",
    "# I - Simple Pipelines with Langchain  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1713515364077,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "Okkq2c5w_eI8"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.prompts import HumanMessagePromptTemplate\n",
    "from langchain_core.prompts import ChatMessagePromptTemplate\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.messages import AIMessage\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "import json\n",
    "from pydantic import BaseModel, Field, model_validator, EmailStr, ValidationError\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain.output_parsers import PandasDataFrameOutputParser\n",
    "from langchain.output_parsers.json import SimpleJsonOutputParser\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "from langchain.chains import ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "executionInfo": {
     "elapsed": 615,
     "status": "ok",
     "timestamp": 1713515361990,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "bh56xPYvyv16"
   },
   "outputs": [],
   "source": [
    "#os.environ[\"OPENAI_API_KEY\"] = \"<the key>\"\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "model1 = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "model2 = ChatOpenAI(model=\"gpt-4o\")\n",
    "model3 = ChatOpenAI(model=\"gpt-4o-mini\", \n",
    "                   temperature=0)\n",
    "model4 = OllamaLLM(model=\"gemma2:2b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a3I6VXnL0OVp"
   },
   "source": [
    "### Once you've installed and initialized the LLM of your choice, we could use it!   \n",
    "### Let's ask it some question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2939,
     "status": "ok",
     "timestamp": 1713515370013,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "WuKYnH2Jzxb8",
    "outputId": "f4623a5d-d7c8-4501-b6ff-3a491fdd49dc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"The sky appears blue primarily due to a phenomenon known as Rayleigh scattering. When sunlight enters the Earth's atmosphere, it is made up of different colors, which correspond to different wavelengths of light. Blue light has a shorter wavelength compared to other colors, such as red, which has a longer wavelength.\\n\\nAs sunlight passes through the atmosphere, it interacts with air molecules and small particles. The shorter wavelengths of light (blue and violet) are scattered in all directions more than the longer wavelengths (red, orange, yellow). Although violet light is scattered even more than blue light, our eyes are more sensitive to blue light, and some of the violet light is absorbed by the ozone layer, making the sky predominantly appear blue to us.\\n\\nDuring sunrise and sunset, the sky can appear red or orange because the sunlight has to pass through a thicker layer of the atmosphere. This means that the shorter blue wavelengths are scattered out of our line of sight, and the longer red wavelengths become more prominent.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 195, 'prompt_tokens': 13, 'total_tokens': 208, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0ba0d124f1', 'finish_reason': 'stop', 'logprobs': None}, id='run-000452a1-6bd0-492b-a076-b2a6981e4687-0', usage_metadata={'input_tokens': 13, 'output_tokens': 195, 'total_tokens': 208, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.invoke(\"Why is the sky blue?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The blue color of the sky is a result of a fascinating phenomenon called **Rayleigh scattering**. Here's how it works:\\n\\n**1. Sunlight and Molecules:** \\nSunlight, which appears white to our eyes, is actually made up of all the colors of the rainbow.  When this sunlight enters Earth's atmosphere, it interacts with tiny gas molecules (mainly nitrogen and oxygen) present in the air.\\n\\n**2. Scattering:** \\nThese molecules scatter light in all directions when it hits them. However, blue light scatters more effectively than other colours due to a specific property of its wavelength.  Blue light has shorter wavelengths than red or orange light.\\n\\n**3. Blue Sky:** \\nThe scattered blue light reaches our eyes from all directions across the sky, making us see a predominantly blue color. This is why we often see a clear, bright blue sky during the day.\\n\\n **Additional Points:**\\n\\n* **Time of Day:** At sunrise and sunset, the sunlight travels through a thicker layer of atmosphere.  This causes more scattering of shorter wavelengths like blue light, resulting in warm colors like orange and red.\\n* **Cloud Color:** Clouds appear white or gray because they are made up of larger water droplets that scatter all colors of light equally. \\n\\n\\nLet me know if you have any other questions! üåé üåû üåà \\n\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4.invoke(\"Why is the sky blue?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EhhGOhxq0W_W"
   },
   "source": [
    "### Prompt templates convert raw user input to better input to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3054,
     "status": "ok",
     "timestamp": 1713515377035,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "PnJOraQa0E8I",
    "outputId": "32f2b136-dcaa-43f6-f1ab-acd1643374fe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"That's a great question!  Did you know that the sky looks blue because of something called **sunlight** and **tiny things in the air**? \\n\\nImagine sunlight as tiny, bouncy balls. They hit the air and scatter all over the place. The blue light from the sun is like those bouncy balls bouncing around more than other colors -  so it gets scattered all around!  That's why we see the sky as blue most of the time. üåà \\n\\nBut sometimes the sky can look different, right? ü§î  It might be red or orange when the sun is setting because the sunlight has to travel a longer distance through more air.  And if there are clouds in the sky, they can block out the sunlight and make the sky appear gray!\\n\\n\\nDo you have any other questions about why the sky looks so amazing? üòä \\n\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([(\"system\", \"You are a first grade teacher.\"),\n",
    "                                           (\"user\", \"{input}\")\n",
    "                                         ])\n",
    "\n",
    "chain = prompt | model4\n",
    "chain.invoke({\"input\": \"Why is the sky blue?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n5cGE6b42KE9"
   },
   "source": [
    "### The output of a ChatModel (and therefore, of this chain) is a message object. However, it's often much more convenient to work with strings.  \n",
    "### Let's add a simple output parser to convert the chat message to a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 313,
     "status": "ok",
     "timestamp": 1713515382544,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "UsCRlie31g3u"
   },
   "outputs": [],
   "source": [
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "executionInfo": {
     "elapsed": 2646,
     "status": "ok",
     "timestamp": 1713515388989,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "46h0vZIf2PDV",
    "outputId": "c5c35513-e44e-4d8f-c235-df4557a0c432"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Okay, great question!  You know how sometimes you see rainbows in the rain and sunlight shines so brightly through the water? üåà‚òÄÔ∏è Well, that's what makes the sky look blue too!\\n\\nHere's the secret: The sun's light is made up of all different colors - just like a rainbow! üåà  But the air we breathe has tiny bits of stuff in it called molecules.  They are so small they let some colors through better than others. Blue and violet colors bounce around more easily because they are smaller! \\n\\nSo, when the sunlight shines on Earth's air, blue and violet light bounces around a lot more than the other colors, which makes us see the sky as blue! üíô  Pretty neat, huh? ‚ú®\\n\\n\\nDo you have any other questions about how things work in the world around us? üòä \\n\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | model4 | output_parser\n",
    "chain.invoke({\"input\": \"Why is the sky blue?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3MHk0lzH3fuN"
   },
   "source": [
    "# II - Exploring Chain Elements - Langchain Expression Language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d3hI0QJe45VY"
   },
   "source": [
    "Notice this line of the code, where we piece together these different components into a single chain using LCEL:  \n",
    "\n",
    "**chain = prompt | model | output_parser**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XW6Fd-Ko4YvE"
   },
   "source": [
    "## 1. [Prompts](https://python.langchain.com/docs/modules/model_io/prompts/)    \n",
    "prompt is a BasePromptTemplate, which means it takes in a dictionary of template variables and produces a PromptValue.  \n",
    "+ A PromptValue is a wrapper around a completed prompt that can be passed to either a LLM (which takes a string as input) or ChatModel (which takes a sequence of messages as input).  \n",
    "+ It can work with either language model type because it defines logic both for producing BaseMessages and for producing a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 1316,
     "status": "ok",
     "timestamp": 1713474572510,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "E5ycNd1q3tcA",
    "outputId": "a8c073ad-6640-4ce9-e685-8f99d563c564"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why did the chicken cross the playground? \\n\\nTo get to the other slide! üòÇ \\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"tell me a short joke about {topic}\")\n",
    "\n",
    "chain = prompt | model4 | output_parser\n",
    "\n",
    "chain.invoke({\"topic\": \"chicken\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 221,
     "status": "ok",
     "timestamp": 1713474583609,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "p5s4zuDi4aFq",
    "outputId": "2a4a9515-797a-4517-ed23-495586dbf66c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content='tell me a short joke about ice cream', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_value = prompt.invoke({\"topic\": \"ice cream\"})\n",
    "prompt_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 213,
     "status": "ok",
     "timestamp": 1713474593988,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "u1hBd2io9dWI",
    "outputId": "adb45ee8-0068-4202-e85d-e9d15c25b9e7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='tell me a short joke about ice cream', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_value.to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 245,
     "status": "ok",
     "timestamp": 1713474597165,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "binqpy-79f4z",
    "outputId": "19d2ddf6-5aaf-45ff-e61b-ca722244e7da"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: tell me a short joke about ice cream'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_value.to_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "executionInfo": {
     "elapsed": 2380,
     "status": "ok",
     "timestamp": 1713474723187,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "bz5qa_TCDN4d",
    "outputId": "faa4f7a3-22d3-4e24-c416-4d281ebda444"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The sky looks blue because of a process called Rayleigh scattering. When sunlight comes into the Earth's atmosphere, it is made up of many colors, like a rainbow. Blue light is scattered in all directions by the tiny particles in the air. Since blue light is scattered more than the other colors, we see a blue sky during the day! On sunrise and sunset, the light has to travel through more air, and the blue light scatters away, allowing us to see more reds and oranges. Isn't that cool?\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([(\"system\", \"You are a first grade teacher.\"),\n",
    "                                           (\"user\", \"{input}\")\n",
    "                                         ])\n",
    "chain = prompt | model3 | output_parser\n",
    "chain.invoke({\"input\": \"Why is the sky blue?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rGLNmBQy5B8S"
   },
   "source": [
    "## 2. [Models](https://python.langchain.com/docs/modules/model_io/chat/quick_start/)    \n",
    "#### The PromptValue is then passed to model.  \n",
    "#### In this case our model is an OpenAI ChatModel, meaning it will output a BaseMessage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1433,
     "status": "ok",
     "timestamp": 1713473317439,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "X2wt5_jf4eEe",
    "outputId": "4f2a654e-77fe-4f9c-d29e-4c60fcb2a29d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='What did the ice cream cone say to the scoop? \\n\\n\"I‚Äôm sweet on you!\" üç¶' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 15, 'total_tokens': 35, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0ba0d124f1', 'finish_reason': 'stop', 'logprobs': None} id='run-9a5e2128-09bb-448d-86ae-38736a840c41-0' usage_metadata={'input_tokens': 15, 'output_tokens': 20, 'total_tokens': 35, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "<class 'langchain_core.messages.ai.AIMessage'>\n"
     ]
    }
   ],
   "source": [
    "message = model3.invoke(prompt_value)\n",
    "print(message)\n",
    "print(type(message))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZCME5D_A5VTc"
   },
   "source": [
    "#### If our model was an Ollama LLM, it would output a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the ice cream cone go to the doctor? \n",
      "\n",
      "Because it was feeling crumby! üòÇüç¶ \n",
      "\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "message = model4.invoke(prompt_value)\n",
    "print(message)\n",
    "print(type(message))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 1237,
     "status": "ok",
     "timestamp": 1713473332520,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "yREwkxMj5J5E",
    "outputId": "e27443f4-67be-4694-d19d-ffeee3b8ac6f"
   },
   "source": [
    "#### That is why we should treat the model output with an output parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GEeoPLa652D3"
   },
   "source": [
    "## 3. [Output parsers](https://python.langchain.com/docs/modules/model_io/output_parsers/)    \n",
    "And lastly we pass our model output to the output_parser, which is a BaseOutputParser meaning it takes either a string or a BaseMessage as input. The specific StrOutputParser simply converts any input into a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 199,
     "status": "ok",
     "timestamp": 1713473342078,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "Kt6xF6wt5THU",
    "outputId": "c6ff9d82-73a0-4805-cbbf-e9aab16ffe3d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why did the ice cream cone go to the doctor? \\n\\nBecause it was feeling crumby! üòÇüç¶ \\n'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parser.invoke(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(message))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vifudtoO6Kt2"
   },
   "source": [
    "## 4. [Chains](https://python.langchain.com/v0.1/docs/modules/chains/)  \n",
    "\n",
    "#### Chains are combination of steps. We have followed the steps along:\n",
    "\n",
    "+ We passed the user input on the desired topic as {\"topic\": \"ice cream\"}\n",
    "+ The prompt component takes the user input, which is then used to construct a PromptValue after using the topic to construct the prompt.\n",
    "+ The model component takes the generated prompt, and passes into the OpenAI LLM model for evaluation.\n",
    "+ The generated output from the model is a ChatMessage object.\n",
    "+ Finally, the output_parser component takes in a ChatMessage, and transforms this into a Python string, which is returned from the invoke method.\n",
    "\n",
    "#### Langchain has many types of chains using [LCEL](https://python.langchain.com/docs/how_to/#langchain-expression-language-lcel). We are just exploring some in this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3K1Ovj-EZr7"
   },
   "source": [
    "# 5. [Types of Prompts](https://python.langchain.com/docs/modules/model_io/prompts/quick_start/)  \n",
    "#### Let's explore some other types of Prompts  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xqYB7XnjEqJV"
   },
   "source": [
    "## 5.1 PromptTemplate\n",
    "\n",
    "Use PromptTemplate to create a template for a string prompt.  \n",
    "By default, PromptTemplate uses Python‚Äôs str.format syntax for templating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 250,
     "status": "ok",
     "timestamp": 1713515601642,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "GEsb8EBw57ld",
    "outputId": "2a759fbf-62b8-4850-a4d5-7b288ba70dc8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell me a funny joke about chickens.'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = PromptTemplate.from_template(\"Tell me a {adjective} joke about {content}.\")\n",
    "prompt_template.format(adjective=\"funny\", content=\"chickens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 291,
     "status": "ok",
     "timestamp": 1713515623928,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "MR0Ip8O6E1db",
    "outputId": "d64d8ca5-92fd-4381-9311-06879338c46c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell me a joke'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = PromptTemplate.from_template(\"Tell me a joke\")\n",
    "prompt_template.format()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4zLArLmYFBK1"
   },
   "source": [
    "## 5.2 ChatPromptTemplate\n",
    "\n",
    "The prompt to chat models/ is a list of chat messages.  \n",
    "Each chat message is associated with content, and an additional parameter called role. For example, in the OpenAI Chat Completions API, a chat message can be associated with an AI assistant, a human or a system role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 222,
     "status": "ok",
     "timestamp": 1713476275035,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "2jUJRy4RE7bL",
    "outputId": "eed9ebea-f868-418f-8ad8-c31efdae3426"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful AI bot. Your name is Bob.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Hello, how are you doing?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"I'm doing well, thanks!\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='What is your name?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
    "        (\"human\", \"Hello, how are you doing?\"),\n",
    "        (\"ai\", \"I'm doing well, thanks!\"),\n",
    "        (\"human\", \"{user_input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "messages = chat_template.format_messages(name=\"Bob\", user_input=\"What is your name?\")\n",
    "messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "erMamCwOFWiY"
   },
   "source": [
    "#### This is equivalent as doing the following, directly with OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "Rsgp2i-HFOT9"
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful AI bot. Your name is Bob.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Hello, how are you doing?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"I'm doing well, thanks!\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is your name?\"},\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SnzZFr30I1tt"
   },
   "source": [
    "The ChatPromptTemplate.from_messages static method accepts a variety of message representations and is a convenient way to format input to chat models with exactly the messages you want.  \n",
    "\n",
    "For example, in addition to using the 2-tuple representation of (type, content) used above, you could pass in an instance of MessagePromptTemplate or BaseMessage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 221,
     "status": "ok",
     "timestamp": 1713476445408,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "GkIPB2MsI5XR",
    "outputId": "cf7bd703-c86b-4038-ada4-e7ba2797959e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content=\"You are a helpful assistant that re-writes the user's text to sound more upbeat.\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"I don't like eating tasty things\", additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [SystemMessage(content=(\"You are a helpful assistant that re-writes the user's text to \"\n",
    "                            \"sound more upbeat.\")),\n",
    "     HumanMessagePromptTemplate.from_template(\"{text}\"),\n",
    "    ])\n",
    "\n",
    "\n",
    "messages = chat_template.format_messages(text=\"I don't like eating tasty things\")\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48LkjjFSKEah"
   },
   "source": [
    "## 5.3 Message Prompts  \n",
    "LangChain provides different types of MessagePromptTemplate. The most commonly used are\n",
    "+ AIMessagePromptTemplate  \n",
    "+ SystemMessagePromptTemplate  \n",
    "+ HumanMessagePromptTemplate  \n",
    "\n",
    "Which create an AI message, system message and human message respectively.  \n",
    "In cases where the chat model supports taking chat message with arbitrary role, you can use ChatMessagePromptTemplate, which allows user to specify the role name.  \n",
    "https://python.langchain.com/docs/modules/model_io/chat/message_types/  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 224,
     "status": "ok",
     "timestamp": 1713479238486,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "CD6XMqdiJicg",
    "outputId": "8e2700cf-29ff-4341-e6c2-e2237f3ea44b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatMessage(content='May the force be with you', additional_kwargs={}, response_metadata={}, role='Jedi')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"May the {subject} be with you\"\n",
    "\n",
    "chat_message_prompt = ChatMessagePromptTemplate.from_template(role=\"Jedi\", template=prompt)\n",
    "chat_message_prompt.format(subject=\"force\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "exBvBGtwUote"
   },
   "source": [
    "# 5.4 MessagesPlaceholder  \n",
    "LangChain also provides MessagesPlaceholder, which gives you full control of what messages to be rendered during formatting. This can be useful when you are uncertain of what role you should be using for your message prompt templates or when you wish to insert a list of messages during formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 274,
     "status": "ok",
     "timestamp": 1713479503244,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "SecPlj7fKjVY",
    "outputId": "70c306a1-6724-49dd-ae70-3b789b775947"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='What is the best way to learn programming?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='1. Choose a programming language: Decide on a programming language that you want to learn.\\n2. Start with the basics: Familiarize yourself with the basic programming concepts such as variables, \\ndata types and control structures.\\n3. Practice, practice, practice: The best way to learn programming is through hands-on experience', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Summarize our conversation so far in 10 words.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_prompt = \"Summarize our conversation so far in {word_count} words.\"\n",
    "human_message_template = HumanMessagePromptTemplate.from_template(human_prompt)\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [MessagesPlaceholder(variable_name=\"conversation\"),\n",
    "     human_message_template]\n",
    ")\n",
    "\n",
    "human_message = HumanMessage(content=\"What is the best way to learn programming?\")\n",
    "ai_message = AIMessage(content=\"\"\"\\\n",
    "1. Choose a programming language: Decide on a programming language that you want to learn.\n",
    "2. Start with the basics: Familiarize yourself with the basic programming concepts such as variables, \n",
    "data types and control structures.\n",
    "3. Practice, practice, practice: The best way to learn programming is through hands-on experience\\\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "chat_prompt.format_prompt(conversation=[human_message, ai_message], word_count=\"10\").to_messages()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eg7X0M2hYKgd"
   },
   "source": [
    "# 6 - [Types of Output Parsers](https://python.langchain.com/docs/modules/model_io/output_parsers/)  \n",
    "\n",
    "Language models output text. But many times you may want to get more structured information than just text back. This is where output parsers come in. Output parsers are classes that help structure language model responses.  \n",
    "There are two main methods an output parser must implement:  \n",
    "+ ‚ÄúGet format instructions‚Äù: A method which returns a string containing instructions for how the output of a language model should be formatted.\n",
    "+ ‚ÄúParse‚Äù: A method which takes in a string (assumed to be the response from a language model) and parses it into some structure.\n",
    "\n",
    "And then one optional one:\n",
    "\n",
    "+ ‚ÄúParse with prompt‚Äù: A method which takes in a string (assumed to be the response from a language model) and a prompt (assumed to be the prompt that generated such a response) and parses it into some structure. The prompt is largely provided in the event the OutputParser wants to retry or fix the output in some way, and needs information from the prompt to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BZHW6BBHh5OT"
   },
   "source": [
    "#### Let's explore the **CSV Parser**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "executionInfo": {
     "elapsed": 1022,
     "status": "ok",
     "timestamp": 1713515934432,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "juqjG-q5h_zb"
   },
   "outputs": [],
   "source": [
    "output_parser = CommaSeparatedListOutputParser()\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "prompt = PromptTemplate(template=\"List five {subject}.\\n{format_instructions}\",\n",
    "                        input_variables=[\"subject\"],\n",
    "                        partial_variables={\"format_instructions\": format_instructions},\n",
    ")\n",
    "\n",
    "chain = prompt | model4 | output_parser\n",
    "a = chain.invoke({\"subject\": \"ice cream flavors\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vanilla', 'chocolate', 'strawberry', 'mint chip', 'cookies and cream ']\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(a)\n",
    "print(type(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 762,
     "status": "ok",
     "timestamp": 1713483705005,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "4FuMrePlllnR",
    "outputId": "4aa6ded2-d67b-4ac9-8371-40b1d5a470d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Vanilla']\n",
      "['chocolate']\n",
      "['strawberry']\n",
      "['mint chip']\n",
      "['coffee ']\n"
     ]
    }
   ],
   "source": [
    "for s in chain.stream({\"subject\": \"ice cream flavors\"}):\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ISARcJkltMH"
   },
   "source": [
    "#### Let's take a look at the **Pandas Dataframe Parser**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "executionInfo": {
     "elapsed": 656,
     "status": "ok",
     "timestamp": 1713516190136,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "Kme-2Acllzar"
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "from typing import Any, Dict\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "executionInfo": {
     "elapsed": 288,
     "status": "ok",
     "timestamp": 1713516191328,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "E54nWnY2mFuF"
   },
   "outputs": [],
   "source": [
    "# Solely for documentation purposes.\n",
    "def format_parser_output(parser_output: Dict[str, Any]) -> None:\n",
    "    for key in parser_output.keys():\n",
    "        parser_output[key] = parser_output[key].to_dict()\n",
    "    return pprint.PrettyPrinter(width=4, compact=True).pprint(parser_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fZnAeaY8mnXf"
   },
   "source": [
    "Define your desired Pandas DataFrame and set up a parser + inject instructions into the prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1713516193677,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "EzjjJb65mNe8"
   },
   "outputs": [],
   "source": [
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"num_legs\": [2, 4, 8, 0],\n",
    "        \"num_wings\": [2, 0, 0, 0],\n",
    "        \"num_specimen_seen\": [10, 2, 1, 8],\n",
    "    }\n",
    ")\n",
    "\n",
    "parser = PandasDataFrameOutputParser(dataframe=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X5i7udQWmwEh"
   },
   "source": [
    "Here's an example of a column operation being performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 539,
     "status": "ok",
     "timestamp": 1713516450242,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "kLj8jwNZmNRa",
    "outputId": "5e4b4615-2069-4dd4-dc3a-6afa84924bbc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_wings': {0: 2,\n",
      "               1: 0,\n",
      "               2: 0,\n",
      "               3: 0}}\n"
     ]
    }
   ],
   "source": [
    "df_query = \"Retrieve the num_wings column.\"\n",
    "\n",
    "prompt = PromptTemplate(template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "                        input_variables=[\"query\"],\n",
    "                        partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "                        )\n",
    "\n",
    "chain = prompt | model4 | parser\n",
    "parser_output = chain.invoke({\"query\": df_query})\n",
    "format_parser_output(parser_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZXcQRVcbm3Fi"
   },
   "source": [
    "Here's an example of a row operation being performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 725,
     "status": "ok",
     "timestamp": 1713516458832,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "EyLK0qB6mUig",
    "outputId": "9b4b8247-e030-4d58-a9bd-1a0f97df381a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': {'num_legs': 4,\n",
      "       'num_specimen_seen': 2,\n",
      "       'num_wings': 0}}\n"
     ]
    }
   ],
   "source": [
    "df_query = \"Retrieve the first row.\"\n",
    "\n",
    "prompt = PromptTemplate(template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "                        input_variables=[\"query\"],\n",
    "                        partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "                        )\n",
    "chain = prompt | model4 | parser\n",
    "parser_output = chain.invoke({\"query\": df_query})\n",
    "format_parser_output(parser_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o2cWky88aEpC"
   },
   "source": [
    "#### Below we go over a more powerful output parser, the **PydanticOutputParser**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lb5BA6iGaaBd"
   },
   "source": [
    "#### a) Define your desired data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserInfo(BaseModel):\n",
    "    name: str\n",
    "    age: int\n",
    "    email: EmailStr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Create a parser instance for the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = PydanticOutputParser(pydantic_object=UserInfo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Create a prompt that formats the output in the required structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "Extract the following information from the text:\n",
    "\n",
    "name: The full name of the user.\n",
    "age: The user's age (as an integer).\n",
    "email: The user's email address.\n",
    "\n",
    "Return the data in JSON format. \n",
    "\n",
    "Here's the text:\n",
    "{text}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d) Set up the Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=prompt_template\n",
    ")\n",
    "\n",
    "# Example text input\n",
    "text_input = \"John Doe, a 30-year-old, can be reached at john.doe@example.com.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e) And a query intended to prompt a language model to populate the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"name\": \"John Doe\",\n",
      "  \"age\": 30,\n",
      "  \"email\": \"john.doe@example.com\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "prompt_and_model = prompt | model3\n",
    "output = prompt_and_model.invoke({\"text\": text_input})\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UserInfo(name='John Doe', age=30, email='john.doe@example.com')"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = parser.invoke(output)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"name\":\"John Doe\",\"age\":30,\"email\":\"john.doe@example.com\"}'"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A more complex example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Define your desired data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "executionInfo": {
     "elapsed": 263,
     "status": "ok",
     "timestamp": 1713516574944,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "suvoyAIIYnbk"
   },
   "outputs": [],
   "source": [
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "    # You can add custom validation logic easily with Pydantic.\n",
    "    @model_validator(mode=\"before\")\n",
    "    @classmethod\n",
    "    def question_ends_with_question_mark(cls, values: dict) -> dict:\n",
    "        setup = values.get(\"setup\")\n",
    "        if setup and setup[-1] != \"?\":\n",
    "            raise ValueError(\"Badly formed question!\")\n",
    "        return values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pFiNe5uxai_b"
   },
   "source": [
    "#### b) Set up a parser and the prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "executionInfo": {
     "elapsed": 243,
     "status": "ok",
     "timestamp": 1713516578193,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "tLwcQWypagFB"
   },
   "outputs": [],
   "source": [
    "parser = PydanticOutputParser(pydantic_object=Joke)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rGshQwP-a2Xt"
   },
   "source": [
    "#### c) And a query intended to prompt a language model to populate the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1232,
     "status": "ok",
     "timestamp": 1713516602581,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "jAGBOHFwa3Sa",
    "outputId": "187bb9cf-2995-4359-d9b0-c69405767b0e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Joke(setup=\"Why don't scientists trust atoms?\", punchline='Because they make up everything!')"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_and_model = prompt | model3\n",
    "output = prompt_and_model.invoke({\"query\": \"Tell me a joke.\"})\n",
    "\n",
    "result = parser.invoke(output)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"setup\":\"Why don\\'t scientists trust atoms?\",\"punchline\":\"Because they make up everything!\"}'"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 - [Memory](https://python.langchain.com/v0.1/docs/modules/memory/)  \n",
    "\n",
    "Most LLM applications have a conversational interface. An essential component of a conversation is being able to refer to information introduced earlier in the conversation. At bare minimum, a conversational system should be able to access some window of past messages directly. A more complex system will need to have a world model that it is constantly updating, which allows it to do things like maintain information about entities and their relationships.\n",
    "\n",
    "### Outline\n",
    "* ConversationBufferMemory\n",
    "* ConversationBufferWindowMemory\n",
    "* ConversationTokenBufferMemory\n",
    "* ConversationSummaryMemory\n",
    "\n",
    "![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQcYZAK7bD8PPvjbAIe5tLk19SX9zKaSGHlVrE_vKrDk09tCgj7ujp_re7SpYHI8I7yUA&usqp=CAU) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 - ConversationBufferMemory  \n",
    "\n",
    "We are going to use the simplest memory type.  \n",
    "Let's check what is happening behind the scenes using the ```verbose``` mode of the LLM:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, my name is Renato\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello, Renato! It's great to meet you! How's your day going so far?\""
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(llm=model3, memory=memory, verbose=True)\n",
    "conversation.predict(input=\"Hi, my name is Renato\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Renato\n",
      "AI: Hello, Renato! It's great to meet you! How's your day going so far?\n",
      "Human: What is 1+1?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"1 + 1 equals 2! It's a simple addition problem, but it's the foundation of so many mathematical concepts. Do you enjoy math, or is there another subject you prefer?\""
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Renato\n",
      "AI: Hello, Renato! It's great to meet you! How's your day going so far?\n",
      "Human: What is 1+1?\n",
      "AI: 1 + 1 equals 2! It's a simple addition problem, but it's the foundation of so many mathematical concepts. Do you enjoy math, or is there another subject you prefer?\n",
      "Human: What is my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Your name is Renato! It's a nice name. Do you have any favorite hobbies or interests you'd like to share?\""
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As we are using a memory module, we can check what is stored in there:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi, my name is Renato\n",
      "AI: Hello, Renato! It's great to meet you! How's your day going so far?\n",
      "Human: What is 1+1?\n",
      "AI: 1 + 1 equals 2! It's a simple addition problem, but it's the foundation of so many mathematical concepts. Do you enjoy math, or is there another subject you prefer?\n",
      "Human: What is my name?\n",
      "AI: Your name is Renato! It's a nice name. Do you have any favorite hobbies or interests you'd like to share?\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi, my name is Renato\\nAI: Hello, Renato! It's great to meet you! How's your day going so far?\\nHuman: What is 1+1?\\nAI: 1 + 1 equals 2! It's a simple addition problem, but it's the foundation of so many mathematical concepts. Do you enjoy math, or is there another subject you prefer?\\nHuman: What is my name?\\nAI: Your name is Renato! It's a nice name. Do you have any favorite hobbies or interests you'd like to share?\"}"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's erase the memory buffer and insert something else in it:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi\n",
      "AI: What's up\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationBufferMemory()\n",
    "\n",
    "memory.save_context({\"input\": \"Hi\"}, \n",
    "                    {\"output\": \"What's up\"})\n",
    "\n",
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi\\nAI: What's up\"}"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's add even more context:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi\\nAI: What's up\\nHuman: Not much, just hanging\\nAI: Cool\"}"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.save_context({\"input\": \"Not much, just hanging\"}, \n",
    "                    {\"output\": \"Cool\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now, let's check if the model still know the information from a previous interaction:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f5cd994-d4f1-49c3-acd2-f39e471eaa82",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi\n",
      "AI: What's up\n",
      "Human: Not much, just hanging\n",
      "AI: Cool\n",
      "Human: What is my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I don't know your name yet! But I'd love to learn it if you'd like to share. What should I call you?\""
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation = ConversationChain(llm=model3,\n",
    "                                 memory=memory,\n",
    "                                 verbose=True\n",
    "                                 )\n",
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "534bc8b5-433a-4476-a3ee-a66cda0ed923",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## ConversationBufferWindowMemory  \n",
    "[See changes](https://python.langchain.com/docs/versions/migrating_memory/)\n",
    "\n",
    "We can define a buffer size to determine how much context will be kept:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf576a1d-5b6d-46bb-b6f9-05d6c7ef8853",
     "showTitle": false,
     "title": ""
    },
    "height": 30
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Renato Rocha Souza\\AppData\\Local\\Temp\\ipykernel_11776\\1737933177.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferWindowMemory(k=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Not much, just hanging\\nAI: Cool'}"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "\n",
    "memory.save_context({\"input\": \"Hi\"},\n",
    "                    {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e078bbf4-b9e5-42b6-b970-7e8c60b0e3a2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### We see that only one interaction was saved. Now let's see how it would be with the LLM:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6f4f0c4-fbb3-4983-9f3e-ab03ae5b039f",
     "showTitle": false,
     "title": ""
    },
    "height": 132
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Renato! It's great to meet you! I'm here to chat about anything you'd like. How's your day going so far?\""
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "conversation = ConversationChain(llm=model, memory=memory, verbose=False)\n",
    "conversation.predict(input=\"Hi, my name is Renato\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "784f5ca5-c343-4f58-9c21-413095ba833e",
     "showTitle": false,
     "title": ""
    },
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"1 + 1 equals 2! It's one of the simplest math problems out there. Do you enjoy math, or is there another subject that interests you more?\""
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb6b6b71-5700-4d90-ba48-dd78b9fa8a70",
     "showTitle": false,
     "title": ""
    },
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't have access to your name unless you tell me! But I‚Äôd love to know what it is. What's your name?\""
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fdf9c65-c96c-4139-8e52-b46855ded00f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## ConversationTokenBufferMemory\n",
    "\n",
    "Similarly, we can define a buffer size in the quantity of tokens:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a4a2e17-608f-4d7a-8761-568c928e210b",
     "showTitle": false,
     "title": ""
    },
    "height": 132
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Renato Rocha Souza\\AppData\\Local\\Temp\\ipykernel_11776\\3519355949.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationTokenBufferMemory(llm=model, max_token_limit=30)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'history': 'AI: Something Beautiful!\\nHuman: Chatbots are what?\\nAI: Charming!'}"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = ConversationTokenBufferMemory(llm=model, max_token_limit=30)\n",
    "\n",
    "memory.save_context({\"input\": \"AI is what?!\"},\n",
    "                    {\"output\": \"Amazing!\"})\n",
    "\n",
    "memory.save_context({\"input\": \"Backpropagation is what?\"},\n",
    "                    {\"output\": \"Something Beautiful!\"})\n",
    "\n",
    "memory.save_context({\"input\": \"Chatbots are what?\"}, \n",
    "                    {\"output\": \"Charming!\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "143f5c8b-28e3-40a1-a78a-241ff1eef074",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## ConversationSummaryMemory  \n",
    "\n",
    "This last type of memory creates an automatic summary of the previous interactions to store:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "704981c9-5f30-4bd9-8f26-36d00331e87e",
     "showTitle": false,
     "title": ""
    },
    "height": 285
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Renato Rocha Souza\\AppData\\Local\\Temp\\ipykernel_11776\\1480727767.py:10: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryBufferMemory(llm=model, max_token_limit=100)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'history': \"System: The human greets the AI, and they exchange casual pleasantries. The human then inquires about the day's schedule.\\nAI: There is a meeting at 8am with your product team. You will need your powerpoint presentation prepared. 9am-12pm have time to work on your LangChain project which will go quickly because Langchain is such a powerful tool. At Noon, lunch at the italian resturant with a customer who is driving from over an hour away to meet you to understand the latest in AI. Be sure to bring your laptop to show the latest LLM demo.\"}"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a long string\n",
    "schedule = \"There is a meeting at 8am with your product team. \\\n",
    "You will need your powerpoint presentation prepared. \\\n",
    "9am-12pm have time to work on your LangChain \\\n",
    "project which will go quickly because Langchain is such a powerful tool. \\\n",
    "At Noon, lunch at the italian resturant with a customer who is driving \\\n",
    "from over an hour away to meet you to understand the latest in AI. \\\n",
    "Be sure to bring your laptop to show the latest LLM demo.\"\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(llm=model, max_token_limit=100)\n",
    "\n",
    "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})\n",
    "memory.save_context({\"input\": \"What is on the schedule today?\"}, \n",
    "                    {\"output\": f\"{schedule}\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7f74747-7948-4e96-9f47-2ceba8d34d87",
     "showTitle": false,
     "title": ""
    },
    "height": 98
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: The human greets the AI, and they exchange casual pleasantries. The human then inquires about the day's schedule.\n",
      "AI: There is a meeting at 8am with your product team. You will need your powerpoint presentation prepared. 9am-12pm have time to work on your LangChain project which will go quickly because Langchain is such a powerful tool. At Noon, lunch at the italian resturant with a customer who is driving from over an hour away to meet you to understand the latest in AI. Be sure to bring your laptop to show the latest LLM demo.\n",
      "Human: What would be a good demo to show?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'A great demo to showcase would be the capabilities of a conversational AI model, like a chatbot that can answer questions, assist with tasks, or engage in casual conversation. You could create a scenario where the chatbot helps a user book a restaurant reservation or provides recommendations based on their preferences. Another impressive option could be demonstrating how the model generates text based on a prompt, such as writing a short story or creating a marketing email. If you have access to a tool like LangChain, you could also demonstrate how it integrates different data sources or APIs to provide a more dynamic and interactive experience. This would really highlight the versatility and real-world applications of LLMs!'"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation = ConversationChain(llm=model, memory=memory, verbose=True)\n",
    "conversation.predict(input=\"What would be a good demo to show?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50141950-8a4e-453a-80c7-c07a9e369112",
     "showTitle": false,
     "title": ""
    },
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"System: The human greets the AI, and they exchange casual pleasantries. The human then inquires about the day's schedule, to which the AI responds with details about meetings and tasks, including a product team meeting at 8 am, time to work on a LangChain project, and a lunch appointment with a customer at noon. The human asks for a good demo to show, and the AI suggests showcasing a conversational AI model, like a chatbot that can assist with tasks or engage in conversations, as well as demonstrating text generation capabilities. The AI emphasizes using LangChain to integrate different data sources or APIs to highlight the versatility and real-world applications of LLMs.\"}"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNRBbpHqLlsR3qsDTCKjs0x",
   "provenance": [
    {
     "file_id": "1LtCkSgnYYJO5Q4IxLEbUqn4Chmc-vJPw",
     "timestamp": 1713469816474
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
