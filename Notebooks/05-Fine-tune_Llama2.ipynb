{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OSHlAbqzDFDq"
   },
   "source": [
    "# Fine-tune Llama 2 in Google Colab\n",
    "[source](https://medium.com/@csakash03/fine-tuning-llama-2-llm-on-google-colab-a-step-by-step-guide-cf7bb367e790)  \n",
    "\n",
    "## Google Colab limitations:\n",
    "\n",
    "Thereâ€™s a prevalent misconception regarding fine-tuning Language Models (LLMs) on the free version of Google Colab, and itâ€™s essential to clarify this misunderstanding.\n",
    "\n",
    "Hereâ€™s the reality check: While itâ€™s feasible to fine-tune an LLM on Google Colabâ€™s free environment, thereâ€™s a significant caveat to consider. The platform provides a free-tier service with time limitations. Users have a 12-hour window to execute their code continuously. However, thereâ€™s a catch â€” if thereâ€™s no ongoing activity, the session disconnects after a mere 15â€“30 minutes of inactivity. Additionally, thereâ€™s a constraint on GPU usage, limited to approximately 12 hours per day.\n",
    "\n",
    "Fine-tuning a large LLM on Google Colabâ€™s free version comes with its challenges! Due to these constraints, users may find themselves restricted to fine-tuning smaller LLMs with limited datasets, often constrained to approximately 2 epochs and around 10k samples. Consequently, while it remains possible to fine-tune an LLM using Google Colabâ€™s free tier, the process can be quite demanding, especially for substantial models.  \n",
    "This notebook runs on a T4 GPU.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LjTt_TdBT1ko"
   },
   "source": [
    "## [Llama 2 Family of Models](https://huggingface.co/models?other=llama-2)   \n",
    "Llama 2, developed by Meta, is a family of large language models ranging from 7 billion to 70 billion parameters. It is built on the Google transformer architecture and has been fine-tuned for dialogue use cases, outperforming open-source chat models on various benchmarks. Llama 2 is known for its few-shot learning capability, efficiency, and multitask learning. However, training Llama 2 from scratch can be computationally intensive and time-consuming. Despite its advantages, Llama 2 models face challenges in stop generation, which affects their ability to determine the appropriate â€˜stopâ€™ point when generating text. Additionally, data contamination has been identified as a critical issue in Llama 2 evaluation, impacting the integrity of model assessments\n",
    "\n",
    "The journey to greatness for Llama 2 commenced with rigorous training involving an extensive dataset encompassing text and code from diverse sources like books, articles, and code repositories. What distinguishes Llama 2 is its exceptional refinement process, where it gleaned insights from over 1 million human annotations. This phase was pivotal, enhancing its precision and fluency remarkably. Remarkably, Llama 2â€™s prowess extends beyond controlled environments, surpassing other open-source language models across various practical assessments. Its versatility enables utilization in both research and commercial spheres, making it an adaptable tool poised for limitless possibilities. Brace yourselves, as Llama 2 is on a mission to redefine the AI landscape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VoF4zHqxUBDk"
   },
   "source": [
    "## LLMâ€™s Training process.\n",
    "\n",
    "The training process for LLM primarily involves two key steps:\n",
    "\n",
    "    + Pre-training: This initial phase is akin to introducing a language model to the fundamental elements of language. It involves exposing the model to an extensive array of text derived from the vast expanse of the internet. This stage serves to provide the model with a broad comprehension of grammar, vocabulary, and prevalent language patterns. Throughout this phase, the model learns to anticipate and predict subsequent words or phrases in a sentence, thereby developing an understanding of language structure. In essence, itâ€™s comparable to teaching a student the basics, similar to mastering the ABCs, before delving into more complex reading material like books.\n",
    "\n",
    "    + Fine-tuning constitutes the pivotal phase where the model, having acquired a foundational understanding of language through pre-training, undergoes a more specialized process. This step resembles providing tailored lessons to a well-rounded student for a specific task. For instance, fine-tuning might involve honing the modelâ€™s proficiency in answering questions or generating code, akin to guiding a student to excel in a particular subject at school. Fine-tuning essentially adapts the broad language knowledge acquired during pre-training to execute specific tasks with precision and effectiveness.\n",
    "    Despite the fine-tuning process, the model encounters persistent challenges. These encompass occasional inaccuracies or nonsensical output, sensitivity to input phrasing, susceptibility to biases within the fine-tuning data, and difficulty in comprehending nuanced contexts within intricate conversations. Moreover, models may struggle when generating coherent lengthy content, impacting their suitability for applications such as content generation and chatbots. These limitations emphasize the ongoing necessity for continuous research and development efforts to refine fine-tuned models and address these issues, ensuring more reliable and ethically sound applications of AI.\n",
    "\n",
    "Reinforcement Learning from Human Feedback (RLHF) serves as a tutor for language models, akin to providing additional guidance after pre-training and fine-tuning. It resembles a teacher reviewing and grading a modelâ€™s responses, aiming to further enhance its capabilities. Human feedback, delivered through evaluations and corrections, serves as a means for the model to learn from errors and refine its language skills. Similar to how students improve through feedback in their studies, RLHF assists language models in excelling at specific tasks by incorporating guidance from humans.\n",
    "\n",
    "Addressing the challenges faced by RLHF, a new technique named Direct Preference Optimization (DPO) is stepping into the game. DPO aims to overcome the limitations of RLHF in fine-tuning large language models (LLMs). Unlike RLHF, which relies on complex learning of reward functions, DPO simplifies the process by treating it as a classification problem using human preference data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F2f0nVsWU3v3"
   },
   "source": [
    "## Fine-Tuning Llama 2 step-by-Step\n",
    "\n",
    "Weâ€™re opting to utilize ğŸ¦™Llama-2â€“7B-HF, a pre-trained smaller model within the Llama-2 lineup, for fine-tuning using the Qlora technique.\n",
    "\n",
    "QLoRA (Quantized Low-Rank Adaptation) serves as an extension of LoRA (Low-Rank Adapters), integrating quantization to enhance parameter efficiency during the fine-tuning process. Notably, QLoRA proves more memory-efficient compared to LoRA by loading the pre-trained model onto GPU memory as 4-bit weights, whereas LoRA uses 8-bit weights. This optimization reduces memory requirements and accelerates computations.\n",
    "\n",
    "In simpler terms, instead of training the entire model from scratch, weâ€™ll introduce an adapter between the model components and focus solely on training that adapter. This approach allows us to fine-tune the Language Model (LLM) specifically on the consumer GPU, thereby expediting the training process significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9mpX76KIVdcL"
   },
   "source": [
    "Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 144638,
     "status": "ok",
     "timestamp": 1712879970614,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "GLXwJqbjtPho",
    "outputId": "98b0a7a9-7d2b-465c-909c-400e38acd4e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m297.4/297.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m263.7/263.7 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m102.0/102.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m267.1/267.1 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m91.9/91.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m144.8/144.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install -qU accelerate peft bitsandbytes transformers trl wandb gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zU-YMBgVViHv"
   },
   "source": [
    "Import required libraries\n",
    "\n",
    "    + transformers: This library offers APIs to facilitate the download and use of pre-trained models.\n",
    "    + bitsandbytes: Designed specifically for quantization purposes, this library focuses on reducing the memory footprint of large language models, particularly on GPUs.\n",
    "    + peft: Utilized for integrating LoRA adapters into Language Models (LLMs).\n",
    "    + trl: This library houses an SFT (Supervised Fine-Tuning) class that aids in fine-tuning models.\n",
    "    + accelerate and xformers: These libraries are employed to enhance the inference speed of the model, thereby optimizing its performance.\n",
    "    + wandb: This tool serves as a monitoring platform, used to track and observe the training process.\n",
    "    + datasets: Utilized in conjunction with Hugging Face, this library facilitates the loading of datasets.\n",
    "    + gradio: This library is employed for the creation of straightforward user interfaces, simplifying the design process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27458,
     "status": "ok",
     "timestamp": 1712879998065,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "nAMzy_0FtaUZ",
    "outputId": "2132ff40-11f5-4ea7-dbbb-42605cbfeb10"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:bitsandbytes.cextension:The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import wandb\n",
    "import platform\n",
    "import gradio\n",
    "import warnings\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, HfArgumentParser, TrainingArguments, pipeline, logging, TextStreamer\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dzIPYNmNWBLe"
   },
   "source": [
    "Check system spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1712879998066,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "ZRIvVnjTVc1q",
    "outputId": "9cdaec1a-659a-4df4-8dae-e8188b366685"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: False\n",
      "Number of CUDA devices: 0\n",
      "--- CPU Information ---\n",
      "Processor: x86_64\n",
      "System: Linux 6.1.58+\n",
      "Python Version: 3.10.12\n"
     ]
    }
   ],
   "source": [
    "def print_system_specs():\n",
    "    # Check if CUDA is available\n",
    "    is_cuda_available = torch.cuda.is_available()\n",
    "    print(\"CUDA Available:\", is_cuda_available)\n",
    "    # Get the number of available CUDA devices\n",
    "    num_cuda_devices = torch.cuda.device_count()\n",
    "    print(\"Number of CUDA devices:\", num_cuda_devices)\n",
    "    if is_cuda_available:\n",
    "        for i in range(num_cuda_devices):\n",
    "            # Get CUDA device properties\n",
    "            device = torch.device('cuda', i)\n",
    "            print(f\"--- CUDA Device {i} ---\")\n",
    "            print(\"Name:\", torch.cuda.get_device_name(i))\n",
    "            print(\"Compute Capability:\", torch.cuda.get_device_capability(i))\n",
    "            print(\"Total Memory:\", torch.cuda.get_device_properties(i).total_memory, \"bytes\")\n",
    "    # Get CPU information\n",
    "    print(\"--- CPU Information ---\")\n",
    "    print(\"Processor:\", platform.processor())\n",
    "    print(\"System:\", platform.system(), platform.release())\n",
    "    print(\"Python Version:\", platform.python_version())\n",
    "\n",
    "print_system_specs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uy8oHbn_bOoK"
   },
   "source": [
    "Setting the model variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 240,
     "status": "ok",
     "timestamp": 1712880025014,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "SQBI73-6bQb2"
   },
   "outputs": [],
   "source": [
    "# The model that you want to train from the Hugging Face hub\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "# The instruction dataset to use\n",
    "dataset_name = \"vicgalle/alpaca-gpt4\"\n",
    "\n",
    "# Hugging face repository link to save fine-tuned model(Create new repository in huggingface,copy and paste here)\n",
    "new_model = \"Repository link here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-p9WRxkbkGL"
   },
   "source": [
    "### Log into hugging face hub  \n",
    "Note : You need to enter the access token, before that you need to apply for access the llama-2 model in [Meta website](https://llama.meta.com/llama-downloads)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qJ9huLS-b3xz"
   },
   "outputs": [],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8uTWBTomcCYw"
   },
   "source": [
    "### Load dataset\n",
    "\n",
    "We are utilizing the pre-processed dataset vicgalle/alpaca-gpt4 from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 2583,
     "status": "ok",
     "timestamp": 1712880059109,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "tlXnLm1ecLRe"
   },
   "outputs": [],
   "source": [
    "# Load dataset (you can process it here)\n",
    "dataset = load_dataset(dataset_name, split=\"train[0:10000]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 294,
     "status": "ok",
     "timestamp": 1712880069275,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "IC4L6LISm5Xh",
    "outputId": "d5443a5a-06a3-4d6c-e4f2-9317e858cb24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Give three tips for staying healthy.\n",
      "\n",
      "### Response:\n",
      "1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\n",
      "\n",
      "2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\n",
      "\n",
      "3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"text\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 329,
     "status": "ok",
     "timestamp": 1712880098695,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "rhikE7Ynm-Gb",
    "outputId": "3f0a0454-e677-4091-88cc-11577bd87b20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Describe the structure of an atom.\n",
      "\n",
      "### Response:\n",
      "An atom is the basic building block of all matter and is made up of three types of particles: protons, neutrons, and electrons. The structure of an atom can be described as a nucleus at the center surrounded by a cloud of electrons.\n",
      "\n",
      "The nucleus of an atom is made up of protons and neutrons. Protons are positively charged particles and neutrons are neutral particles with no charge. Both of these particles are located in the nucleus of the atom, which is at the center of the atom and contains most of the atom's mass.\n",
      "\n",
      "Surrounding the nucleus of the atom is a cloud of electrons. Electrons are negatively charged particles that are in constant motion around the nucleus. The electron cloud is divided into shells or orbitals, and each shell can hold a certain number of electrons. The number of electrons in the outermost shell, called the valence shell, determines the chemical properties of the atom. \n",
      "\n",
      "In a neutral atom, the number of protons in the nucleus is equal to the number of electrons in the electron cloud, so the positive and negative charges balance out and the atom has no overall charge. The number of protons, also called the atomic number, determines what element the atom is.\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"text\"][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t75gkMqEcSqL"
   },
   "source": [
    "### Loading the model and tokenizer\n",
    "\n",
    "We are going to load a Llama-2â€“7B-HF pre-trained model with 4-bit quantization, and the computed data type will be BFloat16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ve7Tl2FLccMo"
   },
   "outputs": [],
   "source": [
    "# Load base model(llama-2-7b-hf) and tokenizer\n",
    "bnb_config = BitsAndBytesConfig(load_in_4bit= True,\n",
    "                                bnb_4bit_quant_type= \"nf4\",\n",
    "                                bnb_4bit_compute_dtype= torch.float16,\n",
    "                                bnb_4bit_use_double_quant= False,\n",
    "                                )\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             quantization_config=bnb_config,\n",
    "                                             device_map={\"\": 0}\n",
    "                                             )\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model.config.use_cache = False # silence the warnings. Please re-enable for inference!\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Load LLaMA tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.add_eos_token = True\n",
    "tokenizer.add_bos_token, tokenizer.add_eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qLsX5YfIi_-W"
   },
   "source": [
    "### Monitoring\n",
    "\n",
    "Apart from training, monitoring is a crucial part we need to consider in LLM training\n",
    "\n",
    "To get started, create a [WandB account](https://wandb.ai/site). After creating your account, enter the authorization token here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u9bwKc9CccIo"
   },
   "outputs": [],
   "source": [
    "#monitoring login\n",
    "wandb.login(key=\"Enter the Authorization code here\")\n",
    "run = wandb.init(project='Fine tuning llama-2-7B', job_type=\"training\", anonymous=\"allow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BYlIO1M5jraD"
   },
   "source": [
    "### Lora config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K5Dy8kZjccEs"
   },
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(lora_alpha= 8,\n",
    "                         lora_dropout= 0.1,\n",
    "                         r= 16,\n",
    "                         bias=\"none\",\n",
    "                         task_type=\"CAUSAL_LM\",\n",
    "                         target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\", \"up_proj\"]\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6KuNHr3Sj3zO"
   },
   "source": [
    "### Training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N4oKFJoUj57g"
   },
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(output_dir= \"./results\",\n",
    "                                       num_train_epochs= 1,\n",
    "                                       per_device_train_batch_size= 8,\n",
    "                                       gradient_accumulation_steps= 2,\n",
    "                                       optim = \"paged_adamw_8bit\",\n",
    "                                       save_steps= 1000,\n",
    "                                       logging_steps= 30,\n",
    "                                       learning_rate= 2e-4,\n",
    "                                       weight_decay= 0.001,\n",
    "                                       fp16= False,\n",
    "                                       bf16= False,\n",
    "                                       max_grad_norm= 0.3,\n",
    "                                       max_steps= -1,\n",
    "                                       warmup_ratio= 0.3,\n",
    "                                       group_by_length= True,\n",
    "                                       lr_scheduler_type= \"linear\",\n",
    "                                       report_to=\"wandb\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Zjc5yDQkNRU"
   },
   "source": [
    "### SFTT Trainer arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X58nFTX8j54G"
   },
   "outputs": [],
   "source": [
    "# Setting sft parameters\n",
    "trainer = SFTTrainer(model=model,\n",
    "                     train_dataset=dataset,\n",
    "                     peft_config=peft_config,\n",
    "                     max_seq_length= None,\n",
    "                     dataset_text_field=\"text\",\n",
    "                     tokenizer=tokenizer,\n",
    "                     args=training_arguments,\n",
    "                     packing= False,\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zTko7yFVkVRV"
   },
   "source": [
    "#### Weâ€™re all set to begin the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8xrkL-n3j51T"
   },
   "outputs": [],
   "source": [
    "# Train model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jPOVwkSkkmqO"
   },
   "source": [
    "Once you initiate the code, youâ€™ll gain access to monitor a range of training metrics such as loss, GPU usage, RAM utilization, and additional data directly on the WandB website. The link for accessing this monitoring interface will be provided to you as part of the code execution process.\n",
    "\n",
    "During this critical phase, itâ€™s essential to keep a vigilant eye on the training loss. Any irregularities or anomalies in the loss pattern serve as a signal to contemplate halting the training process. Overfitting often emerges as a common concern in such instances, necessitating potential adjustments to hyperparameters and the possibility of retrying to attain optimal results . This close monitoring ensures timely intervention and necessary adjustments to enhance the training process.\n",
    "\n",
    "### Good training loss\n",
    "\n",
    "â€œGood training lossâ€ typically refers to a situation where the loss metric during the training of a machine learning model decreases steadily or reaches a low value, indicating that the model is effectively learning from the data. This decline in loss signifies that the modelâ€™s predictions are becoming more accurate or aligned with the actual target values in the training dataset.\n",
    "\n",
    "However, what constitutes a â€œgoodâ€ training loss can vary based on the specific problem, dataset, and model architecture. In some cases, achieving a lower training loss might indicate the model has memorized the training data (overfitting) and might not generalize well to new, unseen data.\n",
    "\n",
    "Hence, while a decreasing training loss is generally desired, itâ€™s crucial to consider other factors such as validation loss, model performance on unseen data, and potential signs of overfitting to assess the true effectiveness of the trained model.\n",
    "\n",
    "### Bad Training loss\n",
    "\n",
    "A â€œbadâ€ training loss typically refers to a situation where the loss metric during the training of a machine learning model exhibits undesirable behavior. This might include:\n",
    "\n",
    "    + High or increasing loss: The loss value remains consistently high or starts increasing during training, indicating that the model is not effectively learning from the data.\n",
    "    + Fluctuating loss: The loss metric fluctuates significantly without showing a clear decreasing trend, indicating instability in the modelâ€™s learning process.\n",
    "    + Convergence to a high value: The loss may converge to a relatively high value and stay stagnant, suggesting that the model may not be capable of adequately capturing patterns in the data.\n",
    "    + Overfitting: In some cases, a decreasing training loss might not necessarily be beneficial if it is accompanied by a significant increase in validation loss, indicating that the model is memorizing the training data rather than learning useful patterns.\n",
    "\n",
    "Identifying a â€œbadâ€ training loss is essential as it can signify issues with the model architecture, hyperparameters, or the quality of the training data. Addressing such issues is crucial to improve the modelâ€™s performance and ensure better generalization to unseen data.\n",
    "\n",
    "### What after training ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0hqOqV24kxjr"
   },
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "trainer.model.save_pretrained(new_model)\n",
    "wandb.finish()\n",
    "model.config.use_cache = True\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hXNhDEASk25q"
   },
   "source": [
    "#### Letâ€™s test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KvkfCzumk4hX"
   },
   "outputs": [],
   "source": [
    "def stream(user_prompt):\n",
    "    runtimeFlag = \"cuda:0\"\n",
    "    system_prompt = 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n'\n",
    "    B_INST, E_INST = \"### Instruction:\\n\", \"### Response:\\n\"\n",
    "    prompt = f\"{system_prompt}{B_INST}{user_prompt.strip()}\\n\\n{E_INST}\"\n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(runtimeFlag)\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "    # Despite returning the usual output, the streamer will also print the generated text to stdout.\n",
    "    _ = model.generate(**inputs, streamer=streamer, max_new_tokens=500)\n",
    "\n",
    "\n",
    "stream(\"what is newtons 3rd law and its formula\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q0Nvr8AYlKlV"
   },
   "source": [
    "### Upload a model to hugging face repository\n",
    "\n",
    "#### Step 1: After completing the training of your model, employing the provided code to release this memory becomes crucial. This action is significant as it aids in preventing your computer from facing memory shortages and can also enhance the performance of other concurrently running programs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_tXYM0WolQXJ"
   },
   "outputs": [],
   "source": [
    "# Clear the memory footprint\n",
    "del model, trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4EuKe3uDlVl9"
   },
   "source": [
    "#### Step 2: The subsequent step involves merging the adapter with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5uN-hIP-la6j"
   },
   "outputs": [],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, low_cpu_mem_usage=True,\n",
    "    return_dict=True,torch_dtype=torch.float16,\n",
    "    device_map= {\"\": 0})\n",
    "model = PeftModel.from_pretrained(base_model, new_model)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Reload tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xXVj0cRBlfxB"
   },
   "source": [
    "#### Step 3: Finally, once the merger is complete, the next action involves pushing the merged model to the Hugging Face hub. This process facilitates the sharing and accessibility of the model for others in the community."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CRZefCqKlhNR"
   },
   "outputs": [],
   "source": [
    "model.push_to_hub(new_model)\n",
    "tokenizer.push_to_hub(new_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tiIWwkgnlpJE"
   },
   "source": [
    "### Conclusion:\n",
    "\n",
    "Our assessment indicates that the modelâ€™s performance is promising but falls short of being outstanding. Itâ€™s essential to highlight that fine-tuning a model on platforms like Google Colab comes with its set of challenges. The time limitations and resource constraints can make this task a formidable one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8BAB2V66l2lp"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1PEQyJO1-f6j0S_XJ8DV50NkpzasXkrzd",
     "timestamp": 1712874648590
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
