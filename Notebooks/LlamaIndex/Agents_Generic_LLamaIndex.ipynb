{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b0L5n3kwNdj9"
   },
   "source": [
    "# Agents with LlamaIndex II - \n",
    "\n",
    "Sources [1](https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/), [2](https://docs.llamaindex.ai/en/stable/understanding/putting_it_all_together/agents/), [3](https://docs.llamaindex.ai/en/stable/examples/agent/custom_agent/), [4](https://docs.llamaindex.ai/en/stable/examples/agent/openai_agent/), [5](https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/agent_runner/), [6](https://medium.com/llamaindex-blog/data-agents-eed797d7972f), [LlamaHub Tools](https://llamahub.ai/?tab=tools)  \n",
    "    \n",
    "--- \n",
    "### Agents\n",
    "\n",
    "An \"agent\" is an automated reasoning and decision engine. It takes in a user input/query and can make internal decisions for executing that query in order to return the correct result. The key agent components can include, but are not limited to:\n",
    "\n",
    "+ Breaking down a complex question into smaller ones\n",
    "+ Choosing an external Tool to use + coming up with parameters for calling the Tool\n",
    "+ Planning out a set of tasks\n",
    "+ Storing previously completed tasks in a memory module\n",
    "\n",
    "LlamaIndex provides a comprehensive framework for building agents. This includes the following components:\n",
    "\n",
    "+ Using agents with tools at a high-level to build agentic RAG and workflow automation use cases **(Previous Notebook)**\n",
    "+ Low-level components for building and debugging agents\n",
    "+ Core agent ingredients that can be used as standalone modules: query planning, tool use, and more **(Previous Notebook)**\n",
    "\n",
    "### Tools\n",
    "\n",
    "Having proper tool abstractions is at the core of building data agents. Defining a set of Tools is similar to defining any API interface, with the exception that these Tools are meant for agent rather than human use. We allow users to define both a Tool as well as a ToolSpec containing a series of functions under the hood.\n",
    "\n",
    "When using an agent or LLM with function calling, the tool selected (and the arguments written for that tool) rely strongly on the tool name and description of the tools purpose and arguments. Spending time tuning these parameters can result in larges changes in how the LLM calls these tools.\n",
    "\n",
    "A Tool implements a very generic interface - simply define __call__ and also return some basic metadata (name, description, function schema).\n",
    "LlamaIndex offer a few different types of Tools:\n",
    "\n",
    "+ FunctionTool: A function tool allows users to easily convert any user-defined function into a Tool. It can also auto-infer the function schema. **(Previous Notebook)**\n",
    "+ QueryEngineTool: A tool that wraps an existing query engine. Note: since our agent abstractions inherit from BaseQueryEngine, these tools can also wrap other agents. **(Previous Notebook)**\n",
    "+ Community contributed ToolSpecs that define one or more tools around a single service (like Gmail): see [LlamaHub](https://llamahub.ai/)\n",
    "+ Utility tools for wrapping other tools to handle returning large amounts of data from a tool: see [OnDemandLoaderTool](https://docs.llamaindex.ai/en/stable/examples/tools/OnDemandLoaderTool/)\n",
    "\n",
    "### Use Cases\n",
    "\n",
    "The scope of possible use cases for agents is vast and ever-expanding. That said, here are some practical use cases that can deliver immediate value.\n",
    "\n",
    "+ Agentic RAG: Build a context-augmented research assistant over your data that not only answers simple questions, but complex research tasks. (Previous Notebook)\n",
    "+ SQL Agent: A subset of the above is a \"text-to-SQL assistant\" that can interact with a structured database. \n",
    "+ Workflow Assistant: Build an agent that can operate over common workflow tools like email, calendar.\n",
    "+ Coding Assistant: Build an agent that can operate over code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HyWM-ThgNdj_"
   },
   "source": [
    "## Installing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU pip\n",
    "!pip install -qU google-genai\n",
    "!pip install -qU python-dotenv\n",
    "!pip install -qU pypdf\n",
    "!pip install -qU docx2txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: llama-index 0.14.12\n",
      "Uninstalling llama-index-0.14.12:\n",
      "  Successfully uninstalled llama-index-0.14.12\n",
      "Found existing installation: llama-index-cli 0.5.3\n",
      "Uninstalling llama-index-cli-0.5.3:\n",
      "  Successfully uninstalled llama-index-cli-0.5.3\n",
      "Found existing installation: llama-index-core 0.14.12\n",
      "Uninstalling llama-index-core-0.14.12:\n",
      "  Successfully uninstalled llama-index-core-0.14.12\n",
      "Found existing installation: llama-index-embeddings-adapter 0.4.1\n",
      "Uninstalling llama-index-embeddings-adapter-0.4.1:\n",
      "  Successfully uninstalled llama-index-embeddings-adapter-0.4.1\n",
      "Found existing installation: llama-index-embeddings-huggingface 0.6.1\n",
      "Uninstalling llama-index-embeddings-huggingface-0.6.1:\n",
      "  Successfully uninstalled llama-index-embeddings-huggingface-0.6.1\n",
      "Found existing installation: llama-index-embeddings-instructor 0.4.1\n",
      "Uninstalling llama-index-embeddings-instructor-0.4.1:\n",
      "  Successfully uninstalled llama-index-embeddings-instructor-0.4.1\n",
      "Found existing installation: llama-index-embeddings-ollama 0.8.6\n",
      "Uninstalling llama-index-embeddings-ollama-0.8.6:\n",
      "  Successfully uninstalled llama-index-embeddings-ollama-0.8.6\n",
      "Found existing installation: llama-index-embeddings-openai 0.5.1\n",
      "Uninstalling llama-index-embeddings-openai-0.5.1:\n",
      "  Successfully uninstalled llama-index-embeddings-openai-0.5.1\n",
      "Found existing installation: llama-index-experimental 0.6.3\n",
      "Uninstalling llama-index-experimental-0.6.3:\n",
      "  Successfully uninstalled llama-index-experimental-0.6.3\n",
      "Found existing installation: llama-index-finetuning 0.4.1\n",
      "Uninstalling llama-index-finetuning-0.4.1:\n",
      "  Successfully uninstalled llama-index-finetuning-0.4.1\n",
      "Found existing installation: llama-index-indices-managed-llama-cloud 0.9.4\n",
      "Uninstalling llama-index-indices-managed-llama-cloud-0.9.4:\n",
      "  Successfully uninstalled llama-index-indices-managed-llama-cloud-0.9.4\n",
      "Found existing installation: llama-index-instrumentation 0.4.2\n",
      "Uninstalling llama-index-instrumentation-0.4.2:\n",
      "  Successfully uninstalled llama-index-instrumentation-0.4.2\n",
      "Found existing installation: llama-index-llms-azure-openai 0.4.2\n",
      "Uninstalling llama-index-llms-azure-openai-0.4.2:\n",
      "  Successfully uninstalled llama-index-llms-azure-openai-0.4.2\n",
      "Found existing installation: llama-index-llms-mistralai 0.7.1\n",
      "Uninstalling llama-index-llms-mistralai-0.7.1:\n",
      "  Successfully uninstalled llama-index-llms-mistralai-0.7.1\n",
      "Found existing installation: llama-index-llms-ollama 0.9.1\n",
      "Uninstalling llama-index-llms-ollama-0.9.1:\n",
      "  Successfully uninstalled llama-index-llms-ollama-0.9.1\n",
      "Found existing installation: llama-index-multi-modal-llms-openai 0.4.3\n",
      "Uninstalling llama-index-multi-modal-llms-openai-0.4.3:\n",
      "  Successfully uninstalled llama-index-multi-modal-llms-openai-0.4.3\n",
      "Found existing installation: llama-index-postprocessor-cohere-rerank 0.5.1\n",
      "Uninstalling llama-index-postprocessor-cohere-rerank-0.5.1:\n",
      "  Successfully uninstalled llama-index-postprocessor-cohere-rerank-0.5.1\n",
      "Found existing installation: llama-index-postprocessor-colbert-rerank 0.4.1\n",
      "Uninstalling llama-index-postprocessor-colbert-rerank-0.4.1:\n",
      "  Successfully uninstalled llama-index-postprocessor-colbert-rerank-0.4.1\n",
      "Found existing installation: llama-index-program-openai 0.3.2\n",
      "Uninstalling llama-index-program-openai-0.3.2:\n",
      "  Successfully uninstalled llama-index-program-openai-0.3.2\n",
      "Found existing installation: llama-index-readers-file 0.5.6\n",
      "Uninstalling llama-index-readers-file-0.5.6:\n",
      "  Successfully uninstalled llama-index-readers-file-0.5.6\n",
      "Found existing installation: llama-index-readers-llama-parse 0.5.1\n",
      "Uninstalling llama-index-readers-llama-parse-0.5.1:\n",
      "  Successfully uninstalled llama-index-readers-llama-parse-0.5.1\n",
      "Found existing installation: llama-index-readers-web 0.4.5\n",
      "Uninstalling llama-index-readers-web-0.4.5:\n",
      "  Successfully uninstalled llama-index-readers-web-0.4.5\n",
      "Found existing installation: llama-index-tools-google 0.5.0\n",
      "Uninstalling llama-index-tools-google-0.5.0:\n",
      "  Successfully uninstalled llama-index-tools-google-0.5.0\n",
      "Found existing installation: llama-index-workflows 2.12.2\n",
      "Uninstalling llama-index-workflows-2.12.2:\n",
      "  Successfully uninstalled llama-index-workflows-2.12.2\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y $(pip freeze | grep llama-index | cut -d= -f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 76369,
     "status": "ok",
     "timestamp": 1714682800048,
     "user": {
      "displayName": "Renato Rocha Souza",
      "userId": "08757946413431057160"
     },
     "user_tz": -120
    },
    "id": "1hOqc6qhNdkA",
    "outputId": "2e5cf4da-cc0e-40ae-8d26-ed5ca7e07f60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "llama-index-readers-web 0.4.5 requires llama-index-core<0.13,>=0.12.0, but you have llama-index-core 0.14.12 which is incompatible.\n",
      "llama-index-tools-google 0.5.0 requires llama-index-core<0.13,>=0.12.0, but you have llama-index-core 0.14.12 which is incompatible.\n",
      "llama-index-agent-openai 0.4.12 requires llama-index-core<0.13,>=0.12.41, but you have llama-index-core 0.14.12 which is incompatible.\n",
      "llama-index-llms-openai 0.4.7 requires llama-index-core<0.13,>=0.12.41, but you have llama-index-core 0.14.12 which is incompatible.\n",
      "llama-index-tools-code-interpreter 0.3.0 requires llama-index-core<0.13.0,>=0.12.0, but you have llama-index-core 0.14.12 which is incompatible.\n",
      "llama-index-question-gen-openai 0.3.1 requires llama-index-core<0.13,>=0.12.0, but you have llama-index-core 0.14.12 which is incompatible.\n",
      "llama-index-program-openai 0.3.2 requires llama-index-core<0.13,>=0.12.0, but you have llama-index-core 0.14.12 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mRequirement already satisfied: llama-index-core in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (0.14.12)\n",
      "Requirement already satisfied: llama-index-llms-openai in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (0.4.7)\n",
      "Collecting llama-index-llms-openai\n",
      "  Downloading llama_index_llms_openai-0.6.13-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: llama-index-agent-openai in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (0.4.12)\n",
      "Requirement already satisfied: llama-index-tools-code-interpreter in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (0.3.0)\n",
      "Collecting llama-index-tools-code-interpreter\n",
      "  Downloading llama_index_tools_code_interpreter-0.4.1-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: llama-index-workflows in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (2.12.2)\n",
      "Requirement already satisfied: llama-index-question-gen-openai in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (0.3.1)\n",
      "Requirement already satisfied: llama-index-readers-web in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (0.4.5)\n",
      "Collecting llama-index-readers-web\n",
      "  Downloading llama_index_readers_web-0.5.6-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: llama-index-tools-google in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (0.5.0)\n",
      "Collecting llama-index-tools-google\n",
      "  Downloading llama_index_tools_google-0.6.2-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: openai in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (1.109.1)\n",
      "Collecting openai\n",
      "  Downloading openai-2.15.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: pydantic in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (2.12.5)\n",
      "Requirement already satisfied: aiohttp<4,>=3.8.6 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from llama-index-core) (3.12.4)\n",
      "Requirement already satisfied: aiosqlite in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from llama-index-core) (0.21.0)\n",
      "Requirement already satisfied: banks<3,>=2.2.0 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from llama-index-core) (2.2.0)\n",
      "Requirement already satisfied: dataclasses-json in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from llama-index-core) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from llama-index-core) (1.2.18)\n",
      "Requirement already satisfied: dirtyjson<2,>=1.0.8 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from llama-index-core) (1.0.8)\n",
      "Requirement already satisfied: filetype<2,>=1.2.0 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from llama-index-core) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from llama-index-core) (2025.5.1)\n",
      "Requirement already satisfied: httpx in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from llama-index-core) (0.28.1)\n",
      "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from llama-index-core) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from llama-index-core) (3.5)\n",
      "Requirement already satisfied: nltk>3.8.1 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from llama-index-core) (3.9.1)\n",
      "Requirement already satisfied: numpy in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from llama-index-core) (2.2.4)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from llama-index-core) (11.1.0)\n",
      "Requirement already satisfied: platformdirs in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from llama-index-core) (4.3.7)\n",
      "Requirement already satisfied: pyyaml>=6.0.1 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from llama-index-core) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.31.0 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from llama-index-core) (2.32.5)\n",
      "Requirement already satisfied: setuptools>=80.9.0 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from llama-index-core) (80.9.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.49 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core) (2.0.40)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from llama-index-core) (9.1.2)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from llama-index-core) (0.12.0)\n",
      "Requirement already satisfied: tqdm<5,>=4.66.1 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from llama-index-core) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from llama-index-core) (4.15.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from llama-index-core) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from llama-index-core) (1.17.2)\n",
      "Requirement already satisfied: llama-index-instrumentation>=0.1.0 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from llama-index-workflows) (0.4.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from aiohttp<4,>=3.8.6->llama-index-core) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from aiohttp<4,>=3.8.6->llama-index-core) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from aiohttp<4,>=3.8.6->llama-index-core) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from aiohttp<4,>=3.8.6->llama-index-core) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from aiohttp<4,>=3.8.6->llama-index-core) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from aiohttp<4,>=3.8.6->llama-index-core) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from aiohttp<4,>=3.8.6->llama-index-core) (1.20.0)\n",
      "Requirement already satisfied: griffe in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from banks<3,>=2.2.0->llama-index-core) (1.7.3)\n",
      "Requirement already satisfied: jinja2 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from banks<3,>=2.2.0->llama-index-core) (3.1.6)\n",
      "Requirement already satisfied: idna>=2.0 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp<4,>=3.8.6->llama-index-core) (3.10)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: sniffio in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from pydantic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from pydantic) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from pydantic) (0.4.2)\n",
      "Requirement already satisfied: certifi in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from httpx->llama-index-core) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from httpx->llama-index-core) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx->llama-index-core) (0.14.0)\n",
      "INFO: pip is looking at multiple versions of llama-index-agent-openai to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting llama-index-agent-openai\n",
      "  Downloading llama_index_agent_openai-0.4.12-py3-none-any.whl.metadata (439 bytes)\n",
      "  Downloading llama_index_agent_openai-0.4.11-py3-none-any.whl.metadata (439 bytes)\n",
      "  Downloading llama_index_agent_openai-0.4.10-py3-none-any.whl.metadata (439 bytes)\n",
      "  Downloading llama_index_agent_openai-0.4.9-py3-none-any.whl.metadata (438 bytes)\n",
      "INFO: pip is still looking at multiple versions of llama-index-agent-openai to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading llama_index_agent_openai-0.4.8-py3-none-any.whl.metadata (438 bytes)\n",
      "  Downloading llama_index_agent_openai-0.4.7-py3-none-any.whl.metadata (438 bytes)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading llama_index_agent_openai-0.4.6-py3-none-any.whl.metadata (727 bytes)\n",
      "  Downloading llama_index_agent_openai-0.4.5-py3-none-any.whl.metadata (727 bytes)\n",
      "  Downloading llama_index_agent_openai-0.4.4-py3-none-any.whl.metadata (727 bytes)\n",
      "  Downloading llama_index_agent_openai-0.4.3-py3-none-any.whl.metadata (727 bytes)\n",
      "  Downloading llama_index_agent_openai-0.4.2-py3-none-any.whl.metadata (727 bytes)\n",
      "  Downloading llama_index_agent_openai-0.4.1-py3-none-any.whl.metadata (726 bytes)\n",
      "  Downloading llama_index_agent_openai-0.4.0-py3-none-any.whl.metadata (726 bytes)\n",
      "  Downloading llama_index_agent_openai-0.3.4-py3-none-any.whl.metadata (728 bytes)\n",
      "  Downloading llama_index_agent_openai-0.3.3-py3-none-any.whl.metadata (728 bytes)\n",
      "  Downloading llama_index_agent_openai-0.3.2-py3-none-any.whl.metadata (734 bytes)\n",
      "  Downloading llama_index_agent_openai-0.3.1-py3-none-any.whl.metadata (677 bytes)\n",
      "  Downloading llama_index_agent_openai-0.3.0-py3-none-any.whl.metadata (728 bytes)\n",
      "  Downloading llama_index_agent_openai-0.2.9-py3-none-any.whl.metadata (729 bytes)\n",
      "  Downloading llama_index_agent_openai-0.2.8-py3-none-any.whl.metadata (729 bytes)\n",
      "  Downloading llama_index_agent_openai-0.2.7-py3-none-any.whl.metadata (678 bytes)\n",
      "  Downloading llama_index_agent_openai-0.2.6-py3-none-any.whl.metadata (678 bytes)\n",
      "  Downloading llama_index_agent_openai-0.2.5-py3-none-any.whl.metadata (678 bytes)\n",
      "  Downloading llama_index_agent_openai-0.2.4-py3-none-any.whl.metadata (678 bytes)\n",
      "  Downloading llama_index_agent_openai-0.2.3-py3-none-any.whl.metadata (678 bytes)\n",
      "  Downloading llama_index_agent_openai-0.2.2-py3-none-any.whl.metadata (677 bytes)\n",
      "  Downloading llama_index_agent_openai-0.2.1-py3-none-any.whl.metadata (644 bytes)\n",
      "  Downloading llama_index_agent_openai-0.2.0-py3-none-any.whl.metadata (644 bytes)\n",
      "  Downloading llama_index_agent_openai-0.1.7-py3-none-any.whl.metadata (644 bytes)\n",
      "  Downloading llama_index_agent_openai-0.1.6-py3-none-any.whl.metadata (644 bytes)\n",
      "  Downloading llama_index_agent_openai-0.1.5-py3-none-any.whl.metadata (695 bytes)\n",
      "  Downloading llama_index_agent_openai-0.1.4-py3-none-any.whl.metadata (695 bytes)\n",
      "Collecting llama-index-llms-openai\n",
      "  Downloading llama_index_llms_openai-0.6.12-py3-none-any.whl.metadata (3.0 kB)\n",
      "  Downloading llama_index_llms_openai-0.6.11-py3-none-any.whl.metadata (3.0 kB)\n",
      "  Downloading llama_index_llms_openai-0.6.10-py3-none-any.whl.metadata (3.0 kB)\n",
      "  Downloading llama_index_llms_openai-0.6.9-py3-none-any.whl.metadata (3.0 kB)\n",
      "  Downloading llama_index_llms_openai-0.6.8-py3-none-any.whl.metadata (3.0 kB)\n",
      "  Downloading llama_index_llms_openai-0.6.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "  Downloading llama_index_llms_openai-0.6.6-py3-none-any.whl.metadata (3.0 kB)\n",
      "  Downloading llama_index_llms_openai-0.6.5-py3-none-any.whl.metadata (3.0 kB)\n",
      "  Downloading llama_index_llms_openai-0.6.4-py3-none-any.whl.metadata (3.0 kB)\n",
      "  Downloading llama_index_llms_openai-0.6.3-py3-none-any.whl.metadata (3.0 kB)\n",
      "  Downloading llama_index_llms_openai-0.6.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "  Downloading llama_index_llms_openai-0.6.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "  Downloading llama_index_llms_openai-0.6.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "  Downloading llama_index_llms_openai-0.5.6-py3-none-any.whl.metadata (3.0 kB)\n",
      "  Downloading llama_index_llms_openai-0.5.5-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting llama-index-core\n",
      "  Downloading llama_index_core-0.13.6-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting llama-index-workflows\n",
      "  Downloading llama_index_workflows-1.3.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting llama-index-llms-openai\n",
      "  Downloading llama_index_llms_openai-0.5.4-py3-none-any.whl.metadata (3.0 kB)\n",
      "  Downloading llama_index_llms_openai-0.5.3-py3-none-any.whl.metadata (3.0 kB)\n",
      "  Downloading llama_index_llms_openai-0.5.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "  Downloading llama_index_llms_openai-0.5.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "  Downloading llama_index_llms_openai-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting llama-index-core\n",
      "  Downloading llama_index_core-0.12.52.post1-py3-none-any.whl.metadata (2.5 kB)\n",
      "INFO: pip is looking at multiple versions of llama-index-tools-code-interpreter to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting llama-index-tools-code-interpreter\n",
      "  Downloading llama_index_tools_code_interpreter-0.4.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: llama-index-program-openai<0.4,>=0.3.0 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from llama-index-question-gen-openai) (0.3.2)\n",
      "Requirement already satisfied: beautifulsoup4<5,>=4.12.3 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from llama-index-readers-web) (4.13.3)\n",
      "Requirement already satisfied: chromedriver-autoinstaller<0.7,>=0.6.3 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from llama-index-readers-web) (0.6.4)\n",
      "Requirement already satisfied: defusedxml<0.8,>=0.7.1 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from llama-index-readers-web) (0.7.1)\n",
      "Requirement already satisfied: firecrawl-py>=4.3.3 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from llama-index-readers-web) (4.13.0)\n",
      "Requirement already satisfied: html2text<2025,>=2024.2.26 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from llama-index-readers-web) (2024.2.26)\n",
      "INFO: pip is looking at multiple versions of llama-index-readers-web to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting llama-index-readers-web\n",
      "  Downloading llama_index_readers_web-0.5.5-py3-none-any.whl.metadata (1.2 kB)\n",
      "  Downloading llama_index_readers_web-0.5.4-py3-none-any.whl.metadata (1.2 kB)\n",
      "  Downloading llama_index_readers_web-0.5.3-py3-none-any.whl.metadata (1.2 kB)\n",
      "  Downloading llama_index_readers_web-0.5.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "  Downloading llama_index_readers_web-0.5.1-py3-none-any.whl.metadata (1.2 kB)\n",
      "  Downloading llama_index_readers_web-0.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: lxml-html-clean>=0.4.2 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from llama-index-readers-web) (0.4.2)\n",
      "Requirement already satisfied: lxml>=5.4.0 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from llama-index-readers-web) (5.4.0)\n",
      "Requirement already satisfied: markdownify>=1.1.0 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from llama-index-readers-web) (1.1.0)\n",
      "Requirement already satisfied: newspaper3k<0.3,>=0.2.8 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from llama-index-readers-web) (0.2.8)\n",
      "Requirement already satisfied: oxylabs>=2.0.0 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from llama-index-readers-web) (2.0.0)\n",
      "Requirement already satisfied: playwright<2.0,>=1.30 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from llama-index-readers-web) (1.52.0)\n",
      "Requirement already satisfied: selenium<5,>=4.17.2 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from llama-index-readers-web) (4.39.0)\n",
      "Requirement already satisfied: spider-client<0.0.28,>=0.0.27 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from llama-index-readers-web) (0.0.27)\n",
      "Requirement already satisfied: urllib3>=1.1.0 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from llama-index-readers-web) (2.6.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from beautifulsoup4<5,>=4.12.3->llama-index-readers-web) (2.6)\n",
      "Requirement already satisfied: packaging>=23.1 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from chromedriver-autoinstaller<0.7,>=0.6.3->llama-index-readers-web) (24.2)\n",
      "Requirement already satisfied: cssselect>=0.9.2 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from newspaper3k<0.3,>=0.2.8->llama-index-readers-web) (1.3.0)\n",
      "Requirement already satisfied: feedparser>=5.2.1 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from newspaper3k<0.3,>=0.2.8->llama-index-readers-web) (6.0.11)\n",
      "Requirement already satisfied: tldextract>=2.0.1 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from newspaper3k<0.3,>=0.2.8->llama-index-readers-web) (5.3.0)\n",
      "Requirement already satisfied: feedfinder2>=0.0.4 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from newspaper3k<0.3,>=0.2.8->llama-index-readers-web) (0.0.4)\n",
      "Requirement already satisfied: jieba3k>=0.35.1 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from newspaper3k<0.3,>=0.2.8->llama-index-readers-web) (0.35.1)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from newspaper3k<0.3,>=0.2.8->llama-index-readers-web) (2.9.0.post0)\n",
      "Requirement already satisfied: tinysegmenter==0.3 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from newspaper3k<0.3,>=0.2.8->llama-index-readers-web) (0.3)\n",
      "Requirement already satisfied: pyee<14,>=13 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from playwright<2.0,>=1.30->llama-index-readers-web) (13.0.0)\n",
      "Requirement already satisfied: greenlet<4.0.0,>=3.1.1 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from playwright<2.0,>=1.30->llama-index-readers-web) (3.1.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from requests>=2.31.0->llama-index-core) (3.4.1)\n",
      "Requirement already satisfied: trio<1.0,>=0.31.0 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from selenium<5,>=4.17.2->llama-index-readers-web) (0.32.0)\n",
      "Requirement already satisfied: trio-websocket<1.0,>=0.12.2 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from selenium<5,>=4.17.2->llama-index-readers-web) (0.12.2)\n",
      "Requirement already satisfied: websocket-client<2.0,>=1.8.0 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from selenium<5,>=4.17.2->llama-index-readers-web) (1.8.0)\n",
      "Requirement already satisfied: sortedcontainers in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from trio<1.0,>=0.31.0->selenium<5,>=4.17.2->llama-index-readers-web) (2.4.0)\n",
      "Requirement already satisfied: outcome in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from trio<1.0,>=0.31.0->selenium<5,>=4.17.2->llama-index-readers-web) (1.3.0.post0)\n",
      "Requirement already satisfied: wsproto>=0.14 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from trio-websocket<1.0,>=0.12.2->selenium<5,>=4.17.2->llama-index-readers-web) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from urllib3[socks]<3.0,>=2.5.0->selenium<5,>=4.17.2->llama-index-readers-web) (1.7.1)\n",
      "Requirement already satisfied: google-api-python-client<3,>=2.115.0 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from llama-index-tools-google) (2.170.0)\n",
      "Requirement already satisfied: google-auth-httplib2<0.3,>=0.2.0 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from llama-index-tools-google) (0.2.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=1.2.0 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from llama-index-tools-google) (1.2.2)\n",
      "INFO: pip is looking at multiple versions of llama-index-tools-google to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting llama-index-tools-google\n",
      "  Downloading llama_index_tools_google-0.6.1-py3-none-any.whl.metadata (1.9 kB)\n",
      "  Downloading llama_index_tools_google-0.6.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from google-api-python-client<3,>=2.115.0->llama-index-tools-google) (0.22.0)\n",
      "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from google-api-python-client<3,>=2.115.0->llama-index-tools-google) (2.47.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from google-api-python-client<3,>=2.115.0->llama-index-tools-google) (2.24.2)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from google-api-python-client<3,>=2.115.0->llama-index-tools-google) (4.1.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client<3,>=2.115.0->llama-index-tools-google) (1.70.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client<3,>=2.115.0->llama-index-tools-google) (5.29.5)\n",
      "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client<3,>=2.115.0->llama-index-tools-google) (1.26.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client<3,>=2.115.0->llama-index-tools-google) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client<3,>=2.115.0->llama-index-tools-google) (4.9.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from google-auth-oauthlib<2,>=1.2.0->llama-index-tools-google) (2.0.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client<3,>=2.115.0->llama-index-tools-google) (3.2.3)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from rsa<5,>=3.1.4->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client<3,>=2.115.0->llama-index-tools-google) (0.6.1)\n",
      "Requirement already satisfied: six in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from feedfinder2>=0.0.4->newspaper3k<0.3,>=0.2.8->llama-index-readers-web) (1.17.0)\n",
      "Requirement already satisfied: sgmllib3k in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from feedparser>=5.2.1->newspaper3k<0.3,>=0.2.8->llama-index-readers-web) (1.0.0)\n",
      "Requirement already satisfied: click in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from nltk>3.8.1->llama-index-core) (8.2.1)\n",
      "Requirement already satisfied: joblib in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from nltk>3.8.1->llama-index-core) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from nltk>3.8.1->llama-index-core) (2024.11.6)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=1.2.0->llama-index-tools-google) (3.2.2)\n",
      "Requirement already satisfied: requests-file>=1.4 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from tldextract>=2.0.1->newspaper3k<0.3,>=0.2.8->llama-index-readers-web) (2.1.0)\n",
      "Requirement already satisfied: filelock>=3.0.8 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from tldextract>=2.0.1->newspaper3k<0.3,>=0.2.8->llama-index-readers-web) (3.18.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from typing-inspect>=0.8.0->llama-index-core) (1.1.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from dataclasses-json->llama-index-core) (3.26.1)\n",
      "Requirement already satisfied: colorama>=0.4 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from griffe->banks<3,>=2.2.0->llama-index-core) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/renato/Documents/Repos/GenAI4Humanists/.venv/lib/python3.12/site-packages (from jinja2->banks<3,>=2.2.0->llama-index-core) (3.0.2)\n",
      "Downloading llama_index_core-0.12.52.post1-py3-none-any.whl (7.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading llama_index_workflows-1.3.0-py3-none-any.whl (42 kB)\n",
      "Installing collected packages: llama-index-workflows, llama-index-core\n",
      "\u001b[2K  Attempting uninstall: llama-index-workflows\n",
      "\u001b[2K    Found existing installation: llama-index-workflows 2.12.2\n",
      "\u001b[2K    Uninstalling llama-index-workflows-2.12.2:\n",
      "\u001b[2K      Successfully uninstalled llama-index-workflows-2.12.2\n",
      "\u001b[2K  Attempting uninstall: llama-index-core\n",
      "\u001b[2K    Found existing installation: llama-index-core 0.14.12\n",
      "\u001b[2K    Uninstalling llama-index-core-0.14.12:\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/2\u001b[0m [llama-index-core]\n",
      "\u001b[2K      Successfully uninstalled llama-index-core-0.14.12━━━━━━━\u001b[0m \u001b[32m1/2\u001b[0m [llama-index-core]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [llama-index-core][llama-index-core]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "llama-index-llms-ollama 0.9.1 requires llama-index-core<0.15,>=0.14.5, but you have llama-index-core 0.12.52.post1 which is incompatible.\n",
      "llama-index-embeddings-huggingface 0.6.1 requires llama-index-core<0.15,>=0.13.0, but you have llama-index-core 0.12.52.post1 which is incompatible.\n",
      "llama-index-embeddings-openai 0.5.1 requires llama-index-core<0.15,>=0.13.0, but you have llama-index-core 0.12.52.post1 which is incompatible.\n",
      "llama-index-embeddings-ollama 0.8.6 requires llama-index-core<0.15,>=0.13.0, but you have llama-index-core 0.12.52.post1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed llama-index-core-0.12.52.post1 llama-index-workflows-1.3.0\n"
     ]
    }
   ],
   "source": [
    "#!pip install -qU llama-index \n",
    "#!pip install -qU llama-index-experimental\n",
    "#!pip install -qU llama_hub\n",
    "!pip install -qU llama-index-llms-ollama llama-index-embeddings-ollama llama_index.embeddings.huggingface llama_index.embeddings.openai\n",
    "!pip install -qU --no-cache-dir llama-index-core llama-index-llms-openai llama-index-agent-openai llama-index-llms-openai llama-index-tools-code-interpreter llama-index-workflows llama-index-question-gen-openai llama-index-readers-web llama-index-tools-google openai pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12.52.post1\n",
      "✅ Sucess! Core version: 0.12.52.post1\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import llama_index.core\n",
    "    print(llama_index.core.__version__)\n",
    "    from llama_index.agent.openai import OpenAIAgent\n",
    "    from llama_index.tools.code_interpreter import CodeInterpreterToolSpec\n",
    "    print(f\"✅ Sucess! Core version: {llama_index.core.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Import error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Another error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SG6gzO4zNdkB"
   },
   "source": [
    "## Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import glob\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from IPython.display import Image\n",
    "import pydantic\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "from google import genai\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import logging\n",
    "#logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "#logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "## Llamaindex LLMs\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "## Llamaindex readers\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "## LlamaIndex Index Types\n",
    "from llama_index.core import ListIndex\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core import TreeIndex\n",
    "from llama_index.core import KeywordTableIndex\n",
    "from llama_index.core import SimpleKeywordTableIndex\n",
    "from llama_index.core import DocumentSummaryIndex\n",
    "from llama_index.core import SummaryIndex\n",
    "from llama_index.core import KnowledgeGraphIndex\n",
    "from llama_index.core.query_engine import PandasQueryEngine\n",
    "\n",
    "## LlamaIndex Context Managers\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.core import load_index_from_storage\n",
    "from llama_index.core.response_synthesizers import get_response_synthesizer\n",
    "from llama_index.core.response_synthesizers import ResponseMode\n",
    "from llama_index.core.schema import Node\n",
    "from llama_index.core import Settings\n",
    "\n",
    "## LlamaIndex Templates\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.core.prompts import ChatPromptTemplate\n",
    "from llama_index.core.base.llms.types import ChatMessage, MessageRole\n",
    "\n",
    "## LlamaIndex Agents\n",
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.core.agent.workflow import ReActAgent\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "## LlamaIndex Callbacks\n",
    "from llama_index.core.callbacks import CallbackManager\n",
    "from llama_index.core.callbacks import LlamaDebugHandler\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "load_dotenv(\"../../.env\", override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ[\"OPENAI_API_KEY\"] = \"<the key>\"\n",
    "#openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "client = openai.OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model=\"gpt-4o\"\n",
    "model=\"gpt-4o-mini\"\n",
    "#model=\"gpt-5\"\n",
    "\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\")\n",
    "Settings.llm = OpenAI(temperature=0, \n",
    "                      model=model, \n",
    "                      #max_tokens=512\n",
    "                      PRESENCE_PENALTY=-2,\n",
    "                      TOP_P=1,\n",
    "                     )\n",
    "\n",
    "llm = Settings.llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- Function Tool  \n",
    "A function tool is a simple wrapper around any existing function (both sync and async are supported!).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "def add(x: int, y: int) -> int:\n",
    "    \"\"\"Adds two integers together.\"\"\"\n",
    "    return x + y\n",
    "\n",
    "def subtract(x: int, y: int) -> int: \n",
    "    \"\"\"Subtract the second number from the first number.\"\"\"\n",
    "    return (x - y)\n",
    "\n",
    "def multiply(x: int, y: int) -> int: \n",
    "    \"\"\"Multiply one number by the other number.\"\"\"\n",
    "    return (x * y)\n",
    "\n",
    "def uppercase(x: str) -> str: \n",
    "    \"\"\"Return the input string in uppercase.\"\"\"\n",
    "    return (x.upper())\n",
    "\n",
    "def square(x: int) -> int: \n",
    "    \"\"\"Return the square of the give number.\"\"\"\n",
    "    return (x + x)\n",
    "\n",
    "add_tool = FunctionTool.from_defaults(fn=add)\n",
    "subtract_tool = FunctionTool.from_defaults(fn=subtract)\n",
    "multiply_tool = FunctionTool.from_defaults(fn=multiply)\n",
    "uppercase_tool = FunctionTool.from_defaults(fn=uppercase)\n",
    "square_tool = FunctionTool.from_defaults(fn=square)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: subtract with args: {\"x\": 3, \"y\": 12}\n",
      "=== Function Output ===\n",
      "-9\n",
      "-9\n"
     ]
    }
   ],
   "source": [
    "response = llm.predict_and_call(\n",
    "    [add_tool, \n",
    "     subtract_tool, \n",
    "     multiply_tool, \n",
    "     uppercase_tool,\n",
    "     square_tool,\n",
    "    ], \n",
    "    \"Tell me the output of 3 - 12 \", \n",
    "    verbose=True\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There is no criticism on the functioning of the tool; tools are called by similarity of query vs docstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: square with args: {\"x\": 3}\n",
      "=== Function Output ===\n",
      "6\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "response = llm.predict_and_call(\n",
    "    [add_tool, \n",
    "     subtract_tool, \n",
    "     multiply_tool, \n",
    "     uppercase_tool,\n",
    "     square_tool,\n",
    "    ], \n",
    "    \"Tell me how much is three to the power of two\", \n",
    "    verbose=True\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- [Coding Assistant and Interpreter](https://llamahub.ai/l/tools/llama-index-tools-code-interpreter?from=)  \n",
    "[Github](https://github.com/run-llama/llama_index/blob/main/llama-index-integrations/tools/llama-index-tools-code-interpreter/examples/code_interpreter.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent.workflow import FunctionAgent\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.tools.code_interpreter import CodeInterpreterToolSpec\n",
    "\n",
    "code_spec = CodeInterpreterToolSpec()\n",
    "tools = code_spec.to_tool_list()\n",
    "\n",
    "agent = FunctionAgent(\n",
    "    tools=tools, \n",
    "    llm=llm, \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../Data/csv/california_housing_train.csv\n"
     ]
    }
   ],
   "source": [
    "!ls ../../Data/csv/california_housing_train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step init_run\n",
      "Step init_run produced event AgentInput\n",
      "Running step setup_agent\n",
      "Step setup_agent produced event AgentSetup\n",
      "Running step run_agent_step\n",
      "Step run_agent_step produced event AgentOutput\n",
      "Running step parse_agent_output\n",
      "Step parse_agent_output produced no event\n",
      "Running step call_tool\n",
      "Step call_tool produced event ToolCallResult\n",
      "Running step aggregate_tool_results\n",
      "Step aggregate_tool_results produced event AgentInput\n",
      "Running step setup_agent\n",
      "Step setup_agent produced event AgentSetup\n",
      "Running step run_agent_step\n",
      "Step run_agent_step produced event AgentOutput\n",
      "Running step parse_agent_output\n",
      "Step parse_agent_output produced event StopEvent\n",
      "\n",
      "--- FINAL AGENT RESPONSE ---\n",
      "The script executed successfully. Here are the results:\n",
      "\n",
      "1. **Path Check**: The path exists: `True`\n",
      "2. **DataFrame Info**:\n",
      "   - The DataFrame has 17,000 entries and 9 columns.\n",
      "   - All columns are of type `float64` and have no missing values.\n",
      "3. **DataFrame Description**:\n",
      "   - Summary statistics for each column were printed, including count, mean, standard deviation, min, max, and quartiles.\n",
      "4. **Plotting**: The scatter plot was created and saved successfully.\n",
      "\n",
      "If you need any further assistance or modifications, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "The file is located at '../../Data/csv/california_housing_train.csv'. \n",
    "Run a SINGLE Python script that does the following:\n",
    "1. Imports pandas and os.\n",
    "2. Checks if the path exists using os.path.exists and PRINTS the result.\n",
    "3. If it exists, loads it and PRINTS df.info() and df.describe().\n",
    "4. Creates the scatter plot and PRINTS 'Plot saved successfully' after plt.savefig().\n",
    "\n",
    "IMPORTANT: You must use PRINT statements for everything you want to see.\n",
    "\"\"\"\n",
    "\n",
    "# 5. Execution (Asynchronous)\n",
    "# If you are in a Jupyter Notebook, use 'await'\n",
    "response = await agent.run(user_msg=prompt)\n",
    "\n",
    "print(\"\\n--- FINAL AGENT RESPONSE ---\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step init_run\n",
      "Step init_run produced event AgentInput\n",
      "Running step setup_agent\n",
      "Step setup_agent produced event AgentSetup\n",
      "Running step run_agent_step\n",
      "Step run_agent_step produced event AgentOutput\n",
      "Running step parse_agent_output\n",
      "Step parse_agent_output produced event StopEvent\n",
      "To show the columns of a DataFrame in Python, particularly when using the pandas library, you can use the following code:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "\n",
      "# Sample DataFrame\n",
      "data = {\n",
      "    'Column1': [1, 2, 3],\n",
      "    'Column2': ['A', 'B', 'C'],\n",
      "    'Column3': [True, False, True]\n",
      "}\n",
      "\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "# Show the columns\n",
      "print(df.columns)\n",
      "```\n",
      "\n",
      "This code creates a simple DataFrame and then prints the names of its columns. If you have a specific DataFrame in mind or need further assistance, please let me know!\n"
     ]
    }
   ],
   "source": [
    "q = \"What was the Python code used to show the columns?\"\n",
    "response = await agent.run(user_msg=q)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step init_run\n",
      "Step init_run produced event AgentInput\n",
      "Running step setup_agent\n",
      "Step setup_agent produced event AgentSetup\n",
      "Running step run_agent_step\n",
      "Step run_agent_step produced event AgentOutput\n",
      "Running step parse_agent_output\n",
      "Step parse_agent_output produced no event\n",
      "Running step call_tool\n",
      "Step call_tool produced event ToolCallResult\n",
      "Running step aggregate_tool_results\n",
      "Step aggregate_tool_results produced event AgentInput\n",
      "Running step setup_agent\n",
      "Step setup_agent produced event AgentSetup\n",
      "Running step run_agent_step\n",
      "Step run_agent_step produced event AgentOutput\n",
      "Running step parse_agent_output\n",
      "Step parse_agent_output produced event StopEvent\n",
      "The DataFrame contains latitude and longitude data. Here are the first few rows:\n",
      "\n",
      "- Longitude: -114.31, Latitude: 34.19\n",
      "- Longitude: -114.47, Latitude: 34.40\n",
      "- Longitude: -114.56, Latitude: 33.69\n",
      "- Longitude: -114.57, Latitude: 33.64\n",
      "- Longitude: -114.57, Latitude: 33.57\n",
      "\n",
      "These coordinates are located in the United States, specifically in the state of California. Would you like to know more about a specific location or perform any further analysis?\n"
     ]
    }
   ],
   "source": [
    "q = \"In which country are these latitudes and longitudes in the Dataframe read from '../../Data/csv/california_housing_train.csv'?\"\n",
    "response = await agent.run(user_msg=q)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step init_run\n",
      "Step init_run produced event AgentInput\n",
      "Running step setup_agent\n",
      "Step setup_agent produced event AgentSetup\n",
      "Running step run_agent_step\n",
      "Step run_agent_step produced event AgentOutput\n",
      "Running step parse_agent_output\n",
      "Step parse_agent_output produced no event\n",
      "Running step call_tool\n",
      "Step call_tool produced event ToolCallResult\n",
      "Running step aggregate_tool_results\n",
      "Step aggregate_tool_results produced event AgentInput\n",
      "Running step setup_agent\n",
      "Step setup_agent produced event AgentSetup\n",
      "Running step run_agent_step\n",
      "Step run_agent_step produced event AgentOutput\n",
      "Running step parse_agent_output\n",
      "Step parse_agent_output produced event StopEvent\n",
      "The DataFrame contains latitude and longitude data. Here are the first few rows:\n",
      "\n",
      "- Longitude: -114.31, Latitude: 34.19\n",
      "- Longitude: -114.47, Latitude: 34.40\n",
      "- Longitude: -114.56, Latitude: 33.69\n",
      "- Longitude: -114.57, Latitude: 33.64\n",
      "- Longitude: -114.57, Latitude: 33.57\n",
      "\n",
      "These coordinates are located in the United States, specifically in the state of California. Would you like to know more about a specific location or perform any further analysis?\n"
     ]
    }
   ],
   "source": [
    "q = \"In which country are these latitudes and longitudes in the Dataframe read from '../../Data/csv/california_housing_train.csv'?\"\n",
    "\n",
    "response = await agent.run(user_msg=q)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step init_run\n",
      "Step init_run produced event AgentInput\n",
      "Running step setup_agent\n",
      "Step setup_agent produced event AgentSetup\n",
      "Running step run_agent_step\n",
      "Step run_agent_step produced event AgentOutput\n",
      "Running step parse_agent_output\n",
      "Step parse_agent_output produced no event\n",
      "Running step call_tool\n",
      "Step call_tool produced event ToolCallResult\n",
      "Running step aggregate_tool_results\n",
      "Step aggregate_tool_results produced event AgentInput\n",
      "Running step setup_agent\n",
      "Step setup_agent produced event AgentSetup\n",
      "Running step run_agent_step\n",
      "Step run_agent_step produced event AgentOutput\n",
      "Running step parse_agent_output\n",
      "Step parse_agent_output produced event StopEvent\n",
      "The graph of median income per total rooms has been created successfully. I saved the graph as an image file named `median_income_per_rooms.png`. You can download it using the link below:\n",
      "\n",
      "[Download median_income_per_rooms.png](sandbox:/median_income_per_rooms.png)\n"
     ]
    }
   ],
   "source": [
    "q = \"Can you make a graph of the median income per total rooms in the Dataframe read from '../../Data/csv/california_housing_train.csv'?\"\n",
    "\n",
    "response = await agent.run(user_msg=q)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3- [SQL Agent](https://llamahub.ai/l/tools/llama-index-tools-database?from=)  \n",
    "[GitHub](https://github.com/run-llama/llama_index/blob/main/llama-index-integrations/tools/llama-index-tools-database/examples/database.ipynb), [article](https://docs.llamaindex.ai/en/stable/examples/index_structs/struct_indices/SQLIndexDemo/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU llama-index-tools-database\n",
    "%pip install -q sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.tools.database import DatabaseToolSpec\n",
    "from llama_index.core import SQLDatabase\n",
    "from sqlalchemy import (\n",
    "    create_engine,\n",
    "    MetaData,\n",
    "    Table,\n",
    "    Column,\n",
    "    String,\n",
    "    Integer,\n",
    "    select,\n",
    "    insert,\n",
    "    text,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(\"sqlite:///:memory:\")\n",
    "metadata_obj = MetaData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create city SQL table\n",
    "table_name = \"city_stats\"\n",
    "city_stats_table = Table(\n",
    "    table_name,\n",
    "    metadata_obj,\n",
    "    Column(\"city_name\", String(16), primary_key=True),\n",
    "    Column(\"population\", Integer),\n",
    "    Column(\"country\", String(16), nullable=False),\n",
    ")\n",
    "metadata_obj.create_all(engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define SQL Database   \n",
    "\n",
    "We first define our SQLDatabase abstraction (a light wrapper around SQLAlchemy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_database = SQLDatabase(engine, include_tables=[\"city_stats\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We add some testing data to our SQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = [\n",
    "    {\"city_name\": \"Toronto\", \"population\": 2930000, \"country\": \"Canada\"},\n",
    "    {\"city_name\": \"Tokyo\", \"population\": 13960000, \"country\": \"Japan\"},\n",
    "    {\"city_name\": \"Chicago\", \"population\": 2679000, \"country\": \"United States\",},\n",
    "    {\"city_name\": \"Seoul\", \"population\": 9776000, \"country\": \"South Korea\"},\n",
    "]\n",
    "for row in rows:\n",
    "    stmt = insert(city_stats_table).values(**row)\n",
    "    with engine.begin() as connection:\n",
    "        cursor = connection.execute(stmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Toronto', 2930000, 'Canada'), ('Tokyo', 13960000, 'Japan'), ('Chicago', 2679000, 'United States'), ('Seoul', 9776000, 'South Korea')]\n"
     ]
    }
   ],
   "source": [
    "# view current table\n",
    "stmt = select(\n",
    "    city_stats_table.c.city_name,\n",
    "    city_stats_table.c.population,\n",
    "    city_stats_table.c.country,\n",
    ").select_from(city_stats_table)\n",
    "\n",
    "with engine.connect() as connection:\n",
    "    results = connection.execute(stmt).fetchall()\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query Index  \n",
    "We first show how we can execute a raw SQL query, which directly executes over the table.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Chicago',)\n",
      "('Seoul',)\n",
      "('Tokyo',)\n",
      "('Toronto',)\n"
     ]
    }
   ],
   "source": [
    "with engine.connect() as con:\n",
    "    rows = con.execute(text(\"SELECT city_name from city_stats\"))\n",
    "    for row in rows:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1: Text-to-SQL Query Engine¶\n",
    "\n",
    "Once we have constructed our SQL database, we can use the NLSQLTableQueryEngine to construct natural language queries that are synthesized into SQL queries.\n",
    "\n",
    "Note that we need to specify the tables we want to use with this query engine. If we don't the query engine will pull all the schema context, which could overflow the context window of the LLM.\n",
    "\n",
    "This query engine should be used in any case where you can specify the tables you want to query over beforehand, or the total size of all the table schema plus the rest of the prompt fits your context window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import NLSQLTableQueryEngine\n",
    "\n",
    "query_engine = NLSQLTableQueryEngine(\n",
    "    sql_database=sql_database, tables=[\"city_stats\"], llm=llm\n",
    ")\n",
    "query_str = \"Which city has the highest population?\"\n",
    "response = query_engine.query(query_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>The city with the highest population is Tokyo, with a population of 13,960,000.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2: Query-Time Retrieval of Tables for Text-to-SQL\n",
    "\n",
    "If we don't know ahead of time which table we would like to use, and the total size of the table schema overflows your context window size, we should store the table schema in an index so that during query time we can retrieve the right schema.\n",
    "\n",
    "The way we can do this is using the SQLTableNodeMapping object, which takes in a SQLDatabase and produces a Node object for each SQLTableSchema object passed into the ObjectIndex constructor.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.indices.struct_store.sql_query import (\n",
    "    SQLTableRetrieverQueryEngine,\n",
    ")\n",
    "from llama_index.core.objects import (\n",
    "    SQLTableNodeMapping,\n",
    "    ObjectIndex,\n",
    "    SQLTableSchema,\n",
    ")\n",
    "#from llama_index.core import VectorStoreIndex\n",
    "\n",
    "# set Logging to DEBUG for more detailed outputs\n",
    "table_node_mapping = SQLTableNodeMapping(sql_database)\n",
    "table_schema_objs = [\n",
    "    (SQLTableSchema(table_name=\"city_stats\"))\n",
    "]  # add a SQLTableSchema for each table\n",
    "\n",
    "obj_index = ObjectIndex.from_objects(\n",
    "    table_schema_objs,\n",
    "    table_node_mapping,\n",
    "    VectorStoreIndex,\n",
    ")\n",
    "query_engine = SQLTableRetrieverQueryEngine(\n",
    "    sql_database, obj_index.as_retriever(similarity_top_k=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we can take our SQLTableRetrieverQueryEngine and query it for our response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>The city with the highest population is Tokyo, with a population of 13,960,000.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = query_engine.query(\"Which city has the highest population?\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Tokyo', 13960000)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can also fetch the raw result from SQLAlchemy!\n",
    "response.metadata[\"result\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You could also add additional context information for each table schema you define."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually set context text\n",
    "city_stats_text = (\n",
    "    \"This table gives information regarding the population and country of a\"\n",
    "    \" given city.\\nThe user will query with codewords, where 'foo' corresponds\"\n",
    "    \" to population and 'bar'corresponds to city.\"\n",
    ")\n",
    "\n",
    "table_node_mapping = SQLTableNodeMapping(sql_database)\n",
    "table_schema_objs = [\n",
    "    (SQLTableSchema(table_name=\"city_stats\", context_str=city_stats_text))\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 3: Text-to-SQL Retriever\n",
    "\n",
    "So far our text-to-SQL capability is packaged in a query engine and consists of both retrieval and synthesis.\n",
    "\n",
    "You can use the SQL retriever on its own. We show you some different parameters you can try, and also show how to plug it into our RetrieverQueryEngine to get roughly the same results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import NLSQLRetriever\n",
    "\n",
    "# default retrieval (return_raw=True)\n",
    "nl_sql_retriever = NLSQLRetriever(\n",
    "    sql_database, tables=[\"city_stats\"], return_raw=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = nl_sql_retriever.retrieve(\n",
    "    \"Return the top 5 cities (along with their populations) with the highest population.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 3f237861-a6e8-4bdb-99f5-9ff5c870b0aa<br>**Similarity:** None<br>**Text:** [('Tokyo', 13960000), ('Seoul', 9776000), ('Toronto', 2930000), ('Chicago', 2679000)]<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.core.response.notebook_utils import display_source_node\n",
    "\n",
    "for n in results:\n",
    "    display_source_node(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default retrieval (return_raw=False)\n",
    "nl_sql_retriever = NLSQLRetriever(\n",
    "    sql_database, tables=[\"city_stats\"], return_raw=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = nl_sql_retriever.retrieve(\n",
    "    \"Return the top 5 cities (along with their populations) with the highest population.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Node ID:** b24a1777-72f3-40b6-803e-a9c2e34f0342<br>**Similarity:** None<br>**Text:** <br>**Metadata:** {'city_name': 'Tokyo', 'population': 13960000}<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 77d266e5-f342-4992-8a99-b0b75ea2b3d0<br>**Similarity:** None<br>**Text:** <br>**Metadata:** {'city_name': 'Seoul', 'population': 9776000}<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 35be7e4b-db9a-48dd-95b1-aaf5e66d5133<br>**Similarity:** None<br>**Text:** <br>**Metadata:** {'city_name': 'Toronto', 'population': 2930000}<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 458df849-f99e-40d1-99ff-1acb009a6ca3<br>**Similarity:** None<br>**Text:** <br>**Metadata:** {'city_name': 'Chicago', 'population': 2679000}<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# NOTE: all the content is in the metadata\n",
    "for n in results:\n",
    "    display_source_node(n, show_source_metadata=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plug into our RetrieverQueryEngine\n",
    "\n",
    "We compose our SQL Retriever with our standard RetrieverQueryEngine to synthesize a response. The result is roughly similar to our packaged Text-to-SQL query engines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from llama_index.core.agent.workflow import FunctionAgent\n",
    "#from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "query_engine = RetrieverQueryEngine.from_args(nl_sql_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\n",
    "    \"Return the top 5 cities (along with their populations) with the highest population.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Tokyo - 13,960,000\n",
      "2. Seoul - 9,776,000\n",
      "3. Toronto - 2,930,000\n",
      "4. Chicago - 2,679,000\n"
     ]
    }
   ],
   "source": [
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Database Tool\n",
    "\n",
    "This tool connects to a database (using SQLAlchemy under the hood) and allows an Agent to query the database and get information about the tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step init_run\n",
      "Step init_run produced event AgentInput\n",
      "Running step setup_agent\n",
      "Step setup_agent produced event AgentSetup\n",
      "Running step run_agent_step\n",
      "Step run_agent_step produced event AgentOutput\n",
      "Running step parse_agent_output\n",
      "Step parse_agent_output produced no event\n",
      "Running step call_tool\n",
      "Step call_tool produced event ToolCallResult\n",
      "Running step aggregate_tool_results\n",
      "Step aggregate_tool_results produced event AgentInput\n",
      "Running step setup_agent\n",
      "Step setup_agent produced event AgentSetup\n",
      "Running step run_agent_step\n",
      "Step run_agent_step produced event AgentOutput\n",
      "Running step parse_agent_output\n",
      "Step parse_agent_output produced event StopEvent\n",
      "The database contains the table named **city_stats**.\n"
     ]
    }
   ],
   "source": [
    "db_tools = DatabaseToolSpec(sql_database)\n",
    "tools = db_tools.to_tool_list()\n",
    "agent = FunctionAgent(\n",
    "    tools=tools, \n",
    "    llm=llm, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "prompt = \"What tables does this database contain?\"\n",
    "response = await agent.run(user_msg=prompt)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step init_run\n",
      "Step init_run produced event AgentInput\n",
      "Running step setup_agent\n",
      "Step setup_agent produced event AgentSetup\n",
      "Running step run_agent_step\n",
      "Step run_agent_step produced event AgentOutput\n",
      "Running step parse_agent_output\n",
      "Step parse_agent_output produced no event\n",
      "Running step call_tool\n",
      "Step call_tool produced event ToolCallResult\n",
      "Running step aggregate_tool_results\n",
      "Step aggregate_tool_results produced event AgentInput\n",
      "Running step setup_agent\n",
      "Step setup_agent produced event AgentSetup\n",
      "Running step run_agent_step\n",
      "Step run_agent_step produced event AgentOutput\n",
      "Running step parse_agent_output\n",
      "Step parse_agent_output produced event StopEvent\n",
      "The table `city_stats` has the following structure:\n",
      "\n",
      "- **city_name**: A VARCHAR field with a maximum length of 16 characters, which cannot be null. This serves as the primary key for the table.\n",
      "- **population**: An INTEGER field that represents the population of the city.\n",
      "- **country**: A VARCHAR field with a maximum length of 16 characters, which cannot be null. This indicates the country in which the city is located.\n",
      "\n",
      "The primary key for this table is `city_name`.\n"
     ]
    }
   ],
   "source": [
    "q = \"Describe the table *city_stats*\"\n",
    "response = await agent.run(user_msg=q)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step init_run\n",
      "Step init_run produced event AgentInput\n",
      "Running step setup_agent\n",
      "Step setup_agent produced event AgentSetup\n",
      "Running step run_agent_step\n",
      "Step run_agent_step produced event AgentOutput\n",
      "Running step parse_agent_output\n",
      "Step parse_agent_output produced no event\n",
      "Running step call_tool\n",
      "Step call_tool produced event ToolCallResult\n",
      "Running step aggregate_tool_results\n",
      "Step aggregate_tool_results produced event AgentInput\n",
      "Running step setup_agent\n",
      "Step setup_agent produced event AgentSetup\n",
      "Running step run_agent_step\n",
      "Step run_agent_step produced event AgentOutput\n",
      "Running step parse_agent_output\n",
      "Step parse_agent_output produced no event\n",
      "Running step call_tool\n",
      "Step call_tool produced event ToolCallResult\n",
      "Running step aggregate_tool_results\n",
      "Step aggregate_tool_results produced event AgentInput\n",
      "Running step setup_agent\n",
      "Step setup_agent produced event AgentSetup\n",
      "Running step run_agent_step\n",
      "Step run_agent_step produced event AgentOutput\n",
      "Running step parse_agent_output\n",
      "Step parse_agent_output produced no event\n",
      "Running step call_tool\n",
      "Step call_tool produced event ToolCallResult\n",
      "Running step aggregate_tool_results\n",
      "Step aggregate_tool_results produced event AgentInput\n",
      "Running step setup_agent\n",
      "Step setup_agent produced event AgentSetup\n",
      "Running step run_agent_step\n",
      "Step run_agent_step produced event AgentOutput\n",
      "Running step parse_agent_output\n",
      "Step parse_agent_output produced no event\n",
      "Running step call_tool\n",
      "Step call_tool produced event ToolCallResult\n",
      "Running step aggregate_tool_results\n",
      "Step aggregate_tool_results produced event AgentInput\n",
      "Running step setup_agent\n",
      "Step setup_agent produced event AgentSetup\n",
      "Running step run_agent_step\n",
      "Step run_agent_step produced event AgentOutput\n",
      "Running step parse_agent_output\n",
      "Step parse_agent_output produced no event\n",
      "Running step call_tool\n",
      "Step call_tool produced event ToolCallResult\n",
      "Running step aggregate_tool_results\n",
      "Step aggregate_tool_results produced event AgentInput\n",
      "Running step setup_agent\n",
      "Step setup_agent produced event AgentSetup\n",
      "Running step run_agent_step\n",
      "Step run_agent_step produced event AgentOutput\n",
      "Running step parse_agent_output\n",
      "Step parse_agent_output produced event StopEvent\n",
      "It appears that there is a persistent issue with accessing the `city_stats` table, despite it being defined in the database. This could be due to a variety of reasons, such as the database context or a misconfiguration.\n",
      "\n",
      "Would you like me to check for any other tables or perform a different action?\n"
     ]
    }
   ],
   "source": [
    "q = \"Retrieve the first row of that table *city_stats*\"\n",
    "response = await agent.run(user_msg=q)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4- Community Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A) [Wikipedia Tool](https://llamahub.ai/l/tools/llama-index-tools-wikipedia?from=tools)  \n",
    "[Github](https://github.com/run-llama/llama_index/blob/main/llama-index-integrations/tools/llama-index-tools-wikipedia/examples/wikipedia.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q llama-index-tools-wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step init_run\n",
      "Step init_run produced event AgentInput\n",
      "Running step setup_agent\n",
      "Step setup_agent produced event AgentSetup\n",
      "Running step run_agent_step\n",
      "Step run_agent_step produced event AgentOutput\n",
      "Running step parse_agent_output\n",
      "Step parse_agent_output produced no event\n",
      "Running step call_tool\n",
      "Step call_tool produced event ToolCallResult\n",
      "Running step aggregate_tool_results\n",
      "Step aggregate_tool_results produced event AgentInput\n",
      "Running step setup_agent\n",
      "Step setup_agent produced event AgentSetup\n",
      "Running step run_agent_step\n",
      "Step run_agent_step produced event AgentOutput\n",
      "Running step parse_agent_output\n",
      "Step parse_agent_output produced event StopEvent\n",
      "Ben Affleck is currently married to Jennifer Lopez. They began dating again in April 2021, after previously being in a relationship from 2002 to 2004. They announced their second engagement in April 2022 and were married in a Las Vegas ceremony on July 16, 2022.\n"
     ]
    }
   ],
   "source": [
    "#from llama_index.core.agent.workflow import FunctionAgent\n",
    "#from llama_index.llms.openai import OpenAI\n",
    "from llama_index.tools.wikipedia import WikipediaToolSpec\n",
    "\n",
    "tool_spec = WikipediaToolSpec()\n",
    "tools = tool_spec.to_tool_list()\n",
    "\n",
    "agent = FunctionAgent(\n",
    "    tools=tools, \n",
    "    llm=llm, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "response = await agent.run(user_msg=\"Who is Ben Affleck's spouse?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B) [Arxiv Tool](https://llamahub.ai/l/tools/llama-index-tools-arxiv?from=tools)  \n",
    "[Github](https://github.com/run-llama/llama_index/tree/main/llama-index-integrations/tools/llama-index-tools-arxiv)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q llama-index-tools-arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step init_run\n",
      "Step init_run produced event AgentInput\n",
      "Running step setup_agent\n",
      "Step setup_agent produced event AgentSetup\n",
      "Running step run_agent_step\n",
      "Step run_agent_step produced event AgentOutput\n",
      "Running step parse_agent_output\n",
      "Step parse_agent_output produced no event\n",
      "Running step call_tool\n",
      "Step call_tool produced event ToolCallResult\n",
      "Running step aggregate_tool_results\n",
      "Step aggregate_tool_results produced event AgentInput\n",
      "Running step setup_agent\n",
      "Step setup_agent produced event AgentSetup\n",
      "Running step run_agent_step\n",
      "Step run_agent_step produced event AgentOutput\n",
      "Running step parse_agent_output\n",
      "Step parse_agent_output produced no event\n",
      "Running step call_tool\n",
      "Running step call_tool\n",
      "Step call_tool produced event ToolCallResult\n",
      "Running step aggregate_tool_results\n",
      "Step aggregate_tool_results produced no event\n",
      "Step call_tool produced event ToolCallResult\n",
      "Running step aggregate_tool_results\n",
      "Step aggregate_tool_results produced event AgentInput\n",
      "Running step setup_agent\n",
      "Step setup_agent produced event AgentSetup\n",
      "Running step run_agent_step\n",
      "Step run_agent_step produced event AgentOutput\n",
      "Running step parse_agent_output\n",
      "Step parse_agent_output produced event StopEvent\n",
      "Here are the top 3 most recent papers related to Prompt Engineering:\n",
      "\n",
      "1. **[Alterbute: Editing Intrinsic Attributes of Objects in Images](https://arxiv.org/pdf/2601.10714v1)**\n",
      "   - This paper introduces Alterbute, a diffusion-based method for editing an object's intrinsic attributes in images. The method allows for changes in color, texture, material, and shape while preserving the object's identity and scene context. It employs a relaxed training objective that enables the model to modify both intrinsic and extrinsic attributes based on a reference image, a textual prompt, and a background image. The authors also utilize Visual Named Entities (VNEs) to facilitate scalable, identity-preserving supervision. The results show that Alterbute outperforms existing methods in preserving identity during intrinsic attribute editing.\n",
      "\n",
      "2. **[The Impact of Generative AI on Architectural Conceptual Design: Performance, Creative Self-Efficacy and Cognitive Load](https://arxiv.org/pdf/2601.10696v1)**\n",
      "   - This study examines how generative AI (GenAI) affects performance, creative self-efficacy, and cognitive load in architectural design tasks. The research involved 36 student participants who completed design tasks both independently and with GenAI assistance. While no overall performance advantage was found for GenAI users, it significantly improved performance for novice designers. However, the use of GenAI was associated with a decline in general creative self-efficacy. The findings suggest that the effectiveness of GenAI is influenced by users' prior expertise and their prompting strategies.\n",
      "\n",
      "3. **[UFO Trees: Practical and Provably-Efficient Parallel Batch-Dynamic Trees](https://arxiv.org/pdf/2601.10706v1)**\n",
      "   - This paper presents UFO trees, a new data structure designed for maintaining dynamic trees under edge updates while supporting various queries. Unlike traditional link-cut trees, UFO trees can handle parallel batch-dynamic updates efficiently. The authors demonstrate that UFO trees perform updates and queries in sub-logarithmic time for low-diameter trees, making them competitive with existing structures. The experimental results indicate that UFO trees are the fastest dynamic tree data structure, with low space usage and scalability to large inputs, which could be beneficial for implementing complex dynamic graph algorithms.\n",
      "\n",
      "These papers highlight advancements in prompt engineering and its applications across different domains, including image editing and architectural design.\n"
     ]
    }
   ],
   "source": [
    "#from llama_index.core.agent.workflow import FunctionAgent\n",
    "#from llama_index.llms.openai import OpenAI\n",
    "from llama_index.tools.arxiv.base import ArxivToolSpec\n",
    "\n",
    "arxiv_tool = ArxivToolSpec()\n",
    "tools = arxiv_tool.to_tool_list()\n",
    "\n",
    "agent = FunctionAgent(\n",
    "    tools=tools, \n",
    "    llm=llm, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "prompt = \"What are the newest discoveries on Prompt Engineering? Summarize the top 3 most recent papers.\"\n",
    "response = await agent.run(user_msg=prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C) [DuckDuckGoSearch Tool](https://llamahub.ai/l/tools/llama-index-tools-duckduckgo?from=tools)  \n",
    "[Github](https://github.com/run-llama/llama_index/blob/main/llama-index-integrations/tools/llama-index-tools-duckduckgo/examples/duckduckgo_search.ipynb)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q llama-index-tools-duckduckgo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step init_run\n",
      "Step init_run produced event AgentInput\n",
      "Running step setup_agent\n",
      "Step setup_agent produced event AgentSetup\n",
      "Running step run_agent_step\n",
      "Step run_agent_step produced event AgentOutput\n",
      "Running step parse_agent_output\n",
      "Step parse_agent_output produced no event\n",
      "Running step call_tool\n",
      "Step call_tool produced event ToolCallResult\n",
      "Running step aggregate_tool_results\n",
      "Step aggregate_tool_results produced event AgentInput\n",
      "Running step setup_agent\n",
      "Step setup_agent produced event AgentSetup\n",
      "Running step run_agent_step\n",
      "Step run_agent_step produced event AgentOutput\n",
      "Running step parse_agent_output\n",
      "Step parse_agent_output produced no event\n",
      "Running step call_tool\n",
      "Step call_tool produced event ToolCallResult\n",
      "Running step aggregate_tool_results\n",
      "Step aggregate_tool_results produced event AgentInput\n",
      "Running step setup_agent\n",
      "Step setup_agent produced event AgentSetup\n",
      "Running step run_agent_step\n",
      "Step run_agent_step produced event AgentOutput\n",
      "Running step parse_agent_output\n",
      "Step parse_agent_output produced no event\n",
      "Running step call_tool\n",
      "Step call_tool produced event ToolCallResult\n",
      "Running step aggregate_tool_results\n",
      "Step aggregate_tool_results produced event AgentInput\n",
      "Running step setup_agent\n",
      "Step setup_agent produced event AgentSetup\n",
      "Running step run_agent_step\n",
      "Step run_agent_step produced event AgentOutput\n",
      "Running step parse_agent_output\n",
      "Step parse_agent_output produced event StopEvent\n",
      "It seems that I'm currently unable to retrieve the latest information on developments in machine learning due to a rate limit issue with the search service. However, I can summarize some general trends and advancements in machine learning that have been prominent in recent times:\n",
      "\n",
      "1. **Transformers and NLP**: The transformer architecture continues to dominate natural language processing (NLP) tasks. Models like GPT-3 and BERT have set new benchmarks for various language tasks.\n",
      "\n",
      "2. **Generative Models**: There has been significant progress in generative models, particularly in generating images, text, and music. Models like DALL-E and Stable Diffusion have gained attention for their ability to create high-quality images from textual descriptions.\n",
      "\n",
      "3. **Federated Learning**: This approach allows models to be trained across multiple devices while keeping data localized, enhancing privacy and security. It's becoming increasingly relevant in industries like healthcare and finance.\n",
      "\n",
      "4. **Explainable AI (XAI)**: As machine learning models are deployed in critical applications, the need for transparency and interpretability has grown. Techniques for making AI decisions more understandable are being actively researched.\n",
      "\n",
      "5. **Reinforcement Learning**: Advances in reinforcement learning, particularly in applications like robotics and game playing, have shown promising results. Techniques like deep reinforcement learning are being applied to complex decision-making tasks.\n",
      "\n",
      "6. **AI Ethics and Fairness**: There is a growing focus on the ethical implications of AI, including bias in algorithms and the societal impact of AI technologies. Researchers are working on frameworks to ensure fairness and accountability in AI systems.\n",
      "\n",
      "For the most current and specific developments, I recommend checking reputable tech news websites, academic journals, or conferences in the field of machine learning.\n"
     ]
    }
   ],
   "source": [
    "#from llama_index.core.agent.workflow import FunctionAgent\n",
    "#from llama_index.llms.openai import OpenAI\n",
    "from llama_index.tools.duckduckgo import DuckDuckGoSearchToolSpec\n",
    "\n",
    "tool_spec = DuckDuckGoSearchToolSpec()\n",
    "tools = tool_spec.to_tool_list()\n",
    "\n",
    "agent = FunctionAgent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    verbose=True\n",
    ")\n",
    "prompt = \"what are the latest developments in machine learning\"\n",
    "response = await agent.run(user_msg=prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D) [Yahoo Finance Tool](https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/tools/)  \n",
    "[Github](https://github.com/run-llama/llama_index/tree/main/llama-index-integrations/tools/llama-index-tools-yahoo-finance)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q llama-index-tools-yahoo-finance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step init_run\n",
      "Step init_run produced event AgentInput\n",
      "Running step setup_agent\n",
      "Step setup_agent produced event AgentSetup\n",
      "Running step run_agent_step\n",
      "Step run_agent_step produced event AgentOutput\n",
      "Running step parse_agent_output\n",
      "Step parse_agent_output produced no event\n",
      "Running step call_tool\n",
      "Step call_tool produced event ToolCallResult\n",
      "Running step aggregate_tool_results\n",
      "Step aggregate_tool_results produced event AgentInput\n",
      "Running step setup_agent\n",
      "Step setup_agent produced event AgentSetup\n",
      "Running step run_agent_step\n",
      "Step run_agent_step produced event AgentOutput\n",
      "Running step parse_agent_output\n",
      "Step parse_agent_output produced event StopEvent\n",
      "The current price of Apple Inc. (AAPL) stock is $255.53.\n"
     ]
    }
   ],
   "source": [
    "#from llama_index.core.agent.workflow import FunctionAgent\n",
    "#from llama_index.llms.openai import OpenAI\n",
    "from llama_index.tools.yahoo_finance import YahooFinanceToolSpec\n",
    "\n",
    "tool_spec = YahooFinanceToolSpec()\n",
    "tools = tool_spec.to_tool_list()\n",
    "\n",
    "agent = FunctionAgent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    verbose=True\n",
    ")\n",
    "prompt = \"What is the price of Apple stock?\"\n",
    "response = await agent.run(user_msg=prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step init_run\n",
      "Step init_run produced event AgentInput\n",
      "Running step setup_agent\n",
      "Step setup_agent produced event AgentSetup\n",
      "Running step run_agent_step\n",
      "Step run_agent_step produced event AgentOutput\n",
      "Running step parse_agent_output\n",
      "Step parse_agent_output produced no event\n",
      "Running step call_tool\n",
      "Step call_tool produced event ToolCallResult\n",
      "Running step aggregate_tool_results\n",
      "Step aggregate_tool_results produced event AgentInput\n",
      "Running step setup_agent\n",
      "Step setup_agent produced event AgentSetup\n",
      "Running step run_agent_step\n",
      "Step run_agent_step produced event AgentOutput\n",
      "Running step parse_agent_output\n",
      "Step parse_agent_output produced no event\n",
      "Running step call_tool\n",
      "Step call_tool produced event ToolCallResult\n",
      "Running step aggregate_tool_results\n",
      "Step aggregate_tool_results produced event AgentInput\n",
      "Running step setup_agent\n",
      "Step setup_agent produced event AgentSetup\n",
      "Running step run_agent_step\n",
      "Step run_agent_step produced event AgentOutput\n",
      "Running step parse_agent_output\n",
      "Step parse_agent_output produced event StopEvent\n",
      "I am currently unable to retrieve the latest news titles for Apple Inc. (AAPL). If you have any specific questions or need information on a different topic, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "q = \"What is the latest news about Apple Inc. (AAPL)?\"\n",
    "response = await agent.run(user_msg=q, max_iterations=20)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2-Indexing_Querying_LLamaIndex",
   "widgets": {}
  },
  "colab": {
   "provenance": [
    {
     "file_id": "https://github.com/rsouza/Prompt_Engineering_Course/blob/main/Notebooks/5_LlamaIndex/2-Indexing_Querying_LLamaIndex.ipynb",
     "timestamp": 1714082079174
    }
   ]
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1d311bf7ee564be781cfd48286066ba0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6efb132931b04655b350d09375a58bf1",
      "placeholder": "​",
      "style": "IPY_MODEL_a0ad0d2d97514a2ab8dc2ab0a5591178",
      "value": " 10/10 [00:00&lt;00:00, 67.37it/s]"
     }
    },
    "1ece578683ef4f83b58288633c9bb975": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e2d2af3f1254498ca0ed0413fc2e3873",
      "placeholder": "​",
      "style": "IPY_MODEL_a87159485bc84f03889bd9ca3439d020",
      "value": "Parsing nodes: 100%"
     }
    },
    "2a51f4e1f37141848d9023722fe81e15": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3f735494c12145f28654994ad79a0832": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "44ee0f3679a34d8482515e78b478283a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "48c37d7e447d49a8b911134d287d1ab8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "56344af539034c34b5e98563675e6ef1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c1cdb05547004947986e416042a3c758",
      "placeholder": "​",
      "style": "IPY_MODEL_9a7cc7cb31fa4ea5afea4f3ef812e68d",
      "value": " 30/30 [00:01&lt;00:00, 28.29it/s]"
     }
    },
    "5a0c1216c09d45caae2855d23accd01a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "68fc88da1157414192a92094331375ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1ece578683ef4f83b58288633c9bb975",
       "IPY_MODEL_6d704507f27e42f8a89b4fe41f875b8a",
       "IPY_MODEL_1d311bf7ee564be781cfd48286066ba0"
      ],
      "layout": "IPY_MODEL_44ee0f3679a34d8482515e78b478283a"
     }
    },
    "6d704507f27e42f8a89b4fe41f875b8a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_48c37d7e447d49a8b911134d287d1ab8",
      "max": 10,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5a0c1216c09d45caae2855d23accd01a",
      "value": 10
     }
    },
    "6efb132931b04655b350d09375a58bf1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "75c38119547645f591c78bcd8f67cc10": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b4f2c1a82f224dd0ba0faef7dca60d80",
      "max": 30,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ce761a2a57694a4f80c6319cbe633693",
      "value": 30
     }
    },
    "9a7cc7cb31fa4ea5afea4f3ef812e68d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a0ad0d2d97514a2ab8dc2ab0a5591178": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a0ff4a0af9d444128c464b97cb9f13dd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ccfb889b464e40c489f0a873681c3463",
       "IPY_MODEL_75c38119547645f591c78bcd8f67cc10",
       "IPY_MODEL_56344af539034c34b5e98563675e6ef1"
      ],
      "layout": "IPY_MODEL_3f735494c12145f28654994ad79a0832"
     }
    },
    "a87159485bc84f03889bd9ca3439d020": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b4f2c1a82f224dd0ba0faef7dca60d80": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c1cdb05547004947986e416042a3c758": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ccfb889b464e40c489f0a873681c3463": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e5e9a34c12b5444db998dd9f3735f83d",
      "placeholder": "​",
      "style": "IPY_MODEL_2a51f4e1f37141848d9023722fe81e15",
      "value": "Generating embeddings: 100%"
     }
    },
    "ce761a2a57694a4f80c6319cbe633693": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e2d2af3f1254498ca0ed0413fc2e3873": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e5e9a34c12b5444db998dd9f3735f83d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
